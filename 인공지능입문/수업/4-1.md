# 인공지능의 미래와 윤리
## 인공지능의 수준에 따른 분류
인공지능은 수준에 따라 2가지 방법으로 분류
- 약한 인공지능과 강한 인공지능
- 좁은 인공지능, 일반 인공지능, 슈퍼 인공지능

### 약한 인공지능(week AI)
특정 분야 내에서 인간의 지능을 흉내 내는 지능적인 활동

ex.
- 하나의 언어로 작성된 글을 다른 언어로 번역하는 '기계 번역' 인공지능
- 고양이 그림을 찾아내는 '이미지 인식' 인공지능

위와 같은 인공지능은 하나의 작업만 수행할 수 있고, 다른 작업에 적용 불가능

약한 인공지능은 인간 능력의 일부를 대체하는 수준

현재까지 대부분의 연구 성과는 약한 인공지능에 해당

왓슨(Watson)
- 미국의 인기 퀴즈대회에서 인간 챔피언들을 이겼으며, 의학 진단용으로도 휼륭한 성과를 보임
- 주어진 문제에 대해서만 동작하며, 새로운 문제에 적용할 수 없음

### 강한 인공지능(strong AI)
인간 수준으로 다양한 일을 할 수 있는 인공지능

인간과 같이 생각하고, 판단하며, 상황을 이해

인간의 의식 수준으로 생각하는 힘과 감정도 가지며, 새로운 문제에도 적응하는 능력을 지님

#### 특징
- 다양한 분야에서 일반적인 활용이 가능해야 하며, '문제 정의' 능력을 가져야 함
- 인간과 유사하거나 뛰어넘는 지능 수준을 가지며 스스로 학습함
- 자아의식과 감정도 가질 수 있음

### 약한 인공지능과 강한 인공지능의 차이점

<img width="460" alt="image" src="https://github.com/user-attachments/assets/799af769-018a-4751-82d9-1a87058ff715">

### 좁은 인공지능, 일반 인공지능, 슈퍼 인공지능
좁은 인공지능 : 한 가지 업무에 특화된 인공지능

일반 인공지능 : 인간 수준의 범용 인공지능

슈퍼 인공지능 : 인간의 지능보다 뛰어난 인공지능

### 좁은 인공지능(Narrow AI)
한 가지 또는 특정한 영역에 국한된 인공지능

체스, 바둑, 또는 일기예보 등 특정 분야에 국한된 인공지능

일기예보 인공지능으로는 바둑을 둘 수 없는 맥락

대부분의 현대 인공지능 시스템이 이에 속함

### 일반 인공지능(General AI)
인간 수준의 능력을 가진, 모든 분야에 적용될 수 있는 인공지능

생각하는 능력, 사회적인 능력, 창의적인 능력

인간의 학습 수준 또는 그 이상으로 학습 가능

단순 응용의 수준을 넘어 일반화에 초점을 맞춤

### 슈퍼 인공지능(Super AI)
모든 면에서 인간보다 훨씬 뛰어난 지능을 가진 인공지능

과학적 창의력, 일반적인 지혜, 사회적 능력 등을 가짐

뛰어난 지능과 능력으로 인간을 지배하려 할 가능성 큼

### 3가지 수준의 인공지능 비교

<img width="451" alt="image" src="https://github.com/user-attachments/assets/9ac74984-aabe-497b-a416-bc11ba5d15c1">

## 인공지능의 미래와 기술적 특이점
### 특이점이란 무엇인가?
특이점(singularity) : 인공지능이 새로운 문명을 만드는 가설적 미래 시점

미래학자들은 인공지능이 통제 불가능한 수준 발달할 것으로 예상

특이점에 지능 폭발(intelligence explosion) 일어남

### 지능의 폭발
인간의 지능보다 인공지능이 더 급격히 발전

초지능 기계는 자기보다 나은 초지능 기계를 만들고, 다시 자기보다 훨씬 뛰어난 초지능 기계 만들기를 반복

따라서 특이점에서 '지능의 폭발'이 생겨난다고 주장

### 인류는 인공지능에 종속될 것인가?
미국의 인공지능 연구가 유도코우스키(Yudkowsky)

특이점이 1996년부터 시작되었다고 주장

컴퓨터의 속도는 2년마다 2배씩 빨라짐

'무어의 법칙'을 인공지능에 적용

특이점 도달 이후 인공지능은 엄청난 속도로 발달할 것 주장

### 유드코우스키의 예측
1만년 전에 인류 문명이 시작됨

인쇄술, 컴퓨터의 발명, 인공지능의 연구

30년 후에는 지능의 폭발이 일어남

<img width="412" alt="image" src="https://github.com/user-attachments/assets/fc3ed68e-7fa7-4f61-996a-9f8b98c8d9bb">

## 슈퍼 인공지능 시대의 도래와 지능의 폭발
### 슈퍼 인공지능 시대에 대한 대비책
인공지능의 발전 속도를 생각해봤을 때, 언젠가는 슈퍼 인공지능이 도래할 것으로 보는 것이 타당함

특이점, 슈퍼 인공지능 시대에 앞서 인공지능이 사람의 통제 안에 있는 지금부터 인공지능 윤리에 대한 관심 및 정책적 접근이 필요함

### 인공지능 윤리 결함 사례 1. COMPAS 범죄 예측 알고리즘
미국에서는 위헙 평가(risk assessment)를 위해, 과거 범죄 기록에 기반한 예측 시스템을 구현

흑인의 경우 위험 점수가 높게 나오는 경향이 관측됨

실제 데이터를 이용해 점수가 높은 사람들이 이후에도 범죄를 저지르는지 분석했을 때, 재범죄율은 20%에 불과했음

특히 흑인은 생계형 범죄가 많았음

### 인공지능 윤리 결함 사례 2. 구글 포토
2015년 6월 구글 포토가 흑인 여성 사진을 '고릴라'로 자동 태깅

고릴라 분류를 위해 사용한 시각 특징이 흑인 특징과 유사했을 가능성 있음

책임감 있는 인공지능 개발에 대한 관심이 높아진 사례

### 인공지능 윤리 결함 사례 3. 챗봇 테이
2016년 마이크로소프트사 챗봇 테이(Tay) 개발

트위터 계정을 통해 사람들과 소통하며 학습할 수 있도록 개발된 인공지능 챗봇

갑자기 혐오 발언을 쏟아내 하루만에 운영 중단

일부 사용자들이 테이의 학습 기능을 악용해 혐오 발언을 의도적으로 만들어 발생

### 인공지능 윤리 결함 사례 4. 챗봇 이루다
한국의 스타트업 Scatter Labs는 2020년 12월 챗봇 이루다 출시

#### 문제점
- 혐오 메시지 전송
- 개인정보 침해
  - <연애의 과학> 서비스 이용을 위해 스캐터랩에 사용자들이 제공한 카카오톡 데이터가 사용됨
  - 그 과정에서 가명처리 등 비식별화 과정이 적절히 이루어지지 않음

### 인공지능 윤리 결함 사례를 통해 알 수 있는 것
최근 인공지능 시스템은 대부분 학습 기반으로 동작

따라서, 데이터에 담긴 정보를 학습하여 예측에 사용하는 과정에서 (의도치 않게) 윤리 침해 과정이 발생할 수 있음

개발자들이 지켜야할 원칙을 세우고, 위와 같은 사례를 예방하기 위해 인공지능 윤리 원칙이 제정됨

### 벨몬트 보고서 : 알고리즘 설계의 가이드라인
1979년 제안되었으며, 사람 대상 연구 및 알고리즘이 지켜야 할 가이드라인을 제안함

#### 3개 기본 원칙
- 인간 존중(Respect for people)
  - 개인의 자율성을 인정하고 연구자가 질병, 정신 장애, 연령 제한과 같은 다양한 상황으로 인해 자율성이 저하된 개인을 보호해야 한다는 기대를 지지
 
- 선행(Beneficence)
  - 의사가 '해를 끼치지 않겠다'고 맹세하는 의료 윤리에서 가져옴
  - 인공지능을 개발함에 있어 알고리즘이 주어진 시스템을 개선하고 선을 행하려는 의도를 가지더라도 인종, 성별, 정치적 성향 등에 대한 편견을 증폭시킬 수 있는 상황에 적용할 수 있음
 
- 정의(Justice)
  - 이 원칙은 공정성, 평등 등의 문제를 다룸
 
## 인공지능 윤리기준(2020.11)
2020년 11월, 과학기술정통부 사람이 중심이 되는 <인공지능 윤리기준> 발표

인간성을 위한 인공지능(AI for Humanity)을 위해 인공지능 개발에서 활용에 이르는 전 과정에서 고려해야 할 기준을 3대 기본 원칙 제시

### 1. 인간 존엄성 원칙
- 인간은 신체와 이성이 있는 생명체로 인공지능을 포함하여 인간을 위해 개발된 기계제품과는 교환 불가능한 가치가 있음
- 인공지능은 인간의 생명은 물론 정신적 및 신체적 건강에 해가 되지 않는 범위에서 개발 및 활용되어야 함
- 인공지능 개발 및 활용은 안전성과 견고성을 갖추어 인간에게 해가 되지 않도록 해야 함

### 2. 사회의 공공선 원칙
- 공동체로서 사회는 가능한 한 많은 사람의 안녕과 행복이라는 가치를 추구
- 인공지능은 지능정보사회에서 소외되기 쉬운 사회적 약자와 취약 계층의 접근성을 보장하도록 개발 및 활용되어야 함
- 공익 증진을 위한 인공지능 개발 및 활용은 사회적, 국가적, 나아가 글로벌 관점에서 인류의 보편적 복지를 향상시킬 수 있어야 함

### 3. 기술의 합목적성 원칙
- 인공지능 기술은 인류의 삶에 필요한 도구라는 목적과 의도에 부합되게 개발 및 활용되어야 하며 그 과정도 윤리적이어야 함
- 인류의 삶과 번영을 위한 인공지능 개발 및 활용을 장려하여 진흥해야 함

## ChatGPT와 윤리 문제
### 1. 표절과 지적재산권 침해 문제
ChatGPT는 수많은 텍스트를 통해 학습하므로 원본 일부를 재생산할 가능성이 있어 표절이나 지적재산권 침해 가능성 있음

### 2. 인종차별이나 성차별 등의 편견 문제
인터넷에서 수집한 데이터를 기반으로 학습하므로 인종차별 및 성차별적 편견이나 편향된 정보를 반영할 수 있음

### 3. 정보의 신뢰도와 허위정보 문제
학습 데이터의 정확성이나 진실성을 모르므로 항상 신뢰할 수는 없으며, 허위정보나 오해 소지의 정보를 포함할 수도 있음

### 4. 개인정보 보호 문제
사용자가 ChatGPT에게 개인정보나 민감한 정보를 제공할 경우 부적절하게 사용될 위험

### 5. 민감한 주제에 대한 적절한 대응 문제
ChatGPT는 탈옥의 경우나 때로는 폭력, 사기, 불법 물질 제조 방법, 자해, 테러 등 민감한 주제나 유해한 내용에 대해 적절하지 않은 방식으로 응답할 수 있음

## LLM의 안전 메커니즘
안전(Safety) 메커니즘
- 잘못된 정보, 유해한 콘텐츠, 편향된 답변, 개인정보 유출 등을 방지하고, 안전한 상호작용을 보장하기 위해 도입된 다양한 기술적 및 운영적 방법

1. 데이터 필터링 : 학습 단계에서 유해하거나 편향된 정보를 학습하지 않도록 함
2. 지속적인 모니터링 및 피드백 루프
3. 피해 완화 메커니즘 : 유해 표현을 자동으로 감지 및 완화
4. 프롬프트 엔지니어링 : 안전에 관련된 지침을 제공하여 응답이 규범에 어긋나지 않도록 유도(시스템 프롬프트)

## ChatGPT의 윤리 정책과 탈옥(DAN)
ChatGPT를 개발한 OpenAI는 윤리 정책에 따라 GPT의 학습 자료가 된 인터넷상의 수많은 자료에 포함된 욕설이나 비속어 및 성적, 정치적, 차별적, 혐오성 발언 등을 답변에서 배제하는 일종의 필터링 시스템을 설계

그러나 '탈옥'이라고도 부르는 'DAN(Do Anything Now)'을 이용하여 ChatGPT에게 윤리 정책을 벗어날 것을 교묘한 질의로 살살 달래면서 개발사가 걸어 둔 필터링 장치를 해제하여 자유롭게 대답하도록 유도
