# 특성 공학과 규제
## 기존 방법의 한계와 개선 방향
다항 회귀 모델도 일부 과소적합 패턴이 관측되었다.

농어의 특성 중 높이와 두께는 사용되지 않았다.

```
print(lr.score(train_poly, train_target))

print(lr.score(test_poly, test_target))

"""
결과
0.9706807451768623

0.9775935108325122
"""
```

## 다중 회귀(Multiple Regression)
여러 개의 특성(길이, 높이, 두께 등)을 사용한 선형 회귀이다.
- 특성이 2개인 경우, 3차원 공간 내 특성들로부터 타깃을 예측하는 평면을 학습한다.
- 특성이 더 많아질 경우 시각적으로 표현하기 어렵다.

<img width="501" alt="image" src="https://github.com/user-attachments/assets/9fb85311-6b98-4f6e-968f-5ef9595bd738">

## 특성 공학(Feature Engineering)
도메인 지식을 이용해 특성을 선택하거나, 수정하거나, 새로운 특성을 만드는 작업이다.
- ex.
  - 농어 길이 제곱항
  - 농어 길이 x 농어 높이
 
사이킷런을 이용해 여러 특성들을 만들어보자.

## 데이터 준비
특성이 늘어날 때 마다 매번 데이터를 로드하는 것은 번거롭다.

CSV(comma seperated values)는 데이터 보관을 위해 사용하는 텍스트 파일이며, pandas 라이브러리를 이용해 데이터프레임 형태로 로드할 수 있다.

```
df = pd.read_csv('https://bit.ly/perch_csv')
perch_full = df.to_numpy()
print(perch_full)

"""
결과
[[ 8.4   2.11  1.41]
 [13.7   3.53  2.  ]
 [15.    3.82  2.43]
 [16.2   4.59  2.63]
 [17.4   4.59  2.94]
 [18.    5.22  3.32]
 [18.7   5.2   3.12]
 [19.    5.64  3.05]
 [19.6   5.14  3.04]
 [20.    5.08  2.77]
 [21.    5.69  3.56]
 [21.    5.92  3.31]
 [21.    5.69  3.67]
 [21.3   6.38  3.53]
 [22.    6.11  3.41]
 [22.    5.64  3.52]
 [22.    6.11  3.52]
 [22.    5.88  3.52]
 [22.    5.52  4.  ]
 [22.5   5.86  3.62]
 [22.5   6.79  3.62]
 [22.7   5.95  3.63]
 [23.    5.22  3.63]
 [23.5   6.28  3.72]
 [24.    7.29  3.72]
...
 [43.   11.93  7.28]
 [43.   12.51  7.42]
 [43.5  12.6   8.14]
 [44.   12.49  7.6 ]]
"""
```

타깃 데이터는 동일한 방법으로 준비한다.

```
import numpy as np

perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])
```

훈련 / 테스트 세트를 분할한다.

```
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(perch_full, perch_weight, random_state=42)
```

## 사이킷런의 변환기
### 변환기
사이킷런에서 제공하는 특성을 만들거나 전처리하는 클래스이다.

두 가지 메소드를 공통적으로 제공한다.
- `fit()`
  - 준비 작업을 수행하는 메소드이다.
 
- `transform()`
  - 실제로 변환하는 메소드이다.
 
`fit()`, `transform()` 순서대로 호출해야 하며, 두 작업을 동시에 수행하는 `fit_transform()` 메소드로 제공한다.

다항식 특성 추가, 정규화 등 직접 구현했던 작업들도 변환기 클래스를 이용해 수행할 수 있다.

## PolynomialFeatures
다항 특성을 만들어주는 변환기이다.

절편항, 각 특성 항, 각 특성의 제곱항, 특성 간 곱항을 지니도록 변환한다.

```
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures()
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))


"""
결과
[[1. 2. 3. 4. 6. 9.]]
"""
```

필요에 따라 절편항을 없앨 수 있다.

```
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(include_bias=False)
poly.fit([[2, 3]])
print(poly.transform([[2, 3]]))

"""
결과
[[2. 3. 4. 6. 9.]]
"""
```

훈련 데이터에 다항 특성 변환기를 적용한다.

```
poly = PolynomialFeatures(include_bias=False)

poly.fit(train_input)
train_poly = poly.transform(train_input)

print(train_poly.shape)

poly.get_feature_names_out()

"""
결과
(42, 9)

array(['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2',
       'x2^2'], dtype=object)
```

테스트 셋에 적용할 때는 `transform()` 메소드한 호출하여 반환한다.

```
test_poly = poly.transform(test_input)
print(test_poly.shape)

"""
결과
(14, 9)
"""
```

사실 `PolynomialFeatures`의 `fit()` 메소드는 항 개수만 계산하기 때문에 테스트 셋에 `fit()`을 호출하고 `transform()` 호출해도 효과는 같다.

하지만 훈련 셋에서 값을 계산하는 변환기는 `fit()`을 훈련 세트에만 적용해야 한다.

## 다중 회귀 모델 훈련하기
선형 회귀 클래스 `LinearRegression()`을 이용하여 다중 회귀 모델을 훈련한다.
- 다중 회귀도 여러 특성을 사용한 선형 회귀이다.

```
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))

print(lr.score(test_poly, test_target))

"""
결과
0.9903183436982125

0.9714559911594159
"""
```

훈련 세트에 대한 성능이 더 높아져, 과소적합 문제를 일부 해결하였다.

테스트 셋 성능은 유사하다.

`PolynomialFeatures`의 `degree` 매개변수를 조절해 특성을 더 추가해보자.

최고차항 차수를 5로 지정하여 변환헤보자.

```
poly = PolynomialFeatures(degree=5, include_bias=False)

poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)

print(train_poly.shape)

"""
결과
(42, 55)
"""
```

특성이 55개까지 확장되었다.

55개 특성을 이용한 모델을 훈련하고, 성능을 측정해보자.

```
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))

print(lr.score(test_poly, test_target))

"""
결과
0.999999999999769

-144.40490595353674
"""
```

훈련 셋 성능은 1에 가까워졌으나 테스트 셋 성능이 매우 나빠졌다.
- 모델이 과대적합임을 나타낸다.

특성의 개수를 늘리면 훈련 셋의 패턴은 거의 완벽하게 파악할 수 있지만, 과대적합 되게 된다.

## 규제(Regularization)
모델이 훈련 셋을 너무 과도하게 학습하지 못하도록 하는 것이다.

선형 회귀 모델의 경우 특성에 곱해지는 계수의 크기를 작아지게 만든다.

선형 회귀에 규제를 적용한 대표적인 모델로 릿지(ridge)와 라쏘(lasso)가 있다.

<img width="456" alt="image" src="https://github.com/user-attachments/assets/02e8c67b-7d2a-48f9-8848-baaf7857ca86">

## 특성 스케일 변환
선형 회귀 모델에 규제를 적용하기 전 스케일을 맞춰줘야 한다.(정규화)

사이킷런에서 제공하는 변환기 `StandardScaler`를 사용하자.(z - score)
- `fit()`
  - 평균, 표준편차를 계산하여 훈련 셋에만 적용한다.
 
- `transform()`
  - 계산한 평균, 표준편차를 이용해 데이터를 변환한다.
 
```
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly)

train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)
```

## 릿지 회귀
사이킷런의 Ridge 클래스를 이용한다.

```
from sklearn.linear_model import Ridge

ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))

print(ridge.score(test_scaled, test_target))

"""
결과
0.9896101671037343

0.9790693977615388
"""
```

릿지 회귀를 이용해 과대적합 문제를 완화하였다.

## 라쏘 회귀
사이킷런 Lasso 클래스를 이용한다.

```
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))

print(lasso.score(test_scaled, test_target))

"""
결과
0.989789897208096

0.9800593698421884
"""
```

라쏘 회귀를 이용해 과대적합 문제를 완화하였다.

## 릿지와 라쏘 회귀의 차이
릿지 회귀는 계수 크기를 전체적으로 작게 만들며, 라쏘 회귀는 불필요한 특성의 계수값을 0으로 만들어 해당 특성을 제거하는 효과를 만든다.

따라서, 라쏘는 많은 특성들 중에서 중요한 것을 고르는 특성 선택(feature selection) 방법으로 사용할 수 있다.

`coef__` 속성값에 접근하여 계수가 0이 된 특성의 수를 확인할 수 있다.

```
print(np.sum(lasso.coef_ == 0))

"""
결과
42
"""
```

특성 55개 중 42개는 사용하지 않고, 13개만 이용해 예측한다.

## 마무리
### 키워드로 끝내는 핵심 포인트
#### 다중 회귀(Multiple Regression)
여러 개의 특성을 사용하는 회귀 모델이다.

#### 특성 공학(Feature Engineering)
도메인 지식을 활용해 특성을 선택하고, 만드는 일련의 절차를 의미한다.

#### 릿지 회귀(Ridge Regression)
규제가 있는 선형 회귀 모델 중 하나이며 선형 모델의 계수를 작게 만들어 과대적합을 완화시킨다.

#### 라쏘 회귀(Lasso Regression)
규제가 있는 선형 회귀 모델이며, 릿지와 달리 계수 값을 0으로 만들어 특성 선택의 효과를 만든다.

### 핵심 패키지와 함수
#### pandas
`read_csv()`
- CSV 파일을 로컬 컴퓨터 또는 웹에서 읽어 pandas 프레임으로 변환하는 함수

#### scikit - learn
`PolynomialFeatures`
- 주어진 특성을 조합해 새로운 다항 특성을 만든다.

`Ridge`
- 규제가 있는 릿지 회귀 모델을 훈련한다.

`Lasso`
- 규제가 있는 라쏘 회귀 모델을 훈련한다.

## 확인 문제
### 1번
a, b , c 특성으로 이루어진 훈련 세트를 PolynomialFeatures(degree = 3)으로 변환하였다.

다음 중 이 변환된 데이터에 포함되지 않는 특성은 무엇일까?

1. 1
2. a
3. a * b
4. a * b ^ 3

### 2번
다음 중 특성을 표준화하는 사이킷런 변환기 클래스는 무엇일까?

1. Ridge
2. Lasso
3. StandardScaler
4. LinearRegression

### 3번
다음 중 과대적합과 과소적합을 올바르게 표현하지 않은 것은?

1. 과대적합 모델은 훈련 셋 점수가 높다.
2. 과대적합 모델은 테스트 셋 점수도 높다.
3. 과소적합 모델은 훈련 셋 점수가 낮다.
4. 과소적합 모델은 테스트 셋 점수도 낮다.
