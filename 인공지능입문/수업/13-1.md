# K - 평균
## 목표 : 라벨 정보 없이 군집 찾기
<img width="1044" alt="image" src="https://github.com/user-attachments/assets/6ca777dc-3dfd-4f19-a9a3-6b9afae9dfda">

## K - 평균 알고리즘
1. 무작위로 k개의 클러스터 중심(centroid)을 정한다.
2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다.
3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경한다.
4. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아간다.

<img width="912" alt="image" src="https://github.com/user-attachments/assets/fe8038fd-a627-48ba-99d6-421c99b45b9e">

## 과일 사진 데이터 준비하기
wget 명령어를 이용한다.

```
!wget https://bit.ly/fruits_300 -O fruits_300.npy
```

넘파이로 데이터를 로드한다.

```
import numpy as np

fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100 * 100)
```

이미지 데이터는 3차원으로 표현된다.
- (샘플 수, 높이, 너비)

2차원 배열로 변환한다.
- (샘플 수, 높이 x 너비)

## 사이킷런 KMeans 구현
`sklearn.cluster` 모듈 내 KMeans 클래스를 이용해 구현한다.
- `n_clusters`
  - 클러스터 개수를 지정한다.
 
```
from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)
```

군집 결과는 KMeans 클래스 객체의 `labels_` 속성에 배열 형태로 저장된다.
- 각 샘플이 어떤 레이블에 해당되는 지 나타내며, 배열의 길이는 샘플 개수와 같다.

n_cluster = 3인 경우에는 0, 1, 2 세 가지 값을 갖는다.

```
print(km.labels_)

# 클래스 별 분포값
print(np.unique(km.labels_, return_counts=True))

"""
결과
[2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1]

(array([0, 1, 2], dtype=int32), array([112,  98,  90]))
"""
```

## 클러스터 내 사진 시각화
### 유틸리티 함수 정의
```
import matplotlib.pyplot as plt

def draw_fruits(arr, ratio = 1):
  # n은 샘플 개수이다.
  n = len(arr)
  
  # 한 줄에 10개씩 이미지를 그린다.
  # 샘플 개수를 10으로 나누어 전체 행 개수를 계산한다.
  rows = int(np.ceil(n / 10))

  # 행이 1개이면 열 개수는 샘플 개수이고, 아니면 10개이다.
  cols = n if rows < 2 else 10
  fig, axs = plt.subplots(rows, cols, figsize = (cols * ratio, rows * ratio), squeeze=False)
  for i in range(rows):
    for j in range(cols):
      # n개까지만 그린다.
      if(i * 10 + j < n):
        axs[i, j].imshow(arr[i * 10 + j], cmap='gray_r')
      axs[i, j].axis('off')
  plt.show()
```

```
draw_fruits(fruits[km.labels_==0])
```

<img width="567" alt="image" src="https://github.com/user-attachments/assets/d1a9eded-3f1a-4230-996f-89ebb5736df4">

```
draw_fruits(fruits[km.labels_==1])
```

<img width="677" alt="image" src="https://github.com/user-attachments/assets/acf6f3e8-1c3a-4ce6-8936-9840b215e005">

```
draw_fruits(fruits[km.labels_==2])
```

<img width="682" alt="image" src="https://github.com/user-attachments/assets/8db57455-9157-41ca-8595-862b7c4a3902">

## 클러스터 중심 시각화
`cluster_centers_` 속성에 저장된 클러스터 중심값을 그려보자.

```
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio = 3)
```

<img width="614" alt="image" src="https://github.com/user-attachments/assets/9a547cee-a6c3-4aa5-a51f-8a31ed67e29a">

## 각 클러스터 중심값까지 거리 계산
KMeans 클러스터는 샘플로부터 클러스터 중심까지 거리로 변환하는 `transform()` 메소드를 제공한다.
- `StandardScalar`와 같은 변환기

```
print(km.transform(fruits_2d[100 : 101]))

"""
결과
[[3400.24197319 8837.37750892 5279.33763699]]
"""
```

0번 클러스터까지의 거리가 가장 짧다.

`predict()` 메소드를 이용해 속하게 될 클러스터를 예측하자.

```
print(km.predict(fruits_2d[100 : 101]))

"""
결과
0
"""
```

중심값 결과를 보면 0번은 파인애플로 보인다.

이 샘플도 파인애플인지 확인해보자.

```
draw_fruits(fruits[100 : 101])
```

<img width="167" alt="image" src="https://github.com/user-attachments/assets/b33de872-6789-482c-96a5-f5cd4836fdc8">

## 최적의 K 찾기 : 엘보우 방법
### K - 평균 알고리즘의 단점
찾아야 하는 클래스터 개수 k를 사전에 지정해야 하며, 어떤 k가 적절한지(실제로는 데이터 내 몇 개의 그룹이 있는지) 알 수 없다.

### 엘보우 방법
k를 바꿔가며 반복적으로 KMeans 알고리즘을 실행한다.

각 클러스터에 속한 샘플끼리 가까이 위치하면 좋은 클러스터이다.

이너셔(inertia) 값을 계산해 각 클러스터 내 샘플끼리 얼마나 가까이 있는지 측정할 수 있다.
- 값이 낮으면 가깝다.

일반적으로 클러스터 개수를 증가시키면 이너셔가 감소한다.

클러스터 개수 증가시 이너셔 감소 정도가 꺾이는 지점이 있는데 이 지점이 최적 지점이다.

이 지점이 그래프로 그렸을 때 팔꿈치처럼 보여서 엘보우 방법이라 한다.

```
inertia = []
for k in range(2, 7):
  km = KMeans(n_clusters=k, random_state=42)
  km.fit(fruits_2d)
  inertia.append(km.inertia_)

plt.plot(range(2, 7), inertia)
plt.show()
```

<img width="823" alt="image" src="https://github.com/user-attachments/assets/0202324c-ee5b-493b-ab2a-80c0c7789681">

## 마무리
### 키워드로 끝내는 핵심 포인트
#### K - 평균(K - Means)
대표적인 군집화 알고리즘으로 처음에 랜덤하게 클러스터 중심을 정하고 클러스터를 만든다.

그 다음 클러스터의 중심을 이동하고 다시 클러스터를 만드는 식으로 반복해서 최적의 클러스터를 구성한다.

#### 클러스터 중심(Centroid)
k - 평균 알고리즘이 만든 클러스터에 속한 샘플의 특성 평균값이다.

가장 가까운 클러스터 중심을 샘플의 또 다른 특성으로 사용하거나 새로운 샘플에 대한 예측으로 활용할 수 있다.

#### 엘보우 방법(Elbow Method)
최적의 클러스터 개수를 정하는 방법 중 하나이다.

이너셔는 클러스터 중심과 샘플들 간의 거리를 나타내며, 클러스터 개수에 따라 이너셔 감소가 꺾이는 지점이 적절한 클러스터 개수 k가 될 수 있다.

### 핵심 패키지와 함수
