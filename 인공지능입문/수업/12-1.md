# 로지스틱 회귀
## 새로운 기획
럭키백 내 생선을 랜덤하게 넣어 판매하고자 한다.

고객들에게 각 생선의 확률을 보여줄 수 있도록 해야 한다.

## 확률 예측
럭키백에 들어간 생선의 특성(길이, 높이, 두께, 대각선 길이, 무게)을 이용해 7개 생선에 대한 확률을 예측해야 한다.
- 분류? 회귀?

k - 최근접 이웃 분류기를 이용해 주변 샘플을 참조하고, 이웃 내 클래스 비율을 이용해 확률로 예측해보자.

## 데이터 준비하기

```
import pandas as pd

fish = pd.read_csv("https://bit.ly/fish_csv")
fish.head()
```

<img width="482" alt="image" src="https://github.com/user-attachments/assets/cb4526ca-d75c-49e1-adb9-178fd8cf781e">

어떤 생선 품종이 있는지 확인한다.

```
print(pd.unique(fish['Species']))

"""
결과
['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']
"""
```

Species를 타깃으로, 나머지 5개 열은 입력으로 사용한다.

```
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

print(fish_input[ : 5])

"""
결과
[[242.      25.4     30.      11.52     4.02  ]
 [290.      26.3     31.2     12.48     4.3056]
 [340.      26.5     31.1     12.3778   4.6961]
 [363.      29.      33.5     12.73     4.4555]
 [430.      29.      34.      12.444    5.134 ]]
"""
```

훈련 및 테스트 세트를 분할한다.

```
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)
```

특성 표준화를 실시한다.

```
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

## K - 최근접 이웃 분류기의 확률 예측하기
모델을 훈련하고 평가한다.

```
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

print(kn.score(train_scaled, train_target))

print(kn.score(test_scaled, test_target))

"""
결과
0.8907563025210085

0.85
"""
```

확률 예측이 목표이니 우선 평가 점수는 무시하자.

### 다중 분류(Multi - Class Classification)
타깃 데이터에 3개 이상의 클래스가 포함된 데이터를 분류하는 것이다.

이진 분류 모델을 만드는 것과, 코드 수준에서는 동일하다.
- 기존 실습에서는 클래스 번호를 0, 1로 지정하여 전달하였다.
- 사이킷런에서는 문자열로 클래스 이름을 전달할 수 있다.
  - 순서는 알파벳 순으로 정렬된다.
 
```
print(kn.classes_)

"""
결과
['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
"""
```

`predict()`로 예측 시 클래스 이름을 반환한다.

```
print(kn.predict(test_scaled[ : 5]))

"""
결과
['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
"""
```

각 샘플에 대한 확률을 확인하기 위해 `predict_proba()` 메소드를 이용한다.

```
import numpy as np

proba = kn.predict_proba(test_scaled[ : 5])
print(np.round(proba, decimals=4))

"""
결과
[[0.     0.     1.     0.     0.     0.     0.    ]
 [0.     0.     0.     0.     0.     1.     0.    ]
 [0.     0.     0.     1.     0.     0.     0.    ]
 [0.     0.     0.6667 0.     0.3333 0.     0.    ]
 [0.     0.     0.6667 0.     0.3333 0.     0.    ]]
"""
```

컬럼 순서는 알파벳 순으로 정렬된 타깃 클래스 이름이다.

```
distances, indexes = kn.kneighbors(test_scaled[3 : 4])
print(train_target[indexes])

"""
결과
[['Roach' 'Perch' 'Perch']]
"""
```

k - 최근접 이웃 분류 모델(k = 3)이 지닐 수 있는 확률은 0, 1 / 3, 2 / 3, 1 밖에 없다는 한계점이 존재한다.

## 로지스틱 회귀(Logistic Regression)
선형 회귀식에 기반한 분류 모델이다.
- z = a x 무게 + b x 길이 + c x 대각선 + d x 높이 + e x 두께 + f

회귀식으로 출력값(z)을 계산한 후, 시그모이드 함수(로지스틱 함수)를 이용해 변환하여 확률로 만든다.

<img width="405" alt="image" src="https://github.com/user-attachments/assets/5285a15d-ed6a-4545-95b6-6198e62d61d1">

```
import numpy as np
import matplotlib.pyplot as plt

z = np.arange(-5, 5, 0.1)
phi = 1 / (1 + np.exp(-z))

plt.plot(z, phi)
plt.show()
```

<img width="579" alt="image" src="https://github.com/user-attachments/assets/237d934d-5e84-4a84-91ca-5a0be679e13b">

## 로지스틱 회귀로 이진 분류 수행하기
도미(Bream), 빙어(Smelt) 행만 골라 로지스틱 회귀 이진 분류 모델을 훈련한다.

```
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])

"""
결과
['A' 'C']
"""
```

```
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```

훈련 세트에 대한 클래스 / 확률 예측결과를 확인해보자.

```
print(lr.predict(train_bream_smelt[ : 5]))

"""
결과
['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
"""
```

```
print(lr.predict_proba(train_bream_smelt[ : 5]))
```

<img width="670" alt="image" src="https://github.com/user-attachments/assets/473610cc-c8de-4b13-a7cb-fe978885bf50">

학습한 계수를 확인해보자.

```
print(lr.coef_, lr.intercept_)

"""
결과
[[-0.40451732 -0.57582787 -0.66248158 -1.01329614 -0.73123131]] [-2.16172774]
"""
```

- z = - 0.404 x 무게 - 0.576 x 길이 - 0.663 x 대각선 - 0.013 x 높이 - 0.732 x 두께 - 2.161

`decision_function()` 메소드로 샘플에 대한 z 값을 확인할 수 있다.

```
decisions = lr.decision_function(train_bream_smelt[ : 5])
print(decisions)

from scipy.special import expit

print(expit(decisions))

"""
결과
[-6.02991358  3.57043428 -5.26630496 -4.24382314 -6.06135688]

[0.00239993 0.97262675 0.00513614 0.01414953 0.00232581]
"""
```

## 로지스틱 회귀로 다중 분류 수행하기

`LogisticRegression` 클래스를 사용해 이진 분류와 동일한 방식으로 다중 분류를 구현할 수 있다.

```
# C : 규제 정도를 조절하는 설정값이다.
# max_iter : 훈련 반복 횟수이다.
lr = LogisticRegression(C = 20, max_iter=1000)
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))

print(lr.score(test_scaled, test_target))

"""
결과
0.9327731092436975
0.925
"""
```

테스트 셋 5개 샘플에 대해 클래스 / 확률 예측을 수행한다.

```
print(lr.predict(test_scaled[ : 5]))

proba = lr.predict_proba(test_scaled[ : 5])
print(np.round(proba, decimals=3))

print(lr.classes_)

"""
결과
['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']

[[0.    0.014 0.842 0.    0.135 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.934 0.015 0.016 0.   ]
 [0.011 0.034 0.305 0.006 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]

['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
"""
```

다중 로지스틱 회귀의 선형 방정식은 각 타겟 클래스 별로 훈련된다.(z 값이 7개이다.)

```
print(lr.coef_.shape, lr.intercept_.shape)

"""
결과
(7, 5) (7, )
"""
```

<img width="268" alt="image" src="https://github.com/user-attachments/assets/59f562ab-c3b4-4ab7-9ca9-2f5e9d796f28">

시그모이드 대신 소프트맥스(softmax) 함수를 사용해 z값을 확률로 변환한다.

```
decisions = lr.decision_function(test_scaled[ : 5])
print(np.round(decisions, decimals=2))

from scipy.special import softmax

phoba = softmax(decisions, axis = 1)
print(np.round(proba, decimals=3))

"""
결과
[[ -6.51   1.04   5.17  -2.76   3.34   0.35  -0.63]
 [-10.88   1.94   4.78  -2.42   2.99   7.84  -4.25]
 [ -4.34  -6.24   3.17   6.48   2.36   2.43  -3.87]
 [ -0.69   0.45   2.64  -1.21   3.26  -5.7    1.26]
 [ -6.4   -1.99   5.82  -0.13   3.5   -0.09  -0.7 ]]

[[0.    0.014 0.842 0.    0.135 0.007 0.003]
 [0.    0.003 0.044 0.    0.007 0.946 0.   ]
 [0.    0.    0.034 0.934 0.015 0.016 0.   ]
 [0.011 0.034 0.305 0.006 0.567 0.    0.076]
 [0.    0.    0.904 0.002 0.089 0.002 0.001]]
"""
```

## 마무리
### 키워드로 끝내는 핵심 포인트
#### 로지스틱 회귀(Logistic Regression)
선형 회귀식을 사용한 분류 알고리즘이다.

시그모이드 또는 소프트맥스 함수를 사용해 클래스 확률을 출력한다.

#### 다중 분류(Multi - Class Classification)
타깃 클래스가 3개 이상인 분류 문제이다.

로지스틱 회귀는 다중 분류를 위해 소프트맥스 함수를 사용해 클래스 확률을 예측한다.

#### 시그모이드(Sigmoid)
선형 회귀식의 출력을 0과 1 사이의 값으로 압축하며, 이진 분류를 위해 사용한다.

#### 소프트맥스(Softmax)
다중 분류에서 여러 선형 회귀식의 출력 결과를 정규화하여 합이 1이 되도록 만든다.

### 핵심 패키지와 함수
#### scikit - learn
