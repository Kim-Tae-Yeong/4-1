# 선형 회귀
## 새롭게 들어온 의뢰
농어 담당 직원이 길이가 50cm인 농어로 테스트를 해 달라고 한다.

모델로 예측해보니 실제 무게와의 차이가 크다.

## 기존 모델 준비
```
import numpy as np

perch_length = np.array([8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0, 21.0,
       21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5, 22.5, 22.7,
       23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5, 27.3, 27.5, 27.5,
       27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0, 36.5, 36.0, 37.0, 37.0,
       39.0, 39.0, 39.0, 40.0, 40.0, 40.0, 40.0, 42.0, 43.0, 43.0, 43.5,
       44.0])
perch_weight = np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,
       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,
       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,
       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,
       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,
       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,
       1000.0])
```

훈련 및 테스트 세트를 분할한다.

```
from sklearn.model_selection import train_test_split

# 훈련 세트와 테스트 세트로 나눈다.
train_input, test_input, train_target, test_target = train_test_split(perch_length, perch_weight, random_state=42)

# 훈련 세트와 테스트 세트를 2차원 배열로 바꾼다.
train_input = train_input.reshape(-1, 1)
test_input = test_input.reshape(-1, 1)
```

모델을 훈련한다.

```
from sklearn.neighbors import KNeighborsRegressor

# k - 최근접 이웃 회귀 모델을 훈련한다.
knr = KNeighborsRegressor()
knr.fit(train_input, train_target)
```

## 길이 50cm 농어에 대한 예측 및 시각화
```
print(knr.predict([[50]]))

print(np.mean(train_target[indexes]))

"""
결과
1033.33333333

1033.33333333
"""
```

```
import matplotlib.pyplot as plt

# 50cm 농어의 이웃을 구한다.
distances, indexes = knr.kneighbors([[50]])

# 훈련 세트의 산점도를 그린다.
plt.scatter(train_input, train_target)

# 훈련 세트 중에서 이웃 샘플만 다시 그린다.
plt.scatter(train_input[indexes], train_target[indexes], marker = 'D')

# 50cm 농어 데이터
plt.scatter(50, 1033, marker = '^')
plt.show()
```

<img width="583" alt="image" src="https://github.com/user-attachments/assets/4de76e02-7f8a-48b4-a007-afeaeb6822bb">

- k - 최근접 회귀는 가장 가까운 샘플(이웃)을 찾아 타깃값을 평균한다.
- 따라서 예측 샘플이 훈련 세트의 범위 밖이라면 잘못된 값을 예측한다.

## 길이 100cm 농어에 대한 예측 및 시각화
길이가 더 긴 농어(100cm)를 넣어도 같은 값을 예측한다.

```
print(knr.predict([[100]]))

"""
결과
1033.33333333
"""
```

```
# 100cm 농어의 이웃을 구한다.
distances, indexes = knr.kneighbors([[100]])

# 훈련 세트의 산점도를 그린다.
plt.scatter(train_input, train_target)

# 훈련 세트 중에서 이웃 샘플만 다시 그린다.
plt.scatter(train_input[indexes], train_target[indexes], marker = 'D')

# 100cm 농어 데이터
plt.scatter(100, 1033, marker = '^')
plt.show()
```

- kNN은 길이(입력)와 무게(출력)의 관계를 학습하지 않고, 가까운 샘플을 이용해 예측을 수행한다.

## 선형 회귀(Linear Regression)
입력과 출력간 관계를 학습하는 회귀 알고리즘이다.
- 입력 특성이 하나인 경우 단순 선형 회귀(simple linear regression)라 한다.
- 단순 선형 회귀는 입 - 출력 관계를 나타낸 이차원 평면 내 샘플들을 잘 설명하는 직선을 찾는다.

## 단순 선형 회귀 모델 훈련
`sklearn.linear_model` 모듈의 `LinearRegression` 클래스를 이용한다.

```
from sklearn.linear_model import LinearRegression
```

`fit()`으로 학습한다.

```
lr = LinearRegression()

# 선형 회귀 모델 훈련
lr.fit(train_input, train_target)
```

`predict()`으로 50cm 농어에 대한 무게를 예측한다.

```
print(lr.predict([[50]]))

"""
결과
[1241.83860323]
"""
```

최근접 회귀에 비해 예측값이 더 높아진 것을 확인할 수 있다.

## 단순 선형 회귀 모델의 원리
직선은 y = ax + b 꼴로 표현 가능하다.

우리가 얻게 된 단순 회귀 모델은 x가 농어의 길이, y는 농어의 무게일 때 a(기울기), b(절편)을 구하여 얻어진 것이다.

기울기 및 절편 값은 아래와 같이 확인할 수 있다.

```
print(lr.coef_, lr.intercept_)

"""
결과
[39.01714496] -709.0186449535474
"""
```

- 회귀 모델에서 찾은 coef_, intercept_를 모델 파라미터(parameter)라고 한다.
- 많은 머신러닝 알고리즘의 훈련 과정은 최적의 모델 파라미터를 찾는 것이다.

## 회귀 모델 시각화
학습한 절편과 기울기를 이용해 (15, 50) 범위에 대해 모델을 시각화한다.

```
# 훈련 세트의 산점도를 그린다.
plt.scatter(train_input, train_target)

# 15에서 50까지 1차 방정식 그래프를 그린다.
plt.plot([15, 50], [15 * lr.coef_ + lr.intercept_, 50 * lr.coef_ + lr.intercept_])

# 50cm 농어 데이터
plt.scatter(50, 1241.8, marker = '^')
plt.show()
```

<img width="576" alt="image" src="https://github.com/user-attachments/assets/7341fe01-8ae2-4da7-bdd5-e09f227df522">

- 길이 50cm 농어는 훈련 세트 범위 밖의 샘플이지만, 직선으로 표현된 회귀 모델을 이용해 무게값을 예측할 수 있다.

## 회귀 모델 성능 평가
```
print(lr.score(train_input, train_target))

print(lr.score(test_input, test_target))

"""
결과
0.9398463339976041

0.824750312331356
"""
```

- 훈련 / 테스트 세트 성능이 좋지 않다.
- 과소적합 시그널이 보인다.

## 기존 모델의 문제점

<img width="457" alt="image" src="https://github.com/user-attachments/assets/c82a20c4-6df5-4107-a9cf-40aa63958063">

- 길이가 감소할 수록 무게 감소 패턴이 더디어 지지만, 직선 형태의 모델은 이를 표현하지 못한다.
- 짧은 길이에 대해서는 음수 무게값을 예측하게 되는데, 이는 말이 되지 않는다.
- 곡선 형태의 모델을 학습해보자.

## 다항 회귀
다항식을 사용한 선형 회귀 모델을 다항 회귀(polynomial regression)라 한다.

길이 제곱항을 하나의 특성으로 추가한다.

<img width="306" alt="image" src="https://github.com/user-attachments/assets/bd2f69ef-6493-48ed-a041-4145ed6d12cb">

```
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))

print(train_poly.shape, test_poly.shape)

"""
결과
(42, 2) (14, 2)
"""
```

## 다항 회귀 : 훈련 및 예측
선형 회귀 모델로 훈련한다.

```
lr = LinearRegression()
lr.fit(train_poly, train_target)

print(lr.predict([[50 ** 2, 50]]))

"""
결과
[1573.98423528]
"""
```

모델이 훈련한 식은 아래와 같다.

```
print(lr.coef_, lr.intercept_)

"""
결과
[  1.01433211 -21.55792498] 116.05021078278338
"""
```

- 무게 = 1.01 x 길이<sup>2</sup> - 21.6 x 길이 + 116.05

## 다항 회귀 : 예측 및 평가

```
# 구간별 직선을 그리기 위해 15에서 49까지 정수 배열을 만든다.
point = np.arange(15, 50)

# 훈련 세트의 산점도를 그린다.
plt.scatter(train_input, train_target)

# 15에서 49까지 2차 방정식 그래프를 그린다.
plt.plot(point, 1.01 * point ** 2 - 21.6 * point + 116.05)

# 50cm 농어 데이터
plt.scatter([50], [1574], marker = '^')
plt.show()

print(lr.score(train_poly, train_target))

print(lr.score(test_poly, test_target))

"""
결과
0.9706807451768623

0.9775935108325122
"""
```

<img width="579" alt="image" src="https://github.com/user-attachments/assets/a4db1924-6920-4775-9fed-749cd2938e22">

- 다항 회귀 모델이 단순 선형 회귀 모델보다 좋은 결과를 보인다.
  - 테스트 성능이 높다.(아직 과소적합 경향이 일부 있다.)
  - 학습한 곡선이 데이터에 잘 부합한다.
  - 훈련 셋 범위 밖 샘플에 대해서도 더 정확한 값을 예측한다.
 
## 다항 회귀는 선형 회귀인가?
다항식은 곡선을 표현할 수 있어, 입력 특성과 타겟 값과의 비선형 관계를 표현한다.

"선형"의 의미는 회귀계수에 대해 선형적이라는 의미이다.

다항 회귀는 입 / 출력 관계는 비선형일 수 있지만, 회귀 계수에 대해 선형적인 관계를 유지하므로 "선형 회귀"로 분류된다.

## 마무리
### 키워드로 끝내는 핵심 포인트
#### 선형 회귀(linear regression)
특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾는다.

특성이 하나면 단순 선형 회귀라 하며, 직선으로 표현된다.

선형 회귀가 찾은 특성과 타깃 사이의 관계는 선형 방정식의 계수 또는 가중치에 저장된다.

회귀 모델에서의 가중치는 기울기와 절편을 의미한다.

#### 모델 파라미터(parameters)
머신러닝 알고리즘이 훈련 셋에서 학습하여 얻게 되는 값이다.

단순 선형 회귀 모델의 경우 기울기 및 절편값이다.

#### 다항 회귀(polynomial regression)
다항식을 사용하여 특성과 타깃 사이의 관계를 나타낸다.

선형 회귀에 속하는 방법이다.

### 핵심 패키지와 함수
#### scikit - learn
`LinearRegression`
- 사이킷런의 선형 회귀 클래스이다.

## 확인 문제
### 1번
선형 회귀 모델이 찾은 방정식의 계수를 무엇이라고 부르는가?

1. 회귀 파라미터
2. 선형 파라미터
3. 학습 파라미터
4. 모델 파라미터

### 2번
사이킷런에서 다항 회귀 모델을 훈련할 수 있는 클래스는 무엇인가?

1. LinearRegression
2. PolynomialRegression
3. KNeighborsClassifier
4. PolynomialClassifier
