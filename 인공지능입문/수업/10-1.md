# 데이터 전처리
## 기존 모델의 문제점
기존에 학습한 모델에 대해 예측을 해 보니 오류가 발생했다.

150g, 25cm 도미를 빙어로 예측하는데 그 이유가 뭘까?

## 넘파이로 데이터 준비하기
먼저 도미와 빙어 데이터를 준비해보자.

```
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8,
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]

fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7,
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

### 샘플 데이터 합치기
#### 이전에 사용한 방법

```
fish_data = [[l, w] for l, w in zip(fish_data, fish_weight)]
```

#### 새롭게 사용할 방법(넘파이 이용)

```
import numpy as np
fish_data = np.column_stack((fish_length, fish_weight))

print(fish_data[ : 5])

"""
결과
[[ 25.4 242. ]
 [ 26.3 290. ]
 [ 26.5 340. ]
 [ 29.  363. ]
 [ 29.  430. ]]
"""
```

### 타깃 데이터 합치기
#### 이전에 사용한 방법

```
fish_target = [1] * 35 + [0] * 14
```

#### 새롭게 사용할 방법(넘파이 이용)

```
fish_target = np.concatenate((np.ones(35), np.zeros(14)))

print(fish_target)

"""
결과
[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.
 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0.]
"""
```

`column_stack()`과 `concetenate()`의 동작 방식은 아래와 같다.

<img width="322" alt="image" src="https://github.com/user-attachments/assets/56c24139-1a2a-4d6a-96b0-6fbd68c88522">

- 데이터 크기가 작을 때는 파이썬 리스트를 접합하여 사용할 수 있지만, 데이터 크기가 클 땐 넘파이를 사용하는 것이 효율적이다.
- 핵심 부분이 C, C++과 같은 저수준 언어로 개발되어 빠르다.

## 사이킷런으로 훈련 세트와 테스트 세트 나누기
사이킷런을 사용할 경우 더 쉽게 나눌 수 있다.

model_selection 모듈에 있는 `train_test_split()` 함수를 이용한다.
- 나누고 싶은 리스트나 배열을 인자로 전달한다.
- random_state를 지정하여 항상 같은 결과를 얻도록 한다.

<img width="264" alt="image" src="https://github.com/user-attachments/assets/1fbfd63a-d513-42b6-bde3-f1fc76b1ab5b">

```
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, random_state=42)
```

잘 나누어졌는지 넘파이 배열의 형상을 확인해본다.

```
print(train_input.shape, test_input.shape)
print(train_target.shape, test_target.shape)

"""
결과
(36, 2) (13, 2)
(36,) (13,)
"""
```

도미와 빙어가 잘 섞였는 지 테스트 데이터를 출력해 본다.

```
print(test_target)

"""
결과
[1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
"""
```

- 13개 테스트 세트 중 10개가 도미(1), 3개가 빙어(0)로 3.3 : 1 비율이다.
- 원래 도미와 빙어의 개수가 각각 35, 14개이므로 2.5 : 1 비율이다.
- 샘플링 편향이 일부 나타났음을 보여주는 결과이다.

`train_test_split()` 함수의 `stratify` 매개변수를 활용해 클래스 비율에 맞게 데이터를 나눌 수 있다.
- 클래스 데이터(타깃 데이터)를 인자로 전달한다.

```
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target, random_state=42)

print(test_target)

"""
결과
[0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1.]
"""
```

- 13개 테스트 세트 중 9개가 도미(1), 4개가 빙어(0)로 2.25 : 1 비율이다.
- 완전한 동일 비율을 맞추지는 못했지만, 샘플링 편향이 줄어들었다.

## 새로운 분할 데이터로 모델 학습하기
k - 최근접 이웃 분류기를 학습하고, 점수를 측정한다.

```
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)

"""
결과
1.0
"""
```

- 비율을 고려한 테스트 세트에서도 완벽한 정확도를 보인다.

## 오류 데이터에 대한 예측
분류를 잘 못했던 생선(실제로 도미)을 입력으로 넣어 예측을 수행한다.

```
print(kn.predict([[25, 150]]))

"""
결과
[0.]
"""
```

- 도미 데이터임에도 여전히 빙어(0)로 예측한다.
- 직접 데이터를 살펴보자.

## 데이터 시각화해서 살펴보기

```
import matplotlib.pyplot as plt

plt.scatter(train_input[ : , 0], train_input[ : , 1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<img width="372" alt="image" src="https://github.com/user-attachments/assets/50b4ef0b-24e9-4902-bd34-7d3a12435cc7">

- 예측해야 하는 샘플을 세모로 표시하였다.
- 분명 도미와 유사한 곳에 위치하고 있다.
- 왜 그런데도 모델은 빙어로 판단한 걸까?

## 분류기가 참조한 주변 샘플 살펴보기
k - 최근접 이웃 분류기는 k개 이웃을 참조해 예측을 수행한다.

예측하려고 하는 생선이 참조한 샘플이 무엇이었는지 확인해보자.
- 분류기 객체의 `kneighbors()` 메소드를 이용한다.

```
distances, indexes = kn.kneighbors([[25, 150]])

plt.scatter(train_input[ : , 0], train_input[ : , 1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<img width="577" alt="image" src="https://github.com/user-attachments/assets/46c4169e-92be-4a4c-bce1-165ba72a48f0">

- 눈으로 봤을 땐 도미 쪽에 위치해 보였지만, 참조한 데이터 중 다수가 빙어 샘플이었다.

샘플 특징을 직접 출력하면 이웃 중 대다수가 빙어임을 더 명확하게 확인할 수 있다.

```
print(train_input[indexes])
print(train_target[indexes])

"""
결과
[[[ 25.4 242. ]
  [ 15.   19.9]
  [ 14.3  19.7]
  [ 13.   12.2]
  [ 12.2  12.2]]]
[[1. 0. 0. 0. 0.]]
"""
```

## 이웃으로 판별된 샘플과의 거리 살펴보기
`kneighbors()` 메소드에서 반환한 distances 배열은 예측 대상 샘플과 이웃 샘플들 간의 거리를 나타낸다.

```
print(distances)

"""
결과
[[ 92.00086956 130.48375378 130.73859415 138.32150953 138.39320793]]
"""
```

<img width="278" alt="image" src="https://github.com/user-attachments/assets/fe784463-e039-4d34-90f0-f5b565f77e2c">

- 길이(length)로 측정한 거리와 무게(weight)로 측정한 거리가 눈으로 봤을 때 같은 간격 기준으로 다르게 측정된다.
- x축은 범위가 좁고(10 ~ 40) y축은 범위가 넓다(0 ~ 1000).

## 범위 고정시켜서 시각화하기
앞서 발견한 x축, y축 간격 차이를 명확하게 보기 위해, x축 범위를 고정시켜 출력하자.

```
plt.scatter(train_input[ : , 0], train_input[ : , 1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker='D')
plt.xlim(0, 1000)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<img width="573" alt="image" src="https://github.com/user-attachments/assets/504d30fb-89e8-4f08-b444-df467c317b30">

- 산점도가 한쪽에 쏠린 형태로 나타난다.
- 데이터 범위(scale)가 달라 나타나는 문제이다.
- x축(길이)는 큰 영향을 미치지 못할 것이다.

## 데이터 전처리
데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측할 수 없다.

특히, 거리 기반의 알고리즘(k - 최근접 이웃 등)이 큰 영향을 받는다.

### 데이터 전처리(Data Preprocessing)
실험 전 적절한 형태로 만들어 주는 작업을 의미한다.

### 데이터 정규화(Data Normalization)
특성 값을 일정한 기준으로 맞춰주는 작업으로 데이터 전처리에 속한다.

## 표준 점수를 이용한 데이터 정규화
표준점수를 이용해 모든 특징이 평균 0, 분산 1에 가깝게 변환해준다.

<img width="162" alt="image" src="https://github.com/user-attachments/assets/65d6b58d-a99b-49b1-8490-90c602dab597">

```
# 행(axis = 0) 기준으로 함수를 적용한다.
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)

print(mean, std)

train_scaled = (train_input - mean) / std

"""
결과
[ 27.29722222 454.09722222] [  9.98244253 323.29893931]
"""
```

## 전처리 적용 후 시각화
표준 점수를 적용한 데이터와 함께 예측 실패 도미 샘플을 산점도로 그려보자.

```
plt.scatter(train_scaled[ : , 0], train_scaled[ : , 1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<img width="582" alt="image" src="https://github.com/user-attachments/assets/8ec6b7a8-730d-4e2f-aaa4-55343f7d020b">

- 그림 패턴이 이상한 이유가 뭘까?

## 새로운 샘플에 대한 전처리
예측 실패한 도미 샘플은 표준 점수 전처리가 적용되지 않아 발생하는 문제이다.

표준 점수를 적용해 전처리하자.

유의할 점
- 훈련 세트를 사용할 때 계산한 평균, 표준편차 값을 사용해야 한다.

```
new = ([25, 150] - mean) / std

plt.scatter(train_scaled[ : , 0], train_scaled[ : , 1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<img width="573" alt="image" src="https://github.com/user-attachments/assets/92be7813-8a17-4a96-8c0a-39cc554869b4">

- 산점도의 x, y축 범위가 바뀐 것을 알 수 있다.

## 전처리 완료 데이톨 훈련 및 예측
예측 실패한 샘플을 드디어 도미로 잘 예측한다.

```
kn.fit(train_scaled, train_target)
test_scaled = (test_input - mean) / std

kn.score(test_scaled, test_target)

print(kn.predict([new]))

"""
결과
1.0

[1.]
"""
```

```
distances, indexes = kn.kneighbors([new])

plt.scatter(train_scaled[ : , 0], train_scaled[ : , 1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<img width="576" alt="image" src="https://github.com/user-attachments/assets/4107af45-1a64-4449-b3e3-4abebd4fb96a">

## 특성 전처리의 중요성
많은 머신러닝 모델이 표준점수 등 특성 전처리를 필요로 한다.

전처리시 유의사항은 훈련 세트를 변환할 때 사용한 값을 사용해야 한다는 것이다.

이는 최대 - 최소 정규화 등에도 동일하게 적용되는 원칙이다.

## 마무리
### 키워드로 끝내는 핵심 포인트
#### 데이터 전처리(Data Preprocessing)
머신러닝 모델에 훈련 데이터를 주입하기 전에 가공하는 단계를 말하며, 많은 시간이 소요된다.

#### 표준 점수(Z - Score)
훈련 세트의 스케일을 바꾸는 대표적인 방법 중 하나이다.

특성의 평균을 빼고 표준편차로 나눈다.

테스트 세트 변환 시 훈련 세트에서 얻은 평균 및 표준 편차 값을 사용한다.

### 핵심 패키지와 함수
#### scikit - learn
`train_test_split()`
- 데이터를 훈련 세트와 테스트 세트로 나누는 함수이다.
- 테스트 세트 비율은 test_size 매개변수로 설정 가능하며, 기본값은 0.25이다.
- stratify 매개변수에 클래스 레이블이 담긴 배열을 전달하면 비율을 맞춰 분할한다.

`kneighbors()`
- k - 최근접 이웃 객체 메소드로, 입력한 샘플에 가장 가까운 이웃을 찾아 거리와 샘플 index를 반환한다.

## 확인 문제
### 1번
이 방식은 스케일 조정 방식의 하나로 특성값을 0에서 표준편차의 몇 배수만큼 떨어져 있는지로 변환한 값이다.

이 값을 무엇이라 하는가?

1. 기본 점수
2. 원점수
3. 표준점수
4. 사분위수

### 2번
테스트 세트의 스케일을 조정하려고 한다.

다음 중 어떤 데이터의 통계값을 사용해야 하는가?

1. 훈련 세트
2. 테스트 세트
3. 전체 데이터
