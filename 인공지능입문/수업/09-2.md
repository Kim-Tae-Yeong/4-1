# 훈련 세트와 테스트 세트
## 지도 학습과 비지도 학습
머신러닝은 크게 지도 학습, 비지도 학습, 강화 학습으로 나뉜다.

앞서 구현한 k - 최근접 이웃 방법은 지도 학습 방법 중 하나로, 데이터와 정답을 이용해 각 샘플의 특성으로부터 타깃을 예측하는 방법을 학습한다.

훈련에 사용된 입력(input), 타깃(targit) 데이터를 합쳐서 훈련 데이터(training data)라고 부른다.

지도 학습은 정답(타깃)이 있으니 알고리즘이 정답을 맞출 수 있도록 한다.(도미 vs 빙어)

비지도 학습 알고리즘은 타깃 없이 입력 데이터만 사용한다.
- 정답이 없으니 맞출 수 없고, 데이터를 잘 파악(군집화)하거나 변형(차원 축소)하는데 도움을 준다.

## 훈련 세트와 테스트 세트
중간고사를 보기 전에 모든 시험 문제와 정답을 알려주고 시험을 본다면, 문제와 정답을 모두 외워 100점을 맞을 수 있다.

머신러닝도 마찬가지로, 훈련한 데이터로 테스트하면 모두 맞추는 것이 당연하다.

연습 문제와 시험 문제가 달라야 하는 것 처럼, 머신러닝 모델도 훈련용 데이터와 평가용 데이터가 달라야 한다.

평가에 사용하는 데이터를 테스트 세트(test set), 훈련에 사용하는 데이터를 훈련 세트(train set)라 한다.
- 훈련에 사용한 데이터는 평가에 사용되지 않아야 한다.

### 데이터 세트 준비
도미와 빙어 데이터를 합쳐 파이썬 리스트로 준비한다.
```
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]

fish_data = [[l, w] for l, w in zip(length, weight)]
fish_target = [1] * 35 + [0] * 14
```

### 훈련 및 테스트 세트 준비
하나의 생선 데이터를 샘플(sample)이라고 부른다.

도미와 빙어가 35, 14마리 있으므로 총 샘플은 49개이다.

일반적으로, 주어진 데이터를 분할하여 훈련 세트와 테스트 세트로 나눈다.

첫 35개를 훈련 세트로, 나머지 14개를 테스트 세트로 사용하자.

<img width="305" alt="image" src="https://github.com/user-attachments/assets/5ddb11f3-f290-4f3a-a36b-07e7095fe08c">

```
train_input = fish_data[ : 35]
train_target = fish_target[ : 35]

test_input = fish_data[35 : ]
test_target = fish_target[ : 35]
```

### K - 최근접 이웃 분류기 학습 및 평가
훈련 데이터로 모델을 학습하고(데이터 보관), 테스트 데이터에 대해 성능을 구해보자.
- `.fit(train_input, train_target)`
  - 훈련 셋으로 학습한다.
 
- `.score(test_input, test_target)`
  - 테스트 셋으로 평가
 
```
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn = kn.fit(train_input, train_target)

kn.score(test_input, test_target)

"""
결과
0.0
"""
```

정확도가 0인 이유가 뭘까?

### 샘플링 편향(Sampling Bias)
k - 최근접 이웃 분류기는 훈련 세트에 도미 데이터만 가지고 있으니, 모든 샘플을 도미로 예측할 것이다.

그런데 테스트 세트에는 빙어만 있으니, 모든 예측이 틀린다.

훈련 세트와 테스트 세트에 각 클래스 별 샘플이 골고루 들어가 있어야 한다.

샘플링 편향이란 클래스 별 샘플이 골고루 들어가 있지 않은 경우를 말한다.

<img width="655" alt="image" src="https://github.com/user-attachments/assets/7e61b9a1-2b1b-4ca0-abe5-165b1b767efb">

### 넘파이 사용하기
다차원 데이터를 축에 따라 보관할 수 있도록 한다.

리스트로 보관한 데이터를 넘파이로 변환해보자.

```
import numpy as np

input_arr = np.array(fish_data)
target_arr = np.array(fish_target)

print(input_arr)

"""
결과
[[  25.4  242. ]
 [  26.3  290. ]
 [  26.5  340. ]
 [  29.   363. ]
 [  29.   430. ]
 [  29.7  450. ]
 [  29.7  500. ]
 [  30.   390. ]
 [  30.   450. ]
 [  30.7  500. ]
 [  31.   475. ]
 [  31.   500. ]
 [  31.5  500. ]
 [  32.   340. ]
 [  32.   600. ]
 [  32.   600. ]
 [  33.   700. ]
 [  33.   700. ]
 [  33.5  610. ]
 [  33.5  650. ]
 [  34.   575. ]
 [  34.   685. ]
 [  34.5  620. ]
 [  35.   680. ]
 [  35.   700. ]
 ...
 [  12.4   13.4]
 [  13.    12.2]
 [  14.3   19.7]
 [  15.    19.9]]
"""
```

### 데이터 섞고 분할
샘플링 편향이 발생하지 않도록, 랜덤하게 샘플을 선택해 훈련 / 테스트 세트로 나누자.

index(행 번호)를 받아와 무작위로 섞는 방법을 사용한다.

```
np.random.seed(42)

index = np.arange(49)
np.random.shuffle(index)
print(index)

train_input = input_arr[index[ : 35]]
train_target = target_arr[index[ : 35]]

"""
결과
[13 45 47 44 17 27 26 25 31 19 12  4 34  8  3  6 40 41 46 15  9 16 24 33
 30  0 43 32  5 29 11 36  1 21  2 37 35 23 39 10 22 18 48 20  7 42 14 28
 38]
"""
```

### 나눈 데이터 확인하기
훈련 셋, 테스트 셋에 대한 산점도를 그린다.
- 색상은 데이터 셋 종류를 나타낸다.

두 데이터 셋에 도미, 빙어 샘플이 섞여있음을 알 수 있다.

```
test_input = input_arr[index[35 : ]]
test_target = target_arr[index[35 : ]]

import matplotlib.pyplot as plt

plt.scatter(train_input[:, 0], train_input[:, 1])
plt.scatter(test_input[:, 0], test_input[:, 1])
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```

<img width="353" alt="image" src="https://github.com/user-attachments/assets/d512ab35-91be-4242-ab41-8aa8838e53b1">

### 두 번째 머신러닝 프로그램
훈련 셋, 테스트 셋을 랜덤 분할한 후 모델 구축 및 성능을 측정해보자.

```
kn = kn.fit(train_input, train_target)
kn.score(test_input, test_target)

"""
결과
1.0
"""
```

정확도 1.0을 달성했다.

분할된 데이터에 기반하였으므로 신뢰할 수 있는 성능 지표이다.

### 예측 결과와 타깃 실제로 확인해보기
테스트 셋에 대해 정답과 예측 값이 같은 것을 알 수 있다.

```
kn.predict(test_input)

test_target

"""
결과
array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0])

array([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0])
'""
```

## 마무리
### 키워드로 끝내는 핵심 포인트
#### 지도 학습(Supervised Learning)
입력과 타깃을 전달하여 모델을 훈련한 다음 새로운 데이터를 예측하는데 활용한다.

k - 최근접 이웃은 지도 학습 알고리즘 중 하나이다.

#### 비지도 학습(Unsupervised Learning)
타깃 데이터가 없다.

무엇을 예측하는 것이 아니라 입력 데이터에서 어떤 특징을 찾기 위해 사용한다.

#### 훈련 세트(Training Set)
모델을 훈련할 때 사용한다.

보통 훈련 세트가 클 수록 모델 성능이 좋으며, 테스트 세트를 제외한 모든 데이터를 사용한다.

#### 테스트 세트(Test Set)
전체 데이터에서 20 ~ 30%를 테스트 세트로 사용하는 경우가 많다.

전체 데이터가 아주 크다면, 정해진 수를 사용한다.(ex. 10,000 샘플)

### 핵심 패키지와 함수
#### numpy
`seed()`
- 동일한 난수를 생성하기 위한 정수 초기값을 지정한다.

`arange(start, end, step)`
- 일정한 간격의 정수 또는 실수 배열을 만든다.
- 기본 간격은 1이다.

`shuffle()`
- 주어진 배열을 랜덤하게 섞는다.
- 다차원 배열일 경우 첫 번째 축에 대해서만 섞는다.

## 확인 문제
### 1번
머신러닝 알고리즘의 한 종류로서 샘플의 입력과 타깃(정답)을 알고 있을 때 사용할 수 있는 학습 방법은 무엇인가?

1. 지도학습
2. 비지도 학습
3. 차원 축소
4. 강화 학습

### 2번
훈련 세트와 테스트 세트가 잘못 만들어져 전체 데이터를 대표하지 못하는 현상을 무엇이라고 부르는가?

1. 샘플링 오류
2. 샘플링 실수
3. 샘플링 편차
4. 샘플링 편향

### 3번
사이킷런은 입력 데이터(배열)가 어떻게 구성되어 있을 것으로 기대하는가?

1. 행: 특성, 열 : 샘플
2. 행 : 샘플, 열 : 특성
3. 행 : 특성, 열 : 타킷
4. 행 : 타킷, 열 : 특성
