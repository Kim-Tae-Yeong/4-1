# 선형 회귀
## Simple Linear Regression Model
paired dataset (x1, y1), (x2, y2), ..., (xn, yn)이 있다.
- 이 둘의 관계는 어떻게 될까?
- ex.
  - xi : 키, yi : 몸무게
 
yi를 고정된 xi 값에 영향을 받는 random variable 이라고 생각해보자.
- 즉 yi는 일종의 xi의 종속변수이지만, 오차가 존재한다.
- 만약 두 변수가 선형적인 관계를 보인다고 가정한다면(y ~ x) yi는 xi에 대해 아래의 식이 성립한다.
- xi = ß0 + ß1yi + ℇi
  - 여기서 ℇ은 error를 의미한다.
 
### 예
<img width="356" alt="image" src="https://github.com/user-attachments/assets/df5a510d-61d6-4fe6-9c44-154e96d973c8">

실제 관객 수를 y로 표현하여 좌표평면 상에 나타낸다.

<img width="557" alt="image" src="https://github.com/user-attachments/assets/e81a3fad-b96b-460b-8351-bf1074bd38c3">

두 그래프 중 어떤 것이 기존 데이터를 '잘 표현하는가'
- 예측값이 실제값 대비 차이가 많이 나지 않는 그래프이다.

<img width="564" alt="image" src="https://github.com/user-attachments/assets/ea58b75d-4d10-41c2-8b57-578a2babfe79">

## Simple Linear Regression Model
여기에서 ℇi는 error를 의미한다.
- ℇi는 일반적으로 ℇi ~ N(0, δ ^ 2)의 분포를 가지는 gaussian noise라고 가정한다.
- 각각이 독립적인 random variable 이라고 간주하면 아래의 식이 나온다.
- Yi ~ N(ß0 + ß1xi, δ ^ 2)

<img width="351" alt="image" src="https://github.com/user-attachments/assets/1f672a20-1385-4038-9fbc-3c074ac22cde">

parameter ß0는 intercept parameter(절편)라고 부른다.

parameter ß1은 slope parameter(기울기)라고 부른다.

parameter δ ^ 2는 error variance라고 부른다.

이 세 가지는 모두 데이터에서 추정이 가능하다.

## Error Variance δ ^ 2와 Data의 관계
δ ^ 2가 더 작아질수록, data points (xi, yi)들이 y = ß0 + ß1x 직선에 더 가까이 존재한다.

<img width="635" alt="image" src="https://github.com/user-attachments/assets/cf57c433-6a3e-4e8f-bfc9-ef130f8b9893">

## Slope Parameter ß1
linear regression에서 보통 가장 관심을 많이 가지는 parameter이다.
- 이것은 종속변수 y가 x의 변화에 따라서 어떤 움직임을 보이는 지를 나타내기 때문이다.

<img width="223" alt="image" src="https://github.com/user-attachments/assets/3dbd158a-35fc-430a-bae8-cd037bcb3b10">

## Simple Linear Regression Model 정리
yi = ß0 + ß1xi + ℇi

이 모델은 paired data observation (x1, y1), ..., (xn, yn)에 대해서 직선을 fitting 시키는 모델이다.
- the error terms ℇ1, ..., ℇi는 gaussian 분포를 따르는 ~ N(0, δ ^ 2) independent random variable로 생각한다.
- unknown parameter는 3개가 있다.
  - intercept parameter ß0
  - slope parameter ß1
  - error variance δ ^ 2
  - 위의 값들은 데이터를 통해 추정할 수 있다.
 
### 예 : 자동차 공장의 전기 사용량
공장장이 공장의 효율을 확인하기 위해서 매 달 생산한 자동차의 총 가격과 전기 소모량을 비교하고자 한다.

이 경우 선형 모델(y = ß0 + ß1x)을 통해 전기 사용량을 생산된 자동차 총 가격의 함수로 표현할 수 있다.

<img width="318" alt="image" src="https://github.com/user-attachments/assets/938d2bdd-156f-457d-b55b-4c7a6a93b003">

<img width="574" alt="image" src="https://github.com/user-attachments/assets/9ffa86ba-95c3-4c58-899f-f2fc4bd57bc8">

## Regression Line의 Fitting
Fitting의 목적
- regression line y = ß0 + ß1x을 data point와 가장 가까운 형태로 만드는 것이다.
- 가깝다의 정의는 다양할 수 있다.
  - 거리의 절대값
  - 거리의 제곱
  - 등등
 
일반적으로 거리의 제곱 Q를 최소화하는 방식으로 만든다.
- Q = ∑(i = 1 ~ n)(yi - (ß0 + ß1xi)) ^ 2
- 이 것을 least squares fit이라고 한다.

## 예측 함수와 실제값 차이의 사이
예측 함수는 예측값과 실제값 간의 차이를 최소화하는 방향으로 만들어야 한다.

데이터 n개 중 i번째 데이터의 y값에 대한 실제값과 예측값의 차이를 아래와 같이 정의하자.

<img width="71" alt="image" src="https://github.com/user-attachments/assets/6747e690-97e3-46bf-999a-688127a8db4d">

이러한 오차의 선형합을 구하면 무슨 문제가 있을까?
- 오차 값들이 음수와 양수로 나왔을 때 값들 간의 차이가 상쇄되어 0으로 계산될 수 있다.

값의 제곱을 사용하여 오차의 합을 표현해보자.

<img width="433" alt="image" src="https://github.com/user-attachments/assets/35fa1c0b-587f-46d4-8737-9050f15f9745">

같은 식을 행렬로 표현해보자.

<img width="269" alt="image" src="https://github.com/user-attachments/assets/c89288bd-b2d7-47a1-a7e1-af4c5b044c3f">

## Least Squares Fit(최소자승법)
즉, 거리의 제곱 Q = ∑(i = 1 ~ n)(yi - (ß0 + ß1xi)) ^ 2을 최소화 해야 한다.

Q는 아래와 같이 표현할 수도 있다.

<img width="181" alt="image" src="https://github.com/user-attachments/assets/6d38c56b-ad7e-457b-9e84-1659b77473c0">

### 거리가 아니라 거리의 제곱을 최소화하는 이유
- 거리는 음수가 나온다.(하지만 절대값이 사용 가능하다.)
- 편차를 고려하여 모든 점에서 적절하게 좋은 parameter를 고를 수 있다.
- 또한 이 값을 최소화하면 maximum likelihood estimation으로 구한 값과 같아진다.

## Maximum Likelihood Estimation
likelihood function이라는 것을 아래와 같의 정의한다.
- L(x1, ..., xn, θ) = f(x1, θ) x ... x f(xn, θ)
- f(x, θ)는 임의의 확률 밀도 함수라고 생각해보자.
- x1, ..., xn은 f(x, θ)에서 sampling한 dataset이라고 생각해보자.
- θ는 우리가 아직 알지 못하는 미지의 parameter이다.

maximum likelihood estimation(MLE : 최대우도추정)
- 위의 L 값을 최대화시키는 point estimation θ를 찾아내서 point estimation을 추정하는 것이다.

parameter가 여러개면 f(xi, θ) -> f(xi, θ1, θ2)가 된다.

### Log - Likelihood
log[L(x1, ..., xn, θ)] = log(f(x1, θ)) + ... + log(f(xn, θ))

f(xi, θ) > 0인 경우, f(x)를 최대화하는 X와 log(f(x))를 최대화하는 X는 같다.

## MLE와의 관계
<img width="502" alt="image" src="https://github.com/user-attachments/assets/519a29b9-0d49-4135-9d86-e0749e467b3b">

나머지는 모두 상수이므로, ∑(i = 1 ~ n)ℇi ^ 2를 최소화한다면 joint density function이 최대화가 된다.(negative 이므로)
- 즉, ∑ℇi ^ 2 = ∑(yi - (ß0 + ß1xi)) ^ 2 = Q를 최소화하면 된다.

## Fitting the Line
Q는 ß0, ß1의 2차 함수이므로 그림과 같은 꼴을 가진다.

<img width="137" alt="image" src="https://github.com/user-attachments/assets/08995841-8eab-4d57-9c5c-6fb985b0dd06">

컴퓨터로 numerical approximation
- 임의의 함수에 대해 모두 대응이 가능하다.(즉, linear regression이 아니라도 된다.)
- 'approximation'이므로 부정확할 수 있다.(local minima 등)

## 비용함수
비용함수(cost function) : 머신러닝에서 최소화해야 할 예측값과 실제값의 차이를 말한다.

가설함수(hypothesis function) : 예측값을 예측하는 함수이다.
- f(x) = hθ(x)
- 함수 입력값은 x이고, 함수에서 결정할 것은 매개변수 θ로, 가중치 ß0, ß1을 의미한다.

## 선형회귀의 비용함수
선형회귀에서 비용함수가 두 개의 가중치 값으로 결정된다.

<img width="304" alt="image" src="https://github.com/user-attachments/assets/99bcd258-00b3-4f6a-bb11-6d522c7bcf1a">

- 잔차의 제곱합(error sum of squares) : 예측값인 가설함수와 실제값인 y값 간의 차이를 제곱해서 모두 합한다.
  - 총 데이터는 m개가 존재하고 각 데이터의 예측값과 실제값을 뺀 후 제곱한 값들을 모두 합한 값이다.
 
- 손실함수(loss function) : 비용함수에서 잔차의 제곱합 부분이다.
  - 평균 제곱 오차(mean squared error, MSE) : 잔차의 제곱합을 2m으로 나눈 값이다.
 
비용함수를 수식의 목적이 나타내도록 표기해보자.

<img width="272" alt="image" src="https://github.com/user-attachments/assets/1bbbb251-7d74-49f6-aa9a-776eaec856b7">

비용함수 그래프는 아래와 같은 형태가 될 것이다.

<img width="222" alt="image" src="https://github.com/user-attachments/assets/3d205ca0-3299-47b8-9253-729d511dd9c7">

## Analytic Solution
극값(최소값)을 찾으면 되므로, 아래의 두 방정식을 풀면 된다.

<img width="351" alt="image" src="https://github.com/user-attachments/assets/36c7b1bd-cbe5-48e2-853a-b5679fd5b030">

위의 두 식에서 아래의 두 식을 유도할 수 있음

<img width="338" alt="image" src="https://github.com/user-attachments/assets/9d4a1dcf-cb4e-496c-acd4-29f5e0693e7d">

- 이런 식을 normal equation이라고 부른다.

앞의 식을 정리해서 parameter를 구하면 아래와 같다.

<img width="514" alt="image" src="https://github.com/user-attachments/assets/4ea24278-fdae-451b-871e-97058e90e9b4">

## 행렬식 표현
데이터가 5개만 있다고 가정하고 행렬로 표현해보자.

<img width="347" alt="image" src="https://github.com/user-attachments/assets/2a9f4b4b-48e3-413d-9657-639f1d231468">

<br>

<img width="300" alt="image" src="https://github.com/user-attachments/assets/f261ad27-c8a0-4ce7-bb9c-ebc739028e5c">

## 최소자승법으로 선형회귀 풀기
<img width="311" alt="image" src="https://github.com/user-attachments/assets/871ce1ab-10bf-4e1a-8db2-c473790340f8">

## Analytic Solution
<img width="606" alt="image" src="https://github.com/user-attachments/assets/901fd98f-d24f-462d-a949-bae590a957e1">

## 경사하강법으로 선형회귀 풀기
경사하강법(gradient descent) : 경사를 하강하면서 수식을 최소화하는 매개변수의 값을 찾아내는 방법이다.

<img width="244" alt="image" src="https://github.com/user-attachments/assets/b520ed66-f7b8-4819-85e2-140205542ac0">

<br>

<img width="310" alt="image" src="https://github.com/user-attachments/assets/fc18bb6e-cda3-456d-b001-f1a5088f9038">

점이 최솟값을 달성하는 방향으로 점점 이동시킨다.
- 몇 번 적용할 것인가?
  - 많이 실행할수록 최솟값에 가까워진다.
 
- 한 번에 얼마나 많이 내려갈 것인가?
  - 한 번에 얼마나 많은 공간을 움직일지를 기울기, 즉 경사라고 부른다.
    - 경사(gradient) : 경사하강법의 하이퍼 매개변수
      - 하이퍼 매개변수(hyper parameter)
        - 학습 프로세스의 세부 사항을 지정하는 매개변수이다.
        - 학습률, 옵티마이저 선택 등등
       
### 경사하강법 알고리즘
f(x)는 최소화시켜야 하는 값이라고 생각해보자.

<img width="193" alt="image" src="https://github.com/user-attachments/assets/36c23251-8960-4cb5-9df1-0d8b47498e5d">

경사만큼의 변화가 계속 x에 적용되어 x의 최솟값을 찾는다.

반복적으로 미분 값을 적용시키면서 더 이상 값이 변하지 않거나 변화가 미미해지는 지점까지 값이 줄어든다.

### 경사하강법에서 개발자가 결정해야 할 것
학습룰(learning rate)를 얼마로 할 것인가?
- ɑ 값을 결정해야 한다.

<img width="204" alt="image" src="https://github.com/user-attachments/assets/e755d6d6-4f68-4817-8f13-c0944021e59b">

- 반복이 수행될 때마다 최솟값이 변화한다.
- 값이 너무 작으면 충분히 많은 반복을 적용해도 원하는 최적값을 찾지 못하는 경우가 발생한다.
- 값이 너무 크면 발산하여 최솟값으로 수렴하지 않거나 시간이 너무 오래 걸린다.

얼마나 많은 반복(iteration)으로 돌릴 것인가?
- 반복 횟수가 충분하지 않다면 최솟값을 찾지 못하는 경우가 발생한다.
- 반복 횟수가 너무 많다면 필요 없는 시간을 허비할 수도 있다.

<img width="505" alt="image" src="https://github.com/user-attachments/assets/4a7a34ac-fbf3-4110-b49f-68595afc637b">

## 경사하강법으로 선형회귀 풀기
비용함수 J를 최소화하는 방향으로 학습을 실행하고 그 때의 w0(혹은 ß0)와 w1(혹은 ß1)을 찾아야 한다.

<img width="312" alt="image" src="https://github.com/user-attachments/assets/3bbf68f1-20e9-467c-abfc-c582efe71890">

w 변수가 두 개이기 때문에 3차원 그래프로 J를 표현한다.

<img width="297" alt="image" src="https://github.com/user-attachments/assets/c232e469-b29d-4e75-a91c-90da628401c7">

### 매개변수 w0(혹은 ß0)와 w1(혹은 ß1)에 대한 업데이트
- 먼저 임의의 w0와 w1을 정해 초기화한다.
- 비용함수 J가 최소화 될 때까지 반복을 수행시켜 w0와 w1을 학습시킨다.
- 비용함수가 더 이상 줄어들지 않거나 정해진 반복 조건이 만족되면 학습이 끝난다.

<img width="512" alt="image" src="https://github.com/user-attachments/assets/af95952c-1a2b-4b64-8427-094b9030084e">

- w0와 w1 업데이트는 동시에 일어나 하나의 반복 안에서는 서로의 값에 영향을 주지 않는다.
