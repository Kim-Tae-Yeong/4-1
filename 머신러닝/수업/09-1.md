# 선형 회귀 심화
## 전체 - 배치 경사하강법(Full - Batch g.d. / Batch g.d.)
모든 데이터를 한 번에 입력하는 경사하강법이다.
- 배치(batch)
  - 하나의 데이터셋이다.
- 뒤에 이야기할 mini - batch와는 다르다.

전 주에 이야기한 경사하강법은 하나의 값에 대한 경사도를 구한 다음 값들을 업데이트했다.

<img width="191" alt="image" src="https://github.com/user-attachments/assets/386047ae-592c-412e-b14a-d4c262bf9e40">

실제로는 각 데이터의 경사도를 모두 더해 하나의 값으로 가중치를 업데이트하는 것이 좋다.

점 한 개씩 사용하여 가중치를 업데이트하지 않는 이유는 아래와 같다.
- 시간이 오래 걸린다.
- 시작점에 따라 지역 최적화(local optimum)에 빠진다.
  - 그래프 전체에서 최솟점을 찾지 못하고 부분최솟점에 최적점이 위치한다.
 
<img width="273" alt="image" src="https://github.com/user-attachments/assets/4be15df5-ecc9-4ba7-8195-0b3783394aa9">

### 특징
업데이트 횟수가 감소한다.
- 가중치 업데이트 횟수가 줄어 계산상 효율성이 상승한다.

안정적인 비용함수로 수렴한다.
- 모든 값의 평균을 구하기 때문에 일반적으로 경사하강법이 갖는 지역 최적화 문제를 만날 가능성이 적어진다.

업데이트 속도가 증가한다.
- 대규모 데이터셋을 한 번에 처리하면 모델의 매개변수 업데이트 속도에 문제 발생이 줄어든다.
  - 데이터가 백만 단위 이상을 넘어가면 하나의 머신에서는 처리가 불가능해져서 메모리 문제가 발생한다.
 
## 확률적 경사하강법(Stochastic Gradient Decent, SGD)
학습용 데이터에서 샘플들을 랜덤하게 뽑아서 사용한다.
- 대상 데이터를 섞은(shuffle) 후, 일반적인 경사하강법처럼 데이터를 한 개씩 추출하여 가중치를 업데이트한다.

<img width="442" alt="image" src="https://github.com/user-attachments/assets/55e0ddac-ba01-4a30-9b33-064dcf40f5ec">

### 특징
#### SGD의 장점
빈번한 업데이트를 하기 때문에 데이터 분석가가 모델의 성능 변화를 빠르게 확인한다.

데이터의 특성에 따라 훨씬 더 빠르게 결과값을 낸다.

지역 최적화를 회피한다.

#### SGD의 단점
대용량 데이터를 사용하는 경우 시간이 매우 오래 걸린다.

결과의 마지막 값을 확인하기 어렵다.
- 흔히 '튀는 현상'이라고 불리는데 비용함수의 값이 줄어들지 않고 계속 변화할 떼 정확히 언제 루프(loop)가 종료되는지 알 수 없어 판단이 어렵다.

## 미니 - 배치 경사하강법(Mini - Batch Gradient Descent) 또는 미니 - 배치 SGD(Mini - Batch SGD)
데이터의 랜덤한 일부분만 입력해서 경사도 평균을 구해 가중치를 업데이트한다.

에포크(epoch)
- 데이터를 한 번에 모두 학습시키는 횟수이다.
  - 전체 - 배치 SGD를 한 번 학습하는 루프가 실행될 때 1에포크의 데이터가 학습된다고 말한다.

배치 사이즈(batch - size)
- 한 번에 학습되는 데이터의 개수이다.
  - 총 데이터가 5012개 있고 배치 사이즈가 512라면 10번의 루프가 돌면 1에포크를 학습했다고 말한다.

에포크와 배치 사이즈는 하이퍼 매개변수이므로 직접 골라야 한다.

<img width="456" alt="image" src="https://github.com/user-attachments/assets/f4fd9c62-8fb8-4fe1-985c-dce935909e4c">

## 과대적합과 정규화
### 과대적합 극복하기
편항(bias)
- 학습된 모델이 학습 데이터에 대해 만들어 낸 예측값과 실제값과의 차이이다.
  - 모델의 결과가 얼마나 한쪽으로 쏠려 있는지를 나타낸다.
  - 편향이 크면 학습이 잘 진행되기는 했지만 해당 데이터에만 잘 맞는다.
 
분산(variance)
- 학습된 모델이 테스팅 데이터에 대해 만들어 낸 예측값과 실제값과의 차이이다.
  - 모델의 결과가 얼마나 퍼져 있는지 나타낸다.
 
편향 - 분산 트레이드오프(bias - variance trade - off)
- 편향과 분산의 상충 관계이다.

과대적합(overfitting)
- 높은 분산 낮은 편향 상태로 함수가 훈련 데이터셋에만 맞는 것이다.
  - 피쳐의 개수를 줄이거나 정규화하여 해결한다.
 
과소적합(underfitting)
- 낮은 분산 높은 편향 상태로 함수가 훈련 데이터셋과 테스트 데이터셋에 모두 맞지 않는 것이다.
  - 피쳐를 추가하여 해결한다.
 
<img width="264" alt="image" src="https://github.com/user-attachments/assets/2e511ab7-1b07-467a-a64d-a5e47514ba1c">

과대적합이 발생할 때 경사하강법 루프가 진행될수록
- 학습 데이터셋에 대한 비용함수의 값은 줄어들지만
- 테스트 데이터셋의 비용함수 값은 증가한다.

선형회귀 외에도 결정트리(decision tree)나 딥러닝처럼 연산에 루프가 필요한 모든 알고리즘에서 똑같이 발생한다.

<img width="194" alt="image" src="https://github.com/user-attachments/assets/7b79751d-8514-413e-ab3b-a454dcc80411">

오컴의 면도날(Occam's Razor)
- 보다 적은 수의 논리로 설명이 가능한 경우, 많은 수의 논리를 세우지 않는다.
  - '경제성의 원리' 또는 '단순성의 원리'이다.
  - 머신러닝에서는 너무 많은 피쳐를 사용하지 않는 것이다.
 
선형회귀에서 과대적합 해결책은 아래와 같다.
- 더 많은 데이터를 활용한다.
  - 오류가 없고, 분포가 다양한 데이터를 많이 확보한다.
 
- 피쳐의 개수를 줄인다.
  - 필요한 피쳐만 잘 찾아 사용한다.
 
- 적절한 매개변수를 선정한다.
  - SGD의 학습률이나 루프의 횟수처럼 적절한 하이퍼 매개변수를 선정한다.
 
- 정규화를 적용한다.
  - 데이터 편향성에 따라 필요 이상으로 증가한 피쳐의 가중치 값을 적절히 줄이는 규제 수식을 추가한다.
 
## Norm
선형대수학에서 벡터가 얼마나 큰지를 알려주는 값이다.(magnitude)

V -> R(벡터 공간에서 실수로 매핑하는 함수)에서 아래 성질을 만족해야 한다.

<img width="510" alt="image" src="https://github.com/user-attachments/assets/bd8eefe6-ffb6-4e58-aea0-4a921b69b983">

일반적으로 l<sub>p</sub>norm을 아래와 같이 정의한다.

<img width="208" alt="image" src="https://github.com/user-attachments/assets/2685ac4b-c17e-43e5-b3bd-1f61a6a34179">

## L2 정규화(L2 Regularization) : 리지 회귀(Rigde Regression)
유클리드 공간에서 벡터 크기를 계산한다.

L2 norm을 선형회귀의 비용함수 수식에 적용한다.

<img width="322" alt="image" src="https://github.com/user-attachments/assets/be6b6334-a3f5-4ebe-b4b0-a8d131ec26b3">

뒷 부분에 새로 붙인 수식은 페널티텀(penalty term)이다.
- 모델의 가중치 값들의 제곱의 합이다.
- 가중치 값이 조금이라도 커질 때 비용함수에 매우 큰 영향을 준다.
- λ가 클수록 전체 페널티텀의 값이 커져 θ값이 조절된다.
- λ는 사람이 직접 값을 입력하는 하이퍼 매개변수이다.

리지 회귀 수식을 미분하면 j의 값이 1 이상일 때 페널티가 적용된다.

<img width="634" alt="image" src="https://github.com/user-attachments/assets/19de0e54-3928-4ed1-9337-420bacee1e32">

## L1 정규화(L1 Regularization) : 라쏘 회귀(Lasso Regression)
가중치에 페널티텀을 추가하는데, 기존 수식에다 L1 norm 페널티를 추가하여 계산한다.

<img width="339" alt="image" src="https://github.com/user-attachments/assets/91dcc12a-2c0f-424f-acc9-18397da8d4eb">

L1 norm
- 절대값을 사용하여 거리를 측정한다.

<img width="131" alt="image" src="https://github.com/user-attachments/assets/7fb90b4d-b79e-401f-bbb0-441e01aaf54c">

## L1 정규화와 L2 정규화가 실제 적용되는 과정
타원
- 두 개의 가중치 값의 최적점과 그 가중치 값으로 생기는 비용함수의 공통 범위이다.

아래 마름모나 원
- w가 가질 수 있는 범위이고 타원과 만나는 점이 바로 사용가능한 가중치 값이다.

<img width="306" alt="image" src="https://github.com/user-attachments/assets/628e7b2a-1053-40fc-a82d-f669cb15ca43">

## 과대적합과 정규화
L1 정규화는 직선과 타원 만나는 점이 양쪽 끝에 생성된다.
- 극단적인 값이 생성되어 다른 가중치 값이 선택되지 않는 현상이 발생할 수 있다.
- 사용해야 하는 피쳐와 사용하지 않아도 되는 피쳐를 선택하여 사용하도록 지원한다.

L2 정규화는 원과 타원이 만나는 점이 많아져서 비교적 쉽게 연산되어 계산 효율(computational efficiency)을 확보한다.
- 한 점에서 만나기 때문에 하나의 해답만 제공한다.
