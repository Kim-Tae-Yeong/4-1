# 선형 회귀
## 경사하강법(Gradient Desenct) Remind
경사를 하강할수록 수식을 최소화하는 매개변수의 값을 찾아내는 방법이다.

점이 최솟값을 달성하는 방향으로 점점 내려간다.
- 몇 번 적용할 것인가?
  - 많이 실행할수록 최솟값에 가까워진다.
  - 다만 시간이 더 많이 걸린다.
 
- 한 번에 얼마나 많이 내려갈 것인가?
  - 한 번에 얼마나 많은 공간을 움직일 것인가?
 
- 경사(gradient)
  - 경사하강법의 하이퍼 매개변수
 
### 경사하강법의 변형
#### Full Batch
전체 데이터를 한 번에 사용하여 모델의 가중치를 업데이트하는 방식이다.

#### Mini - Batch
데이터를 조각조각내 일부씩 사용하여 모델의 가중치를 업데이트하는 방식이다.

#### Stochastic
데이터를 셔플링해서(혹은 확률적으로 샘플링해서) 하나씩 사용하여 모델의 가중치를 업데이트하는 방식이다.

### 예
```
# 한 번에 되는 것으로 만들어 보자.
class LinearRegressionGD(object): 
    def __init__(self, fit_intercept=True, copy_X=True,
                 eta0=0.001, epochs=1000, batch_size = 1, 
                 weight_decay=0.9, shuffle = True):
        self.fit_intercept = fit_intercept
        self.copy_X = copy_X
        self._eta0 = eta0
        self._epochs = epochs

        self._cost_history = []

        self._coef = None
        self._intercept = None
        self._new_X = None
        self._w_history = None
        self._weight_decay = weight_decay
        self._batch_size = batch_size
        self._is_SGD = shuffle

    def gradient(self, X, y, theta):
        return X.T.dot(self.hypothesis_function(X, theta)-y) / len(X)

    def fit(self, X, y):
        self._new_X = np.array(X)  # X 데이터 할당
        y = y.reshape(-1, 1)

        if self.fit_intercept:    # intercept 추가 여부
            # 1로만 구성된 상수항을 모든 데이터를 추가
            intercept_vector = np.ones([len(self._new_X), 1])
            self._new_X = np.concatenate(
                    (intercept_vector, self._new_X), axis=1)

        theta_init = np.random.normal(0, 1, self._new_X.shape[1])         
        # weight값 초기화
        self._w_history = [theta_init]
        self._cost_history = [self.cost(
                        self.hypothesis_function(self._new_X, theta_init), y)]

        theta = theta_init

        for epoch in range(self._epochs): # 지정된 epoch의 값만큼 학습 실행
            X_copy = np.copy(self._new_X)

            if self._is_SGD:   # stochastic 적용 여부
                np.random.shuffle(X_copy)

            batch = len(X_copy) // self._batch_size 
            # batch 사이즈를 기준으로 전체데이터를 나눔

            for batch_count  in range(batch):
                X_batch = np.copy(   # BATCH 사이즈를 기준으로 데이터를 slice
                            X_copy[batch_count * self._batch_size : (batch_count+1) & self._batch_size])

                gradient = self.gradient(X_batch  , y, theta).flatten()
                theta = theta - self._eta0 * gradient

            if epoch % 100 == 0:
                self._w_history.append(theta)
                cost = self.cost(
                    self.hypothesis_function(self._new_X, theta), y)
                self._cost_history.append(cost)
            self._eta0 = self._eta0 * self._weight_decay

        if self.fit_intercept:
            self._intercept = theta[0]
            self._coef = theta[1:]
        else:
            self._coef = theta
    def cost(self, h, y):
        return 1/(2*len(y)) * np.sum((h-y).flatten() ** 2)

    def hypothesis_function(self, X, theta):
        return X.dot(theta).reshape(-1, 1) # 선형회귀니까요.

    def gradient(self, X, y, theta):
        return X.T.dot(self.hypothesis_function(X, theta)-y) / len(X)

    def fit(self, X, y):
        self._new_X = np.array(X)
        y = y.reshape(-1, 1)

        if self.fit_intercept:
            intercept_vector = np.ones([len(self._new_X), 1])
            self._new_X = np.concatenate(
                    (intercept_vector, self._new_X), axis=1)

        theta_init = np.random.normal(0, 1, self._new_X.shape[1])
        self._w_history = [theta_init]
        self._cost_history = [self.cost(
                        self.hypothesis_function(self._new_X, theta_init), y)]

        theta = theta_init

        for epoch in range(self._epochs):
            gradient = self.gradient(self._new_X, y, theta).flatten()
            theta = theta - self._eta0 * gradient

            if epoch % 100 == 0: # epoch 100마다 cost function 기록
                self._w_history.append(theta)
                cost = self.cost(
                    self.hypothesis_function(self._new_X, theta), y)
                self._cost_history.append(cost)
            self._eta0 = self._eta0 * self._weight_decay 

        if self.fit_intercept:
            self._intercept = theta[0]
            self._coef = theta[1:]
        else:
            self._coef = theta

    def predict(self, X):
        test_X = np.array(X)

        if self.fit_intercept:
            intercept_vector = np.ones([len(test_X), 1])
            test_X = np.concatenate(
                    (intercept_vector, test_X), axis=1)

            weights = np.concatenate(([self._intercept], self._coef), axis=0)
        else:
            weights = self._coef

        return test_X.dot(weights)

    @property
    def coef(self):
        return self._coef

    @property
    def intercept(self):
        return self._intercept

    @property
    def weights_history(self):
        return np.array(self._w_history)

    @property
    def cost_history(self):
        return self._cost_history
```

train file을 불러와서 모두 10000 에포크씩 돌려 fitting을 해보자.

```
import pandas as pd
import numpy as np

df = pd.read_csv("./W09/train.csv")

X = df["x"].values.reshape(-1,1)
y = df["y"].values

gd_lr = LinearRegressionGD(eta0=0.001, epochs=10000, batch_size=1, shuffle=False)
bgd_lr = LinearRegressionGD(eta0=0.001, epochs=10000, batch_size=len(X), shuffle=False)
sgd_lr = LinearRegressionGD(eta0=0.001, epochs=10000, batch_size=1, shuffle=True)
msgd_lr = LinearRegressionGD(eta0=0.001, epochs=10000, batch_size=100, shuffle=True)

gd_lr.fit(X, y)
bgd_lr.fit(X, y)
sgd_lr.fit(X,y)
msgd_lr.fit(X,y)
```

```
import matplotlib.pyplot as plt

res = 100 # 100 epoch마다 기록했으므로.

plt.plot(np.arange(len(gd_lr.cost_history))*res, gd_lr.cost_history, c="r", label="GD")
plt.plot(np.arange(len(bgd_lr.cost_history))*res, bgd_lr.cost_history, c="y", label="Full batch")
plt.plot(np.arange(len(sgd_lr.cost_history))*res, sgd_lr.cost_history, c="g", label="Stochastic")
plt.plot(np.arange(len(msgd_lr.cost_history))*res, msgd_lr.cost_history, c="b", label="Mini-batch")

plt.xlabel("Epochs")
plt.ylabel("Cost")
plt.legend()
#plt.yscale("log")
plt.xscale("log")
```

<img width="576" alt="image" src="https://github.com/user-attachments/assets/9ab11686-8564-4534-9ec2-cb2881eacb79">

## Scikit - Learn으로 선형 회귀 구현하기
기본적으로 아래의 함수들이 존재한다.
- Linear Regression
  - least - squared로 계산한다.
 
- Lasso
  - L1 loss 보정을 한 라쏘 알고리즘으로 계산한다.
 
- Ridge
  - L2 loss 보정을 한 리지 알고리즘으로 계산한다.
 
- SDG Regressor
  - stochaastic gd을 사용한 회귀 모형을 만든다.
 
```
import matplotlib.pyplot as plt
import numpy as np

data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# x 데이터
data

"""
결과
array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,
        4.9800e+00],
       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,
        9.1400e+00],
       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,
        4.0300e+00],
       ...,
       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,
        5.6400e+00],
       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,
        6.4800e+00],
       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,
        7.8800e+00]])
"""
```

```
# (506,) -> (506,1)로 shape을 바꿔주자.
target = target.reshape(target.size, 1) 

# feature는 13개가 존재한다.
data.shape

"""
결과
(506, 13)
"""
```

```
# minmax scaling을 해준다.
from sklearn import preprocessing 

minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,5)).fit(data) # (1)
x_scaled_data = minmax_scale.transform(data) # (2)

x_scaled_data[:3]

"""
결과
array([[0.00000000e+00, 9.00000000e-01, 3.39076246e-01, 0.00000000e+00,
        1.57407407e+00, 2.88752635e+00, 3.20803296e+00, 1.34601570e+00,
        0.00000000e+00, 1.04007634e+00, 1.43617021e+00, 5.00000000e+00,
        4.48399558e-01],
       [1.17961270e-03, 0.00000000e+00, 1.21151026e+00, 0.00000000e+00,
        8.64197531e-01, 2.73998850e+00, 3.91349125e+00, 1.74480990e+00,
        2.17391304e-01, 5.24809160e-01, 2.76595745e+00, 5.00000000e+00,
        1.02235099e+00],
       [1.17848872e-03, 0.00000000e+00, 1.21151026e+00, 0.00000000e+00,
        8.64197531e-01, 3.47192949e+00, 2.99691040e+00, 1.74480990e+00,
        2.17391304e-01, 5.24809160e-01, 2.76595745e+00, 4.94868627e+00,
        3.17328918e-01]])
"""
```

```
# 학습데이터와 테스트 데이터를 나눈다.
from sklearn.model_selection import train_test_split 

# X 데이터의 학습 데이터셋, X 데이터의 테스트 데이터셋
# Y 데이터의 학습 데이터셋, Y 데이터의 테스트 데이터셋
X_train, X_test, y_train, y_test = train_test_split(x_scaled_data, target, test_size=0.33)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""
결과
((339, 13), (167, 13), (339, 1), (167, 1))
"""
```

```
from sklearn import  linear_model

regr = linear_model.LinearRegression(fit_intercept=True, copy_X=True, n_jobs=8) 
lasso_regr = linear_model.Lasso(alpha=0.01, fit_intercept=True, copy_X=True)
ridge_regr = linear_model.Ridge(alpha=0.01, fit_intercept=True, copy_X=True)
SGD_regr = linear_model.SGDRegressor(penalty="l2", alpha=0.01, max_iter=1000, tol=0.001, eta0=0.01)

# 각 모델을 fitting 해보자.
regr.fit(X_train, y_train) 
lasso_regr.fit(X_train, y_train)
ridge_regr.fit(X_train, y_train)
SGD_regr.fit(X_train, y_train.reshape(-1))
```

<img width="223" alt="image" src="https://github.com/user-attachments/assets/872c804b-cc75-434c-9e6f-e4ab0aea4e4a">

```
print('Coefficients: ', regr.coef_)
print('intercept: ', regr.intercept_)

regr.predict(X_train[:5])

X_train[:5].dot(regr.coef_.T) + regr.intercept_

"""
결과
Coefficients:  [[-1.85653356  0.37781436 -0.22863511  0.1001391  -1.30829423  4.80641884
  -0.44687927 -2.89438669  0.85077369 -1.12607492 -1.98418314  0.62377165
  -2.58090746]]
intercept:  [25.60741192]

array([[22.63039049],
       [17.02052097],
       [15.08823078],
       [26.58967411],
       [24.86953378]])

array([[22.63039049],
       [17.02052097],
       [15.08823078],
       [26.58967411],
       [24.86953378]])
"""
```

```
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

y_true = y_test.copy()
y_hat_regr = regr.predict(X_test)
y_hat_lasso = lasso_regr.predict(X_test)
y_hat_ridge = ridge_regr.predict(X_test)
y_hat_sgd = SGD_regr.predict(X_test)

print("linear regression:", r2_score(y_true, y_hat_regr), mean_absolute_error(y_true, y_hat_regr), mean_squared_error(y_true, y_hat_regr), "\n", 
      "lasso regression:", r2_score(y_true, y_hat_lasso), mean_absolute_error(y_true, y_hat_lasso), mean_squared_error(y_true, y_hat_lasso), "\n",
      "ridge regression:", r2_score(y_true, y_hat_ridge), mean_absolute_error(y_true, y_hat_ridge), mean_squared_error(y_true, y_hat_ridge), "\n",
      "sgd regression:", r2_score(y_true, y_hat_sgd), mean_absolute_error(y_true, y_hat_sgd), mean_squared_error(y_true, y_hat_sgd), )

"""
결과
linear regression: 0.6061151550894475 3.9724358967457962 39.42664029351168 
 lasso regression: 0.6045421749404153 3.9757691682674454 39.5840906837109 
 ridge regression: 0.6061118833351808 3.972433915421922 39.426967785874645 
 sgd regression: 0.5591530460074958 4.182376040959515 44.12740044238016
"""
```

```
# 그림을 그려보자.
ps = 30
alp = 0.5
plt.scatter(y_true, y_hat_regr, s=ps, alpha=alp, marker='^', label="LR")
plt.scatter(y_true, y_hat_lasso, s=ps, alpha=alp, marker="x", label="LASSO")
plt.scatter(y_true, y_hat_ridge, s=ps, alpha=alp, marker="+", label="RIDGE")
plt.scatter(y_true, y_hat_sgd, s=ps, alpha=alp, label="SGD")
plt.xlabel("Prices: $Y_i$")
plt.ylabel("Predicted prices: $\\hat{Y}_i$")
plt.title("Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$")
plt.legend()

# SGD를 제외하고 거의 비슷한 결과임을 알 수 있다.
# SGD는 파라미터를 바꾸고 학습을 더 하면 더 정확해질 수 있다.
```

<img width="583" alt="image" src="https://github.com/user-attachments/assets/303dcf3b-20c2-4753-8073-3a5e38d10086">

## 최소자승법(Least Squares Fitting)

```
import numpy as np
import pandas as pd
import random
from matplotlib import pyplot as plt

# 미국질병통제예방센터(Disease Control and Prevention, CDC)는 1973년부터 가족성장 국가조사(National Survey of Family Growth, 이하 NSFG)라는 조사를 수행하고 있다.
# NSFG는 가족생활, 결혼 및 이혼, 임신, 출산, 피임, 그리고 남녀 건강 등을 다루는 데이터이다.
# 이 중 2002년1월에서 2003년 3월까지 수행된 6번째 데이터셋(2002FemPreg.dat)을 가져와 보자.

preg = pd.read_csv("./W09/2002FemPreg.tsv", sep = "\t")
live = preg[preg.outcome == 1].dropna(subset=["agepreg", "totalwgt_lb"])
firsts = live[live.birthord == 1]
others = live[live.birthord != 1]
ages = live.agepreg
weights = live.totalwgt_lb
```

```
def LeastSquares(xs, ys):
    meanx = np.mean(xs)
    varx = np.var(xs)
    meany = np.mean(ys)
    cov = np.cov(xs, ys)
    
    slope = cov[0, 1] / varx
    inter = meany - slope * meanx
    return inter, slope

# 실제로 절편(intercept)과 기울기(slope)를 구해보자.
inter, slope = LeastSquares(ages, weights)
inter, slope

"""
결과
(6.830348812252702, 0.01745578284852866)
"""
```

```
# intercept는 independent variable (x, 여기서는 age)가 0일 때 y축과 만나는 점이다.
# 하지만 independent variable의 평균에 대해서 값을 보는 것이 더 적절할 수 있다.
print(np.mean(ages))
print(inter + slope*np.mean(ages))

# slope는 나이가 1년 단위로 보면 너무 작아서 이해가 힘들 수도 있습니다.
# 나이 10년 차이가 얼마나 차이를 보일까요?
slope * 10

"""
결과
24.936128568267314
7.265628457623368

0.17455782848528661
"""
```

```
# 실제 그림을 그려보자.
# 아래의 함수는 입력된 xs 값에 대해서 y의 fitted 값을 리턴해주는 함수이다.
def FitLine(xs, inter, slope):
    fit_xs = np.sort(xs)
    fit_ys = inter + slope * fit_xs
    return fit_xs, fit_ys

fit_xs, fit_ys = FitLine(ages, inter, slope)

plt.plot(fit_xs, fit_ys, color="red")
plt.scatter(ages, weights, s=4, alpha=0.2)
plt.xlabel("Mother's age")
plt.ylabel("Birth weight (lbs)")
plt.show()
```

<img width="579" alt="image" src="https://github.com/user-attachments/assets/a00366e8-e727-4736-b29f-07ba24b35bef">

## Residual
residual은 y의 실제 관측값과 fitted line에서 추정한 값의 차이를 의미한다.

```
def Residuals(xs, ys, inter, slope):
    xs = np.asarray(xs)
    ys = np.asarray(ys)
    res = ys - (inter + slope * xs)
    return res

# Dataframe의 columns는 아래와 같이 추가할 수도 있다.
live['residual'] = Residuals(ages, weights, inter, slope)

# residual을 산모의 나이 별로 구분하고, percentile plot을 그려보자.
bins = np.arange(10, 48, 3)
indices = np.digitize(live.agepreg, bins)
groups = live.groupby(indices)

age_means = [group.agepreg.mean() for _, group in groups][1:-1]
age_means

"""
결과
[15.212333333333335,
 17.740359281437126,
 20.506304824561404,
 23.455752212389378,
 26.435156146179406,
 29.411177432542924,
 32.30232530120482,
 35.240273631840786,
 38.10876470588235,
 40.91205882352941]
"""
```

```
def values2cdf(values):
    sorted_values = sorted(values)
    total = len(values)
    x = []
    y = []
    for i, value in enumerate(sorted_values):
        x.append(value)
        y.append(i/total)
    return x, y

cdfs = [values2cdf(group.residual) for _, group in groups][1:-1]

def PlotPercentiles(age_means, cdfs):
    for percent in [75, 50, 25]:
        weight_percentiles = [cdf[0][int(percent * len(cdf[0]) / 100)] for cdf in cdfs]
        label = '%dth' % percent
        print(weight_percentiles)
        plt.plot(age_means, weight_percentiles, label=label)

PlotPercentiles(age_means, cdfs)
plt.xlabel("Mother's age")
plt.ylabel("Residual (lbs)")
plt.legend()
plt.show()

"""
결과
[0.7276458738157494, 0.7814262797937532, 0.7754913136252535, 0.9280880453304636, 0.8786881798691271, 0.8916137565793054, 0.9338650484750124, 0.9235744339966478, 0.7622664056717081, 0.6313397369054758]
[-0.10527739211702869, 0.03142627979375323, 0.05465076838456362, 0.12134845367047742, 0.09813226248193452, 0.10390096822421668, 0.09081742849008734, 0.07039416255730835, 0.004934976875325958, -0.32658352902730226]
[-0.9118424259485289, -0.8203575290177039, -0.7506923606475402, -0.7258968516003614, -0.6649595746544623, -0.756610732011505, -0.8597744086463095, -0.7407093748148093, -1.102609240276145, -0.9835359090423772]
"""
```

<img width="582" alt="image" src="https://github.com/user-attachments/assets/85eb12d5-5eae-4670-952e-3975ddf38746">

## Sampling Distribution
inter와 slope의 distribution을 보고 싶을 때는, dataset을 sampling해서 여러 번 테스트 해 보면 된다.
```
# 한번 뽑히면 다시 뽑히지 않는 경우는 SampleRows를 씁니다. 
def SampleRows(df, nrows, replace=False):
    indices = np.random.choice(df.index, nrows, replace=replace)
    sample = df.loc[indices]
    return sample

# 한번 뽑은 것을 다시 뽑는 경우는 ResampleRows를 씁니다. 
def ResampleRows(df):
    return SampleRows(df, len(df), replace=True)

# iter 수 만큼 distribution을 뽑아서, inter와 slope를 구하는 SampleDistributions 함수를 만들어보자.
def SamplingDistributions(live, iters=101):
    t = []
    for _ in range(iters):
        sample = ResampleRows(live)
        ages = sample.agepreg
        weights = sample.totalwgt_lb
        estimates = LeastSquares(ages, weights)
        t.append(estimates)

    inters, slopes = zip(*t)
    return inters, slopes

inters, slopes = SamplingDistributions(live, iters=1001)
```

```
# 요약통계를 내주는 함수를 만들어봅시다.
def Summarize(estimates):
    mean = np.mean(estimates)
    stderr = np.std(estimates)
    cdf = values2cdf(estimates)
    # 90% confidence interval
    ci_90 = cdf[0][int(0.05 * len(cdf[0]))], cdf[0][int(0.95 * len(cdf[0]))] 
    print('mean, SE, CI', mean, stderr, ci_90)

# inters의 요약통계를 내보자.
Summarize(inters)

# inters의 요약통계를 내보자.
Summarize(slopes)

# resample rows로 해보자.
inters, slopes = SamplingDistributions(live, iters=1001)
Summarize(inters)
Summarize(slopes)

"""
결과
mean, SE, CI 6.830824706115656 0.07126696397900134 (6.715866503409505, 6.949154365040428)

mean, SE, CI 0.01742431871698792 0.002817456543847988 (0.012611683070660051, 0.021975664764737543)

mean, SE, CI 6.83034689097765 0.07450598635526329 (6.707801445118039, 6.950693717696134)
mean, SE, CI 0.0174211838399017 0.0029111934565185767 (0.012591362501973031, 0.022128884089459767)
"""
```

## Visualizing Uncertainty
위에서 구한 estimate를 시각화해보자.

```
# 모든 선을 반투명하게 만들어 겹치는 방법으로 쉽게 볼 수 있습니다
for slope, inter in zip(slopes, inters):
    fxs, fys = FitLine(age_means, inter, slope)
    plt.plot(fxs, fys, color='gray', alpha=0.01, linewidth=3)

plt.xlabel("Mother's age")
plt.ylabel("Residual (lbs)")
plt.show()
```

<img width="578" alt="image" src="https://github.com/user-attachments/assets/e23978b0-0bd9-4ca7-99c3-a7a30ed33f4b">

```
def PlotConfidenceIntervals(xs, inters, slopes, percent=90, **options):
    fys_seq = []
    for inter, slope in zip(inters, slopes):
        fxs, fys = FitLine(xs, inter, slope)
        fys_seq.append(fys)
    fys_2darray = np.stack(fys_seq, axis=0)
    p = (100 - percent) / 2
    percents = p, 100 - p
    # 하단 경계값을 찾고
    low = [sorted(fys_2darray[:,i])[int(percents[0] / 100 * len(fys_2darray[:,i]))] for i in range(len(xs))]
    # 상단 경계값을 찾는다.
    high = [sorted(fys_2darray[:,i])[int(percents[1] / 100 * len(fys_2darray[:,i]))] for i in range(len(xs))] 
    plt.fill_between(fxs, low, high, **options)

# 산모의 연령과 아이 몸무게의 Confidence interval들을 그려보자.
# 위 그림과 비슷한 그림이 나온다. 
PlotConfidenceIntervals(age_means, inters, slopes, percent=90, color='gray', alpha=0.3, label='90% CI')
PlotConfidenceIntervals(age_means, inters, slopes, percent=50, color='gray', alpha=0.5, label='50% CI')

plt.xlabel("Mother's age")
plt.ylabel("Residual (lbs)")
plt.legend(loc=2)
plt.show()
```

<img width="581" alt="image" src="https://github.com/user-attachments/assets/8c4b0b88-17d6-4027-8d8c-46ff93aa3a80">

## 결정 계수(Coefficient of Determination)
결정 계수는 residual과 dependent variable의 variance를 비교하는 것이다.

```
def CoefDetermination(ys, res):
    return 1 - np.var(res) / np.var(ys)

inter, slope = LeastSquares(ages, weights)
res = Residuals(ages, weights, inter, slope)
r2 = CoefDetermination(weights, res)
r2

# 사실 R^2값은 pearson correlation의 제곱과 같다.
print('rho', np.corrcoef(ages, weights)[0][1])
print('R', np.sqrt(r2))

# dependent variable과 residual의 standard deviation을 비교해보자.
print('Std(ys)', np.std(weights))
print('Std(res)', np.std(res))

"""
결과
0.0047381154166924455

rho 0.0688339703541091
R 0.06883396993267529

Std(ys) 1.4082155338406195
Std(res) 1.4048754288267307
"""
```

## Hypothesis Testing with Slopes
```
# 공통적으로 많이 사용할 Method들에 대한 HyothesisTest class를 만들어 둔다.
class HypothesisTest(object):
    def __init__(self, data):
        self.data = data
        self.MakeModel()
        self.actual = self.TestStatistic(data)
        self.test_stats = None
        self.test_cdf = None        

    # p-value는 가설에 대해서 현재 관측한 data나 worse data가 나올 확률이다.
    def PValue(self, iters=1000):
        # simulation을 통해서 p-value를 구해보자.
        self.test_stats = [self.TestStatistic(self.RunModel()) 
                           for _ in range(iters)]
        count = sum(1 for x in self.test_stats if x >= self.actual)
        return count / iters

    def TestStatistic(self, data):
        raise UnimplementedMethodException()

    def MakeModel(self):
        pass

    def RunModel(self):
        raise UnimplementedMethodException()
        
    def MaxTestStat(self):
        return max(self.test_stats)

    def values2cdf(self, values):
        sorted_values = sorted(values)
        total = len(values)
        x = []
        y = []
        for i, value in enumerate(sorted_values):
            x.append(value)
            y.append(i/total)
        return x, y    
    
    def PlotCdf(self, label=None):
        self.test_cdf = self.values2cdf(self.test_stats)
        def VertLine(x):
            plt.plot([x, x], [0, 1], color='0.8')
        VertLine(self.actual)
        plt.plot(self.test_cdf[0], self.test_cdf[1], label=label)

# 이제 slope 값으로 가설 검정을 해보자.
class SlopeTest(HypothesisTest):
    def TestStatistic(self, data):
        ages, weights = data
        _, slope = LeastSquares(ages, weights)
        return slope

    def MakeModel(self):
        _, weights = self.data
        self.ybar = weights.mean()
        self.res = weights - self.ybar

    def RunModel(self):
        ages, _ = self.data
        weights = self.ybar + np.random.permutation(self.res)
        return ages, weights
```

```
ht = SlopeTest((ages, weights))
pvalue = ht.PValue()
pvalue
# pvalue 계산값은 0이다.
# 이 말은 1000개의 샘플을 썼으므로 p-value < 0.001 이라는 뜻이다.
# iteration을 더 큰 값을 쓰면 더 정확한 값을 구할 수 있다.

ht.actual, ht.MaxTestStat()

"""
결과
0.0

(0.01745578284852866, 0.007991107446131924)
"""
```

```
sampling_cdf = values2cdf(slopes)

ht.PlotCdf(label='null hypothesis')
plt.plot([0, 0], [0, 1], color='0.8')

plt.plot(sampling_cdf[0], sampling_cdf[1], label='sampling distribution')
plt.legend(loc=2)
plt.show()

# 두 개의 CDF를 비교해보자.
# 수직선은 0과, 실제 slope의 추정값인 0.017 lbs / year이다.
# 1000개의 sample 중 null hypothesis를 가정하여 나올 수 있는 sample이 한 개도 없다.
# 1000개의 sample을 보았으므로 p < 0.001이라고 생각할 수 있다.
```

<img width="578" alt="image" src="https://github.com/user-attachments/assets/6b27f0cd-a396-4da5-9e87-db7e6382794e">
