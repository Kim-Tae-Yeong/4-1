# 앙상블 모델
## 앙상블(프랑스어 Ensemble)
앙상블의 의미는 아래와 같다.
- 전체적인 어울림이나 통일
- 드레스와 코트, 스커트와 재킷 따위를 같은 천으로 만들어서 서로 잘 어울리는 한 벌의 여성복
- 음악 2인 이상이 하는 노래나 연주
- 음악 주로 실내악을 연주하는 적은 인원의 합주단
- 연기 배우 전원의 협력에 의하여 통일적인 효과를 얻으려는 연출법

즉 "함께" 조화를 이루어 하는 일을 의미한다.

## 앙상블 방법론
최근 기계학습 분야에서 딥러닝이외에 부스팅(boosting) 알고리즘이 자주 사용된다.
- 딥러닝은 비싸기 때문에 성능이 좋고 가벼운 모델을 만들려면 작은 모델을 사용해야 한다.

선형회귀나 로지스틱 회귀는 가장 쉽고 대중적인 알고리즘이다.

그 다음이 의사결정트리, 앙상블 계열 알고리즘, 딥러닝 등등이다.

## 앙상블
여러 개의 알고리즘들이 하나의 값을 예측하는 기법을 통칭하여 말한다.
- 회귀 문제에서는 가중 평균이나 단순 평균을 구하는 방식으로 y 값을 예측한다.

메타 분류기(meta - classifier)라고도 부른다.
- 메타(meta)는 일종의 상위 또는 추상화라는 개념이다.
- 여러 분류기들을 모아 하나의 분류기를 만들어 이를 메타 분류기라고 부른다.

시간이 굉장히 오래 걸리지만 비교적 좋은 성능을 보인다.
- "오래"라고 해도 대부분의 딥러닝 모델보다는 가볍다.

### 앙상블 모델

1. 하나의 데이터를 넣는다.
2. 이를 여러 모델이 학습한다.
3. 테스트 데이터를 각 모델에 입력한다.
4. 투표 또는 여러 가중치 기법을 적용하여 최종 선택한다.

<img width="392" alt="image" src="https://github.com/user-attachments/assets/9316e369-3819-4c1c-8265-eb5c059c3d6f">

## 앙상블 기법들
### 바닐라 앙상블
가장 기본적인 앙상블 기법이다.

바닐라 앙상블은 아무것도 처리하지 않은 앙상블 모델을 의미한다.

일반적으로 가중치 평균이나 투표 방식으로 만들어지는 앙상블 모델이다.

### 부스팅
하나의 모델에서 여러 데이터를 샘플링한 다음 그 샘플링된 데이터로 각각의 모델을 만드는 기법이다.

### 배깅
boosting aggregation(부스팅 집합)의 줄임말으로 부스팅을 좀 더 발전시킨 기법이다.

## 투표 분류기(Voting Classifier)
여러 개의 모델을 만들어 모두 같은 데이터를 넣고 결과를 취합하여 가장 많이 선택된 결과를 취한다.

<img width="355" alt="image" src="https://github.com/user-attachments/assets/fcebe866-9411-4138-9db7-d36ba2e247a5">

앙상블 모델의 가장 기본적인 형태이다.

다수결 분류기(majority voting classifier)라고도 부른다.

또는 각 분류기마다 가중치를 주고 해당 가중치를 각 모델에 곱하여 가중치의 합을 구하는 방식도 있다.

다양한 모델을 만든 후, 다음 단계로 매우 쉽게 만들 수 있다는 장점이 있다.
- 있는 모델을 결합만 하면 된다.

## 배깅(Bagging)
하나의 데이터셋에서 샘플링을 통해 여러 개의 데이터셋을 만든 다음 각 데이터셋마다 모델을 개발하여 투표 분류기로 만드는 기법이다.
- 단순하면서 성능이 높아 특히 트리 계열 알고리즘과 함께 많이 사용되며 통계적인 샘플링 기법이나 딥러닝 기법과도 함께 사용된다.

### 샘플링(Sampling)
다루고자 하는 데이터가 전체 모수라면 그 모수에서 일부분을 뽑아서 데이터를 분석하는 방법이다.

### 배깅의 장점
다양한 데이터셋에서 강건한 모델(robust model)을 개발할 수 있다.

<img width="573" alt="image" src="https://github.com/user-attachments/assets/645d0d86-15cc-4cb2-825d-d9a3234e9157">

### 약분류기와 강분류기로 알아보는 배깅 기법
배깅 기법은 여러 개의 약분류기(weak learner)로 강분류기(strong learner)를 만드는 것이다.
- 약분류기는 기본적으로 과소적합이 다소 있지만 과적합되어 있지 않은 모델이다.

다소 느슨하게 경계를 생성하는 여러 개의 약한 분류기를 앙상블한다면 좀 더 정확한 경계를 생성한다.

각각의 작은 데이터로는 모든 구체적인 분류 영역을 정할 수 없지만 많은 데이터로 투표한다면 더 높은 성능을 기대할 수 있다.

## 부트스트래핑(Bootstrapping)
모집단 데이터로부터 학습 데이터를 추출할 때 임의의 데이터를 추출한 후 복원추출하는 여러번의 과정이다.

### 복원추출
전체 데이터에서 먼저 일부를 추출하여 이를 '학습 데이터셋 1'을 만든다.

다시 그 데이터를 모수에 집어넣고 '학습 데이터셋 2'를 뽑는다.

위 과정을 반복한다.

<img width="501" alt="image" src="https://github.com/user-attachments/assets/0d38e203-1914-4fe2-b588-338a279979b3">

## Bootstrapping Prediction Error
부트스트랩 방법을 이용해 예측오차를 추정하는 방법에 대해 생각해보자.
- 먼저, 각 bootstrap 데이터셋(b = 1, ..., B)에 대해 모델을 추정하는 방법을 생각해보자.(f<sup>b</sup>)
- 그러면 이를 이용해 손실함수의 표본평균으로 예측오차의 추정량을 아래와 같이 정의할 수 있다.

<img width="395" alt="image" src="https://github.com/user-attachments/assets/de79bb7d-8505-4479-94c3-5383cc13cca7">

- 손실함수를 계산하는데 쓰이는 데이터, 즉 test sample의 역할을 하는 데이터가 각 z_i = (x_i, y_i)로 원래 training set이므로, 이 값을 올바른 추정량이 아니다.

## '.632 부트스트래핑' 기법
전체 데이터 S에서 n번의 데이터를 추출할 때, 이를 많이 추출할수록 각 데이터가 나타날 확률이 63.2%에 가까워진다.
- n -> ∞일 때 한 번의 부트스트랩 샘플링에서 선택되지 않을 확률은 아래와 같다.

<img width="335" alt="image" src="https://github.com/user-attachments/assets/2dde8848-f6dc-4a90-8092-6e5f377c6a87">

- 즉, 전체 데이터셋에서 복원추출로 bootstrap 샘플을 생성하면, 평균적으로 원본 데이터의 63.2%가 선택되고, 36.8%는 선택되지 않는다.

## 개선방법
set을 train과 test로 분리한다.

train set은 부트스트랩으로 학습을 한다.

test set으로도 정확도를 잰다.

총 정확도는 아래와 같이 쓸 수 있다.

<img width="519" alt="image" src="https://github.com/user-attachments/assets/bac6a650-5601-4755-8726-d4c9a95eeb61">

## Out - Of - Bag Error
배깅 모델의 성능을 측정하기 위해서 정확도나 정밀도 외에 'out - of - bag error'라는 지표를 사용한다.
- 일반적으로 'OOB error estimation'이라고 부른다.

배깅에서 부분집합을 생성할 때 일부 데이터만 학습에 사용되는데, 각 부분집합에서 학습에 사용되지 않은 데이터셋에 대해서만 성능을 측정하여 배깅 모델의 효과를 측정하는 것이다.

기본적으로 검증셋(validation set)과 유사한 방식으로 학습에 사용하지 않은 데이터를 가지고 학습의 성능을 측정한다고 이해할 수 있다.

<img width="364" alt="image" src="https://github.com/user-attachments/assets/12943171-274d-4442-b063-53c99b974137">

## 부트스트랩과 배깅
배깅(bagging)은 부트스트랩 집합이라는 의미인 'bootstrap aggregation'의 약자이다.
- 말 그대로 부트스트랩 연산의 집합이라는 개념이다.

데이터셋으로부터 부분집합 n개를 추출한다.
- 앙상블 방법과 달리 하나의 모델에 다양한 데이터셋을 넣어서 n개의 모델을 생성한다.

높은 분산으로, 일반적인 모델로 만들 경우 과적합이 심한 데이터셋에 좀 더 강건하다.
- 각 모델들은 해당 데이터셋에 맞춰진 과적합 모델을 기대한다.

<img width="500" alt="image" src="https://github.com/user-attachments/assets/a070bac5-ba0b-44b1-a7cc-97c093818acb">

## 무작위 숲(Random Forest)
하나의 트리(나무) 모델이다.
- 나무들을 이용해 무작위로 데이터를 뽑아 숲을 만들자.

배깅 알고리즘을 의사결정트리에 적용한 모델이다.

사이킷런에서는 `BaggingClassifier`를 사용한다.

## 부스팅(Boosting)
학습 라운드를 차례로 진행하면서 각 예측이 틀린 데이터에 점점 가중치를 주는 학습 방법이다.

라운드별로 잘못 분류된 데이터를 좀 더 잘 분류하는 모델로 만들어 최종적으로 모델들의 앙상블을 만드는 방식이다.
- 배깅 알고리즘이 처음 성능을 측정하기 위한 기준(baseline) 알고리즘으로 많이 사용된다.
- 부스팅 알고리즘은 높은 성능을 내야 하는 상황에서 가장 좋은 선택지이다.

첫 번째 라운드 결과 모델(1)에서 A점은 오차가 큰 부분이다.

두 번째 라운드에서 오답으로 분류된 A에 가중치를 주고 학습한다.

다시 오류가 큰 B 영역에 가중치를 주고 모델(3)을 학습한다.

<img width="399" alt="image" src="https://github.com/user-attachments/assets/659d0978-fd52-438d-9b00-ffd402d2c872">

틀린 부분만 집중해서 모델들을 순차적으로 만들고, 해당 모델들은 최종적으로 앙상블이 된다.

<img width="484" alt="image" src="https://github.com/user-attachments/assets/d38ac85c-54a7-4783-aeb0-0d82efa620c0">

부스팅은 마치 오답노트를 만드는 것과 같다.

## 배깅과 부스팅의 비교
배깅은 데이터가 n개라면 n개의 CPU로 한번에 처리하도록 구조를 설계할 수 있다.
- 병렬화가 가능하다.
  - 배깅은 데이터를 나눠 데이터마다 조금씩 다른 모델을 생성한다.
 
- 부스팅은 단계적으로 모델들을 생성하고 해당 모델들의 성능을 측정한 후 다음 단계로 넘어가 병렬화하기 어렵다.
- 그래서 부스팅은 배깅에 비해 느리다.

배깅에서 작은 한 개의 모델들은 높은 과대적합으로 모델의 분산이 높다.

부스팅은 각각의 모델에 편향이 높은 기준 추정치를 사용하여 개별 모델들은 과소적합이 발생하지만 전체적으로 높은 성능을 낼 수 있는 방향으로 학습한다.

### 성능 차이
부스팅은 기본적으로 비용이 높은 알고리즘이다.
- 비용은 속도나 시간을 말한다.

배깅은 데이터의 부분집합에 대해 학습을 수행하기 때문에 부스팅보다 좋은 성능을 내기 어렵다.

초기 성능을 측정할 때는 배깅, 이후의 성능 측정은 부스팅하는 것이 좋으나, 문제에 따라 다르다.

||배깅|부스팅|
|---|---|---|
|형태|병렬 앙상블(모든 모델이 독립적)|선형(sequential) 앙상블(다음 모델에 이전 모델이 영향을 줌)|
|샘플링 방식|무작위(부트스트래핑)|오류가 있는 샘플에 가중치|
|목적|편차 줄이기|편향 줄이기|
|베이스 학습기|불안정한 학습기(깊은 결정 트리)|안정적인 학습기(스텀프)|
|예시|Random Forest|XGBoost, LightGBM, AdaBoost, ...|


## Reminder : 분산과 편향
### 과대적합(Overfitting)
높은 분산 낮은 편향 상태로 함수가 훈련 데이터셋에만 맞는다.

### 과소적합(Underfitting)
낮은 분산 높은 편향 상태로 함수가 훈련 데이터셋과 테스트 데이터셋에 모두 맞지 않는다.

<img width="265" alt="image" src="https://github.com/user-attachments/assets/ddce17fe-096f-451b-905b-9cb4556de3c2">

## 에이다부스트(AdaBoost)
매 라운드마다 인스턴스, 즉 개별 데이터의 가중치를 계산한다.
- 매 라운드마다 틀린 값이 존재하고 해당 인스턴스에 가중치를 추가로 주어 가중치를 기준으로 재샘플링(resampling)한다.
- 부스팅 알고리즘 중 대표적인 알고리즘이다.

모든 샘플의 가중치 값을 데이터 개수를 기준으로 1 / N로 초기화한다.

<img width="252" alt="image" src="https://github.com/user-attachments/assets/b32e9c77-b812-449e-a7af-1f5453566c34">

- 먼저 데이터의 가중치를 사용해 모델 G<sub>m</sub>(x<sub>i</sub>)를 학습시킨다.
- 해당 분류기의 오류를 계산한다.

<img width="535" alt="image" src="https://github.com/user-attachments/assets/faacc5cf-0cf8-4d94-86e0-483766357730">

- 해당 분류기의 가중치를 생성한다.

<img width="175" alt="image" src="https://github.com/user-attachments/assets/0a3f24e2-6fc5-4c01-ac1b-4968da90e32b">

- 모델의 가중치를 사용하여 각 데이터의 가중치를 업데이트한다.

<img width="392" alt="image" src="https://github.com/user-attachments/assets/826d5a46-b10f-4a82-92bd-c02a3df73364">

- 최종적으로 각 모델 가중치와 모델 결과값의 가중합을 연산하여 계산한다.

<img width="291" alt="image" src="https://github.com/user-attachments/assets/5f043fd4-523a-4309-b829-dc3816e22723">

## 에이다부스트의 스텀프
학습할 때 큰 나무를 사용하여 학습하는 것이 아니라 나무의 그루터기만을 사용하여 학습한다.

1 뎁스(depth) 또는 2 뎁스 정도의 매우 간단한 모델을 여러 개 만들어 학습한다.
- 해당 모델들의 성능을 에이다부스트 알고리즘을 적용하여 학습한다.

<img width="313" alt="image" src="https://github.com/user-attachments/assets/fec0fc34-e4f5-471b-8bc3-f4e35afeeac0">

## Gradient Boosting
기본적인 원리는 AdaBoosting과 같다.

차이점은 아래와 같다.
- 기울기(gradient)를 이용해서 조금 더 일반적인 학습이 가능하다.
  - loss(cost) 함수를 최소화하는 방향으로 학습한다.
 
- 스텀프보다 더 깊은 트리를 사용할 수 있다.

regression도 가능하고 classificaiton도 가능하다.

만약 loss function이 standard error인 경우, 기울기에 마이너스 값을 취하면 (실제값 - 예측값)이 된다.
- 이는 잔차(residual)과 같다.
- 이 잔차를 학습하며 잔차를 줄여나가는 방식이다.

gradient boosting은 의사 결정 트리를 하나씩 구축하고, 개별 트리들의 예측값을 합산한다.

$D(x) = Tree_i(x) + Tree_2(x) + ...$

다음 트리는 목표 함수 $f(x)$와 현재 앙상블의 예측값 사이의 차이 잔차(residual)를 재구성함으로써 성능을 개선한다.
- 예를 들어, 앙상블이 3개의 트리로 구성되어 있다면 그 앙상블의 예측은 아래와 같다.
  - $D(x) = Tree_1(x) + Tree_2(x) + Tree_3(x)$

앙상블의 다음 트리는 기존 트리들을 잘 보안하고 앙상블의 학습 오차를 최소화해야한다.
- 이상적인 경우라면 다음과 같은 결과를 얻고 싶을 것이다.
  - $D(x) + Tree_4(x) = f(x)$
 
- 목표에 더 가까워지기 위해, 우리는 목표 함수와 현재 앙상블 예측값의 차이(잔차)를 재구성하도록 트리를 학습시킨다.
  - $R(x) = f(x) - D(x)$
 
- 만약 의사 결정 트리가 $R(x)$를 완벽하게 재구성한다면, 새로 학습된 트리를 앙상블에 추가한 후에는 전체 앙상블이 오차가 없어진다.
- 물론 현실에선 절대 이런 일이 일어나지 않기 때문에 반복적으로 학습을 계속 진행한다.

## XGBoost
gradient boosting의 최대 약점은 병렬화가 어렵다는 것이다.
- gradient boosting 알고리즘은 분기할 때마다 모든 피처의 모든 데이터 값을 고려해 손실함수, 즉 MSE 혹은 지니계수가 가장 낮아지는 최적의 분기점을 계산해 분할한다.
- 이는 매우 느리다.

이 문제를 해결하기 위해 XGBoost라는 알고리즘이 제안되었다.
- 핵심 아이디어는 각 피처 데이터들을 일정 간격으로 나눠 최적의 분기점을 찾는다.
- 일정 간격으로 나눠진 서브셋 안에서 최고의 split을 찾는 과정을 병렬 처리할 수 있는 것이다.

아래 예는 최대 39번 손실함수가 최소가 되는 분할점을 계산해야 할 것이다.
- 그룹(버킷)을 10개로 나눈다면 각 그룹에서 3번의 계산이 총 10개 그룹에서 이루어지므로 30번만 계산하면 된다.
- 그룹의 계산을 다른 스레드로 처리할 수 있다.

<img width="524" alt="image" src="https://github.com/user-attachments/assets/56e8697b-b7b5-480b-ab5a-243f1aa3bec1">

## Mixture of Experts

<img width="624" alt="image" src="https://github.com/user-attachments/assets/eccd516a-fd33-45ea-883d-44ec9292e8d9">

<br />

<img width="569" alt="image" src="https://github.com/user-attachments/assets/c168637f-ce0c-4244-bef5-baae9cab19b2">

<br />

<img width="577" alt="image" src="https://github.com/user-attachments/assets/1ae44b79-003c-4148-9a3e-75dd2af13997">
