# 의사 결정 트리
## Desicion Tree
여러 가지 규칙을 순차적으로 적용하면서 독립 변수 공간을 분할하는 분류 모형이다.

분류(classification)와 회귀 분석(regression)에 모두 사용될 수 있기 때문에 CART(Classification And Regression Tree)라고도 한다.

## 간단한 구현
나이와 소득등에 따라 컴퓨터를 구매할지 확인하는 데이터로 구현을 해보자.

의미없는 index 데이터인 RID는 제거한다.

```
import pandas as pd 
import numpy as np

pd_data = pd.read_csv('https://raw.githubusercontent.com/AugustLONG/ML01/master/01decisiontree/AllElectronics.csv')
pd_data = pd_data.drop("RID", axis=1)
```

### 정보 이득 함수
y 값의 클래스를 기준으로 엔트로피 계산을 해보자.
- 예를 들어서 `class_buys_computer`가 yes인 경우와 no인 경우로 나누자.
- $Info(D)=-\sum_{i=1}^n p_i \log_2 (p_i)$

```
def get_info(df):
    buy = df.loc[df["class_buys_computer"]=="yes"]
    not_buy = df.loc[df["class_buys_computer"]=="no"]
    x = np.array([len(buy)/len(df),len(not_buy)/len(df)])
    y = np.log2(x[x!=0])
    
    info_all = - sum(x[x!=0] * y)
    return info_all

get_info(pd_data)

"""
결과
0.9402859586706311
"""
```

이제 각 속성들의 클래스를 기준으로 데이터를 추출하고, 정보량을 구해보자.

```
youth = pd_data.loc[pd_data['age'] == "youth"]
senior = pd_data.loc[pd_data['age'] == "senior"]
middle_aged = pd_data.loc[pd_data['age'] == "middle_aged"]

print(get_info(youth))

print(get_info(senior))

print(get_info(middle_aged))

"""
결과
0.9709505944546686

0.9709505944546686

-0.0
"""
```

이제 속성을 입력하면 자동으로 정보 이득을 계산하는 함수를 구해보자.
- 속성별 정보량은 아래와 같다.
- $Info_{att}(D) = -\sum_{j=1}^v \frac{|D_j|}{D}\times Info(D_j)$
- 즉, 각 속성의 분기별 정보량을 비율대로 가중치 합을 한 값이다.

```
def get_attribute_info(df, attribute_name):
    attribute_values = pd_data[attribute_name].unique()
    get_infos = []
    for value in attribute_values:
        split_df = pd_data.loc[pd_data[attribute_name] == value]
        
        get_infos.append((len(split_df) / len(df)) * get_info(split_df))
    
    return sum(get_infos)

# 예를 들어 age는 아래와 같다.
get_attribute_info(pd_data, "age")

"""
결과
0.6935361388961918
```

이제 정보 이득을 구해보자.
- 원래 정보량에서 속성별 정보량을 뺴면 된다.

```
get_info(pd_data) - get_attribute_info(pd_data, "age")

get_info(pd_data) - get_attribute_info(pd_data, "income")

get_info(pd_data) - get_attribute_info(pd_data, "student")

get_info(pd_data) - get_attribute_info(pd_data, "credit_rating")

"""
결과
0.24674981977443933

0.02922256565895487

0.15183550136234159

0.04812703040826949
```

정보 이득이 가장 큰 age를 기준으로 의사 결정 트리 가지를 만든다.

```
youth = pd_data.loc[pd_data['age'] == "youth"]
```

그리고 이제 age가 youth인 데이터를 기준으로 가지를 분할한다.

```
get_info(youth) - get_attribute_info(youth, "income")

get_info(youth) - get_attribute_info(youth, "student")

get_info(youth) - get_attribute_info(youth, "credit_rating")

"""
결과
-1.580026905978025

-1.2367106860085422

-1.527094404679944
"""
```

위의 데이터를 고려하면 정보 이득이 가장 큰 student 기준으로 다시 분할이 일어날 것이다.

## scikit - learn으로 구현하기
타이타닉 데이터를 잘 구현된 scikit - learn으로 구현해보자.

```
import pandas as pd

train_df = pd.read_csv("W13/train.csv")
test_df = pd.read_csv("W13/test.csv")

# 전처리 후에 train과 test를 구분하기 위해 사용한다.
train_id = train_df["PassengerId"].values 
test_id = test_df["PassengerId"].values

# 교과서 코드가 이상해서 수정한다.
all_df = pd.concat([train_df,test_df]).set_index('PassengerId')
```

### 데이터 전처리
일단 데이터를 부호화시키고 결측치를 채워보자.

```
# 다운캐스팅을 명시적으로 셋팅해야 한다.
all_df["Sex"] = all_df["Sex"].map({"male":0,"female":1}).astype(int) 

# 데이터 중 age 값의 빈칸의 값을 `class의 평균값으로 채운다.
all_df["Age"] = all_df["Age"].fillna(all_df.groupby("Pclass")["Age"].transform("mean"))

all_df["cabin_count"] = all_df["Cabin"].map(
         lambda x : len(x.split()) if type(x) == str else 0)

def transform_status(x):
    if "Mrs" in x or "Ms" in x:
        return "Mrs" 
    elif "Mr" in x:
        return "Mr"
    elif "Miss" in x:
        return "Miss"
    elif "Master" in x:
        return "Master"
    elif "Dr" in x:
        return "Dr"
    elif "Rev" in x:
        return "Rev"
    elif "Col" in x:
        return "Col"
    else:
        return "0"

all_df["social_status"] = all_df["Name"].map(lambda x : transform_status(x))

all_df["social_status"].value_counts()

"""
결과
social_status
Mr        758
Miss      258
Mrs       203
Master     61
0           9
Rev         8
Dr          8
Col         4
Name: count, dtype: int64
"""
```

사용하지 않을 데이터는 지워주자.
- Embarked가 비워져있는 62번과 830번을 지우자.

```
all_df = all_df.drop([62,830])
# 62번과 830번은 train set에 있다.
train_id = np.delete(train_id, [62-1,830-1])
```

적당히 빈 데이터 값도 채워주자.

```
# 성별과 객실에 따른 요금 평균을 보자.
all_df.groupby(["Pclass","Sex"])["Fare"].mean()

all_df.loc[all_df["Fare"].isnull(), "Fare"] = 12.415462

"""
결과
Pclass  Sex
1       0       69.888385
        1      109.826644
2       0       19.904946
        1       23.234827
3       0       12.415462
        1       15.324250
Name: Fare, dtype: float64
"""
```

Cabin은 객실 번호이다.
- 객실 번호에서 맨 앞의 알파벳만 따온다.
- 이건 객실의 종류를 의미한다.

```
all_df["cabin_type"] = all_df["Cabin"].map(lambda x : x[0] if type(x) == str else "99")

all_df["cabin_type"].unique()

"""
결과
array(['99', 'C', 'E', 'G', 'D', 'A', 'B', 'F', 'T'], dtype=object)
```

불필요한 정보는 이제 지워보자.

```
del all_df["Cabin"]
del all_df["Name"]
del all_df["Ticket"]

y = all_df.loc[train_id, "Survived"].values
del all_df["Survived"]
```

### 원핫인코딩과 데이터 스케일링
카테고리 데이터를 원핫벡터로 만든다.

```
X_df = pd.get_dummies(all_df)
X = X_df.values
```

MinMax Scaling도 해주자.

```
from sklearn.preprocessing import MinMaxScaler
minmax_scaler = MinMaxScaler()

minmax_scaler.fit(X)
X = minmax_scaler.transform(X)
```

train set과 test set을 다시 나눈다.

```
X_train = X[:len(train_id)]
X_test = X[len(train_id):]
```

### 학습하기
`DesicionTreeClassifier` 객체를 생성한다.
- `criterion`([gini, entropy])
  - 지니 계수를 기준으로 나눌지, 정보 이득을 기준으로 나눌지 지정한다.
   
- `max_depth`(int)
  - 트리의 깊이를 지정한다.
 
- `min_samples_leaf`(int or float)
  - 마지막 노드의 최소 데이터의 개수를 지정한다.
    - int는 데이터의 개수를 정해주고, float는 전체 데이터에서의 비율을 정해준다.
    - 아래에서 `min_samples_leaf` 수를 3개부터 점점 키워가면서 정확도를 재보자.

```
from sklearn.tree import DecisionTreeClassifier

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score  

test_accuracy = []
train_accuracy = []
for idx in range(3, 20):
    df = DecisionTreeClassifier(min_samples_leaf=idx)
    acc = cross_val_score(df, X_train, y, scoring="accuracy", cv=5).mean()
    train_accuracy.append(
        accuracy_score(df.fit(X_train, y).predict(X_train), y))
    test_accuracy.append(acc)

result = pd.DataFrame(train_accuracy, index=range(3,20), columns=["train"])
result["test"] = test_accuracy

result.plot()
```

<img width="438" alt="image" src="https://github.com/user-attachments/assets/27121cbf-9d96-4425-a732-270e217c2021">

test 데이터셋의 정확성은 11에서 가장 높았다가 계속 떨어진다.
- 의사 결정 트리의 경우 마지막 노드의 데이터 개수가 적으면 적을수록 과대적합이 발생할 확률이 높다.

지난 시간에 사용했던 파이프라인을 통해 두 개 이상의 알고리즘과 매개변수에 대한 테스트를 해보자.

```
from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression

# 기본 parameter에서 수렴을 안 해서 교과서 코드에서 iteration 수를 늘렸다.
algorithmes = [LogisticRegression(max_iter=5000), DecisionTreeClassifier()]

c_params = [0.1,  5.0, 7.0, 10.0, 15.0, 20.0, 100.0]


params = []
params.append([{
    "solver" : ["saga"], # Multinomial
    "penalty" : ["l1"],
    "C" : c_params,
    },{
    "solver" : ['liblinear'], # OVR
    "penalty" : ["l2"],
    "C" : c_params,
    }
    ])
params.append({
    "criterion" : ["gini", "entropy"], # 분할 기준
    "max_depth" : [10,8,7,6,5,4,3,2], # 최대 깊이
    "min_samples_leaf": [1,2,3,4,5,6,7,8,9]}) # leaf node의 최소 데이터 수
```

grid search를 해보자.

```
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score

scoring = ['accuracy']
estimator_results = []
for i, (estimator, params) in enumerate(zip(algorithmes,params)):
    # Accuracy 기준으로 best parameter를 찾는다.
    gs_estimator = GridSearchCV( 
            refit="accuracy", estimator=estimator,param_grid=params,
            scoring=scoring, cv=5, verbose=1, n_jobs=4)

    gs_estimator.fit(X_train, y)
    estimator_results.append(gs_estimator)

"""
결과
Fitting 5 folds for each of 14 candidates, totalling 70 fits
Fitting 5 folds for each of 144 candidates, totalling 720 fits
"""
```

logistic regression과 desicion tree 각각의 best estimator를 찾을 수 있다.

```
estimator_results[0]

estimator_results[1]
```

둘 중에 뭐가 더 나은 선택일까?
- `best_score_`는 위에서 지정한 accuracy를 기준으로 logistic regression과 desicion tree의 최고 점수를 보여준다.

```
# Logistic Regression
estimator_results[0].best_score_

# Decision Tree
estimator_results[1].best_score_
# Decision tree가 조금 더 나은 성능을 보여줍니다.

"""
결과
0.8268075922046594

0.8358154002412238
"""
```

전체의 점수를 dataframe으로 만들어 확인해보자.

```
import pandas as pd
from pandas import DataFrame
from collections import defaultdict

result_df_dict = {}

result_attributes = ["model", "accuracy", "penalty", "solver", "C", "criterion", "max_depth", "min_samples_leaf"]
result_dict = defaultdict(list)

algorithm_name= ["LogisticRegression", "DecisionTreeClassifier"]

for i, estimators in enumerate(estimator_results):
    number_of_estimators = len(estimators.cv_results_["mean_fit_time"])

    for idx_estimator in range(number_of_estimators):
        result_dict["model"].append(algorithm_name[i])
        result_dict["accuracy"].append(
            estimators.cv_results_["mean_test_accuracy"][idx_estimator])
            
    for param_value in estimators.cv_results_["params"]:
        for k,v in param_value.items():
            result_dict[k].append(v)
    for attr_name in result_attributes:
        if len(result_dict[attr_name]) < len(result_dict["accuracy"]):
            result_dict[attr_name].extend([None for i in range(number_of_estimators)])
```

최종적으로 엔트로피로 최대 깊이 4, `min_samples_leaf` 7일 때 가장 좋은 성능을 보인다.
- 다만 파라미터가 바뀌어도 성능은 꽤나 안정적인 것으로 보인다.

```
result_df = DataFrame(result_dict, columns=result_attributes)
result_df.sort_values("accuracy",ascending=False).head()
```

<img width="833" alt="image" src="https://github.com/user-attachments/assets/2cd00ae5-2acc-4a64-84b5-f46d267c2842">

### 시각화와 Feature Importance
의사 결정 트리의 장점 중 하나는 우리가 어떤 feature를 더 중요하게 고려해야 하는지를 보여준다는 것이다.
- `.best_estimator_.feature_importances_`는 각 feature의 중요도를 보여준다.

```
estimator_results[1].best_estimator_.feature_importances_

"""
결과
array([0.13408672, 0.06272407, 0.0412891 , 0.        , 0.        ,
       0.18928639, 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.04394389, 0.        ,
       0.51513495, 0.        , 0.        , 0.01353489, 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.        ,
       0.        , 0.        ])
"""
```

이것은 원래의 X_train의 feature 순서와 같으므로, 아래의 순서를 의미한다.

```
X_df.columns

"""
결과
Index(['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'cabin_count',
       'Embarked_C', 'Embarked_Q', 'Embarked_S', 'social_status_0',
       'social_status_Col', 'social_status_Dr', 'social_status_Master',
       'social_status_Miss', 'social_status_Mr', 'social_status_Mrs',
       'social_status_Rev', 'cabin_type_99', 'cabin_type_A', 'cabin_type_B',
       'cabin_type_C', 'cabin_type_D', 'cabin_type_E', 'cabin_type_F',
       'cabin_type_G', 'cabin_type_T'],
      dtype='object')
"""
```

저렇게 보면 비교가 어려우니, 순서를 정리하고 중요한 순서대로 보자.
- `numpy.argsort`는 array를 정렬하는 순서의 index를 반환해준다.

```
coef = estimator_results[1].best_estimator_.feature_importances_
coef.argsort()[::-1]

X_df.columns[coef.argsort()[::-1]][:5]

"""
결과
array([15,  5,  0,  1, 13,  2, 18, 21,  9, 24,  3,  4, 23,  6,  7,  8, 10,
       20, 11, 12, 25, 14, 22, 16, 17, 19, 26])

Index(['social_status_Mr', 'Fare', 'Pclass', 'Sex', 'social_status_Master'], dtype='object')
"""
```

하나의 노드를 기준으로 어떤 속성에 대해서 어떤 기준으로 가지가 발생했는지, 각 가지마다 데이터의 개수가 어떻게 나누어지는지를 pydot으로 볼 수 있다.

pydot은 Graphviz의 DOT 언어를 파이썬으로 다루기 위한 라이브러리이다.
- Graphviz는 그래프 시각화를 위한 오픈소스 도구이다.

```
import pydot
from six import StringIO
from sklearn import tree
from IPython.core.display import Image

best_tree = estimator_results[1].best_estimator_
column_names = X_df.columns

dot_data = StringIO()
tree.export_graphviz(best_tree, out_file=dot_data, feature_names=column_names) 

graph = pydot.graph_from_dot_data(dot_data.getvalue())[0]
image = graph.create_png()
Image(image)
```

<img width="434" alt="image" src="https://github.com/user-attachments/assets/bd5ff723-f5bf-4733-8db2-24a392faacf1">

## 분류 문제 돌아보기
여기에서는 붓꽃(iris) 분류 문제를 예를 들어 의사 결정 나무를 설명한다.

이 예제에서는 독립변수 공간을 공간상에 표시하기 위해 꽃의 길이와 폭만을 독립변수로 사용한다.

```
from sklearn.datasets import load_iris

data = load_iris()
y = data.target
X = data.data[:, 2:]
feature_names = data.feature_names[2:]

from sklearn.tree import DecisionTreeClassifier
 
tree1 = DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=0).fit(X, y)
```

다음은 의사 결정 나무를 시각화하기 위한 코드이다.

`draw_decision_tree` 함수는 의사 결정 나무의 의사 결정 과정의 세부적인 내역을 다이어그램으로 보여준다.

`plot_decision_tree` 함수는 이러한 의사 결정에 대해 데이터의 영역이 어떻게 나뉘어졌는지를 시각화하여 보여준다.

```
import io
import pydot
from IPython.core.display import Image
from sklearn.tree import export_graphviz
import matplotlib as mpl
from matplotlib import pyplot as plt
import seaborn as sns

def draw_decision_tree(model):
    dot_buf = io.StringIO()
    export_graphviz(model, out_file=dot_buf, feature_names=feature_names)
    graph = pydot.graph_from_dot_data(dot_buf.getvalue())[0]
    image = graph.create_png()
    return Image(image)

def plot_decision_regions(X, y, model, title):
    resolution = 0.01
    markers = ('s', '^', 'o')
    colors = ('red', 'blue', 'lightgreen')
    cmap = mpl.colors.ListedColormap(colors)

    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    Z = model.predict(
        np.array([xx1.ravel(), xx2.ravel()]).T).reshape(xx1.shape)

    plt.contour(xx1, xx2, Z, cmap=mpl.colors.ListedColormap(['k']))
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8,
                    c=[cmap(idx)], marker=markers[idx], s=80, label=cl)

    plt.xlabel(data.feature_names[2])
    plt.ylabel(data.feature_names[3])
    plt.legend(loc='upper left')
    plt.title(title)

    return Z

draw_decision_tree(tree1)
```

<img width="436" alt="image" src="https://github.com/user-attachments/assets/6f933afc-f6ec-4ea7-8940-63cc94b8adb0">

```
plot_decision_regions(X, y, tree1, "Depth 1")
plt.show()
```

<img width="434" alt="image" src="https://github.com/user-attachments/assets/12f342b9-54bf-4627-9605-9a3ca2ad9e6d">

```
from sklearn.metrics import confusion_matrix

confusion_matrix(y, tree1.predict(X))

"""
결과
array([[50,  0,  0],
       [ 0, 50,  0],
       [ 0, 50,  0]])
"""
```

```
tree2 = DecisionTreeClassifier(
    criterion='entropy', max_depth=2, random_state=0).fit(X, y)

draw_decision_tree(tree2)
```

<img width="435" alt="image" src="https://github.com/user-attachments/assets/bc2e97ed-63cd-4ca0-84bd-e0213e9f3974">

```
plot_decision_regions(X, y, tree2, "Depth 2")
plt.show()
```

<img width="434" alt="image" src="https://github.com/user-attachments/assets/7f4aeaf9-1dce-4714-abba-790250bb1d7c">

```
confusion_matrix(y, tree2.predict(X))

"""
결과
array([[50,  0,  0],
       [ 0, 49,  1],
       [ 0,  5, 45]])
"""
```

```
tree3 = DecisionTreeClassifier(
    criterion='entropy', max_depth=3, random_state=0).fit(X, y)

draw_decision_tree(tree3)
```

<img width="431" alt="image" src="https://github.com/user-attachments/assets/499126f5-f13f-46e4-bfc9-327b7a5eec7a">

```
plot_decision_regions(X, y, tree3, "Depth 3")
plt.show()
```

<img width="437" alt="image" src="https://github.com/user-attachments/assets/cfba4735-cf8b-45f2-845c-f78c31f6fca7">

```
confusion_matrix(y, tree3.predict(X))

"""
결과
array([[50,  0,  0],
       [ 0, 47,  3],
       [ 0,  1, 49]])
"""
```

```
tree4 = DecisionTreeClassifier(
    criterion='entropy', max_depth=4, random_state=0).fit(X, y)

draw_decision_tree(tree4)
```

<img width="440" alt="image" src="https://github.com/user-attachments/assets/ddeb270b-a762-42ed-93ec-74bbde81768d">

```
plot_decision_regions(X, y, tree4, "Depth 4")
plt.show()
```

<img width="435" alt="image" src="https://github.com/user-attachments/assets/552203fc-9507-47da-b463-ee00401a8207">

```
confusion_matrix(y, tree4.predict(X))

"""
결과
array([[50,  0,  0],
       [ 0, 49,  1],
       [ 0,  1, 49]])
"""
```

```
tree5 = DecisionTreeClassifier(
    criterion='entropy', max_depth=5, random_state=0).fit(X, y)

draw_decision_tree(tree5)
```

<img width="435" alt="image" src="https://github.com/user-attachments/assets/73b14e8a-14ac-49f9-a3b0-6a0be45f0b54">

```
plot_decision_regions(X, y, tree5, "Depth 5")
plt.show()
```

<img width="435" alt="image" src="https://github.com/user-attachments/assets/46446f4b-e391-44b1-821b-7c51f98a1b65">

```
confusion_matrix(y, tree5.predict(X))

"""
결과
array([[50,  0,  0],
       [ 0, 49,  1],
       [ 0,  0, 50]])
"""
```

### Titanic Survivors, Another Version
다른 방식으로 구현한 titanic 분류기를 통해 아까의 것과 차이를 비교해보자.

```
df = sns.load_dataset("titanic")

feature_names = ["pclass", "age", "sex"]
dfX = df[feature_names].copy()
dfy = df["survived"].copy()
dfX.tail()
```

<img width="243" alt="image" src="https://github.com/user-attachments/assets/654526b7-7bff-4226-bd39-63a00dcf4c4a">

```
from sklearn.preprocessing import LabelEncoder
dfX["sex"] = LabelEncoder().fit_transform(dfX["sex"])
dfX.tail()
```

<img width="217" alt="image" src="https://github.com/user-attachments/assets/73157dea-8a74-4ce5-be1e-fda709bf367a">

```
dfX["age"] = dfX["age"].fillna(dfX["age"].mean())
dfX.tail()
```

<img width="257" alt="image" src="https://github.com/user-attachments/assets/d5648b3f-cb0c-4d06-adf9-34dff7477ac5">

```
from sklearn.preprocessing import LabelBinarizer
dfX2 = pd.DataFrame(LabelBinarizer().fit_transform(dfX["pclass"]),
                    columns=['c1', 'c2', 'c3'], index=dfX.index)
dfX = pd.concat([dfX, dfX2], axis=1)
del(dfX["pclass"])
dfX.tail()
```

<img width="303" alt="image" src="https://github.com/user-attachments/assets/13fcbcc8-a394-4ba9-8ce7-9940fac0f05a">

```
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

X_train, X_test, y_train, y_test = train_test_split(dfX, dfy, test_size=0.25, random_state=0)


model = DecisionTreeClassifier(
    criterion='entropy', max_depth=3, min_samples_leaf=5).fit(X_train, y_train)

command_buf = io.StringIO()
export_graphviz(model, out_file=command_buf, feature_names=[
                'Age', 'Sex', '1st_class', '2nd_class', '3rd_class'])
graph = pydot.graph_from_dot_data(command_buf.getvalue())[0]
image = graph.create_png()

Image(image)
```

<img width="1029" alt="image" src="https://github.com/user-attachments/assets/e484ba28-1fbc-4586-99f6-fb878806658e">

```
confusion_matrix(y_train, model.predict(X_train))

confusion_matrix(y_test, model.predict(X_test))

"""
결과
array([[360,  50],
       [ 73, 185]])

array([[119,  20],
       [ 25,  59]])
"""
```

```
from sklearn.metrics import classification_report

print(classification_report(y_train, model.predict(X_train)))

print(classification_report(y_test, model.predict(X_test)))

"""
결과
              precision    recall  f1-score   support

           0       0.83      0.88      0.85       410
           1       0.79      0.72      0.75       258

    accuracy                           0.82       668
   macro avg       0.81      0.80      0.80       668
weighted avg       0.81      0.82      0.81       668

              precision    recall  f1-score   support

           0       0.83      0.86      0.84       139
           1       0.75      0.70      0.72        84

    accuracy                           0.80       223
   macro avg       0.79      0.78      0.78       223
weighted avg       0.80      0.80      0.80       223
"""
```

## Greedy 의사 결정
의사 결정 나무의 문제점 중 하나는 특징의 선택이 greedy한 방식으로 이루어지기 때문에 선택된 특징이 최적의 선택이 아닐 수도 있다는 점이다.

예를 들어 데이터가 다믕과 같다고 해보자.

```
X = [
    [0, 0, 0],
    [1, 0, 0],
    [0, 0, 1],
    [1, 0, 1],
    [0, 1, 0],
    [1, 1, 0],
    [0, 1, 1],
    [1, 1, 1],
]
y = [0,0,1,1,1,1,0,0]
```

첫 노드에서는 $x_1, x_2, x_3$의 성능이 같다.

만약 첫 노드에서 특징으로 $x_1$을 선택하면 2단계로 완벽한 분류를 하는 것은 불가능하다.

그런데 첫 특징으로 $x_1$이 아니라 $x_3$를 선택하면 2번째 단계에서 $x_2$를 선택함으로써 완벽한 분류를 할 수 있다.

하지만 이후의 상황을 첫 노드에서 특징을 결정할 때는 알 수 없다.

```
model = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=2).fit(X, y)
command_buf = io.StringIO()
export_graphviz(model, out_file=command_buf, 
                feature_names=["X1", "X2", "X3"])
graph = pydot.graph_from_dot_data(command_buf.getvalue())[0]
image = graph.create_png()
Image(image)
```

<img width="1036" alt="image" src="https://github.com/user-attachments/assets/7b7e06d5-cdbd-4400-a618-ba5a2074c974">

```
model = DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=3).fit(X, y)
command_buf = io.StringIO()
export_graphviz(model, out_file=command_buf, 
                feature_names=["X1", "X2", "X3"])
graph = pydot.graph_from_dot_data(command_buf.getvalue())[0]
image = graph.create_png()
Image(image)
```

<img width="732" alt="image" src="https://github.com/user-attachments/assets/6003621e-2d8c-4247-b573-601feb1e6ad8">

## Regression Tree
예측값 $\hat{y}$을 다음처럼 각 특징값 영역마다 고정된 값 $y_1, y_2$를 사용하고,

$$ 
\hat{y} = 
\begin{cases} 
y_1 & \text{ if } x \geq x_{\text{threshold}} \\ 
y_2 & \text{ if } x < x_{\text{threshold}}
\end{cases} $$

기준값 및 $y_1, y_2$를 선택하는 목적함수로 오차 제곱합을 사용하면 회귀분석을 할 수 있다.

이러한 모형을 회귀 나무(regression tree)라고 한다.

```
from sklearn.tree import DecisionTreeRegressor

rng = np.random.RandomState(1)
X = np.sort(5 * rng.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(16))

regtree = DecisionTreeRegressor(max_depth=10)
regtree.fit(X, y)

X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]
y_hat = regtree.predict(X_test)

# Plot the results
plt.figure()
plt.scatter(X, y, s=20, edgecolor="black", c="darkorange", label="Data")
plt.plot(X_test, y_hat, color="cornflowerblue", linewidth=2, label="Prediction")
plt.xlabel("x")
plt.ylabel(r"$y$ & $\hat{y}$")
plt.title("Regression Tree")
plt.legend()
plt.show()
```

<img width="727" alt="image" src="https://github.com/user-attachments/assets/bdbed8db-625a-4cba-8750-817714fcccdd">
