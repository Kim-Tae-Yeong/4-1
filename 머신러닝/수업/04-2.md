# Combining Datasets : Concat and Append
데이터를 다룰 때 가장 많이하는 작업은 다른 소스의 데이터를 결합는 것

dataframe은 기본적으로 SQL같은 table 구조를 가지고 있기 때문에 SQL처럼 데이터를 병합하는 작업을 할 수 있고, 매우 효율적으로 작동함

series와 dataframe은 `pd.concat`을 통해서 결합할 수 있음

dataframe을 편하게 만들도록 아래의 함수를 미리 정의
```
def make_df(cols, ind):
    """Quickly make a DataFrame"""
    data = {c: [str(c) + str(i) for i in ind]
            for c in cols}
    return pd.DataFrame(data, ind)

# example DataFrame
make_df('ABC', range(3))

"""
결과
    A   B   C
0  A0  B0  C0
1  A1  B1  C1
2  A2  B2  C2
"""
```

jupyter에서는 하나의 dataframe을 보여주는 것이 기본이기 때문에, 여러개의 dataframe을 나란히 표시하는 클래도 정의

jupyter의 \_repr_html_ method 사용
```
class display(object):
    """Display HTML representation of multiple objects"""
    template = """<div style="float: left; padding: 10px;">
    <p style='font-family:"Courier New", Courier, monospace'>{0}</p>{1}
    </div>"""
    def __init__(self, *args):
        self.args = args
        
    def _repr_html_(self):
        return '\n'.join(self.template.format(a, eval(a)._repr_html_())
                         for a in self.args)
    
    def __repr__(self):
        return '\n\n'.join(a + '\n' + repr(eval(a))
                           for a in self.args)
```

## Recall : Concatenation of NumPy Arrays
series와 dataframe의 concatenation은 numpy의 array concatenation인 np.concatenate와 매우 유사함
```
x = [1, 2, 3]
y = [4, 5, 6]
z = [7, 8, 9]
np.concatenate([x, y, z])

# axis를 키워드를 통해 연결될 축 정의
x = [[1, 2],
     [3, 4]]
np.concatenate([x, x], axis=1)

"""
결과
array([1, 2, 3, 4, 5, 6, 7, 8, 9])

array([[1, 2, 1, 2],
       [3, 4, 3, 4]])
"""
```

## Simple Concatenation with pd.concat
pandas에 존재하는 `pd.concat` 함수의 작동 방법은 np.concaneate와 비슷하지만, 훨신 다양한 기능을 제공함
```
pd.concat(objs, axis=0, join='outer', ignore_index=False, keys=None, 
          levels=None, names=None, verify_integrity=False, sort=False,
```

pd.concat은 series나 dataframe의 단순 연결을 위해 사용됨
```
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])
ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])
pd.concat([ser1, ser2])

# datframe도 연결 가능
df1 = make_df('AB', [1, 2])
df2 = make_df('AB', [3, 4])
display('df1', 'df2', 'pd.concat([df1, df2])')

# 기본적으로 이러한 연결은 옵션을 지정하지 않으면 행 단위로 이루어짐
# axis를 통해 다른 축으로 연결 가능
df3 = make_df('AB', [0, 1])
df4 = make_df('CD', [0, 1])
display('df3', 'df4', "pd.concat([df3, df4], axis=1)") # or axis="column"

"""
결과
1    A
2    B
3    C
4    D
5    E
6    F
dtype: object

df1
	 A	 B
1	A1	B1
2	A2	B2

df2
	 A	 B
3	A3	B3
4	A4	B4

pd.concat([df1, df2])
	 A	 B
1	A1	B1
2	A2	B2
3	A3	B3
4	A4	B4

df3
	 A	 B
0	A0	B0
1	A1	B1

df4
	 C	 D
0	C0	D0
1	C1	D1

pd.concat([df3, df4], axis=1)
	 A	 B	 C	 D
0	A0	B0	C0	D0
1	A1	B1	C1	D1
"""
```

### Duplicate Indices
np.concatenate와 pd.concat의 가장 중요한 차이는 결과에 중복 index가 있더라도 pandas의 경우 index가 유지된다는 것
```
x = make_df('AB', [0, 1])
y = make_df('AB', [2, 3])
y.index = x.index  # make duplicate indices!
display('x', 'y', 'pd.concat([x, y])')

"""
결과
x
	 A	 B
0	A0	B0
1	A1	B1

y
	 A	 B
0	A2	B2
1	A3	B3

pd.concat([x, y])
	 A	 B
0	A0	B0
1	A1	B1
0	A2	B2
1	A3	B3
"""
```

#### Catching the Repeats as an Error
단순히 `pd.cooncat` 결과의 index가 겹치지 않는지 확인하려면 `verify_intergity` 플래그를 지정하면 됨

true로 설정하면 중복 index가 있는 경우 연결에서 예외 발생
```
try:
    pd.concat([x, y], verify_integrity=True)
except ValueError as e:
    print("ValueError:", e)

"""
결과
ValueError: Indexes have overlapping values: Index([0, 1], dtype='int64')
"""
```

#### Ignoring the Index
index 값 자체가 중요하지 않으면 단순히 무시하는 것이 좋음

이 옵션은 `ignore_index` 플래그를 사용하여 지정할 수 있음

이 값을 true로 설정하면 concatenate는 series 새로운 정수 index 생성
```
display('x', 'y', 'pd.concat([x, y], ignore_index=True)')

"""
결과
x
	 A	 B
0	A0	B0
1	A1	B1

y
	 A	 B
0	A2	B2
1	A3	B3

pd.concat([x, y], ignore_index=True)
	 A	 B
0	A0	B0
1	A1	B1
2	A2	B2
3	A3	B3
"""
```

#### Adding MultiIndex Keys
또 다른 옵션은 `keys` 옵션을 사용하여 데이터 소스에 대한 레이블을 지정하는 것

결과는 데이터를 포함하는 multi index를 가짐
```
display('x', 'y', "pd.concat([x, y], keys=['x', 'y'])")

"""
결과
x
	 A	 B
0	A0	B0
1	A1	B1

y
	 A	 B
0	A2	B2
1	A3	B3

pd.concat([x, y], keys=['x', 'y'])
		 A	 B
x	0	A0	B0
	1	A1	B1
y	0	A2	B2
	1	A3	B3
"""
```

### Concatenation with Joins
방금까지는 주로 dataframe을 같은 column label만 가진 경우을 고려함

그런데 실제로 서로 다른 소스의 데이터는 서로 다른 column name 집합을 가질 수 있음

`pd.concat`은 이 경우에도 여러 옵션을 제공함
```
df5 = make_df('ABC', [1, 2])
df6 = make_df('BCD', [3, 4])
display('df5', 'df6', 'pd.concat([df5, df6])')

"""
결과
df5
	 A	 B	 C
1	A1	B1	C1
2	A2	B2	C2

df6
	 B	 C	 D
3	B3	C3	D3
4	B4	C4	D4

pd.concat([df5, df6])
	  A	 B	 C	  D
1	 A1	B1	C1	NaN
2	 A2	B2	C2	NaN
3	NaN	B3	C3	 D3
4	NaN	B4	C4	 D4
"""
```

기본적으로 데이터를 사용할 수 없는 항목은 NA 값으로 채워짐

이를 변경하기 위해 concatenate 함수의 `join` 및 `join_axes` 매개 변수에 대한 여러 옵션 중 하나를 지정할 수 있음

기본적으로 join은 outer가 기본임

하지만 inner로 변경하면, 겹치는 열만 사용해서 concatenate를 함
```
display('df5', 'df6',
        "pd.concat([df5, df6], join='inner')")

"""
결과
df5
	 A	 B	 C
1	A1	B1	C1
2	A2	B2	C2

df6
	 B	 C	 D
3	B3	C3	D3
4	B4	C4	D4

pd.concat([df5, df6], join='inner')
	 B	 C
1	B1	C1
2	B2	C2
3	B3	C3
4	B4	C4
"""
```

## The append Method
직접 배열 연길이 매우 일반적이기 때문에 series 및 dataframe 객체에는 더 적은 키 입력으로 동일한 작업을 수행 할 수 있는 `append` 함수가 정의되어 있음

예를 들어 구버전에서는 `pd.concat([df1, df2])`를 호출하는 대신 단순히 `df1.append(df2)`를 호출할 수 있었음

하지만 지금은 작동하지 않음

python list의 `append` 및 `extend` method와 달리 pandas의 `append` method는 원래 객체를 수정하지 않고 대신 새 객체를 생성함

또한 새로운 index와 데이터 버퍼를 생성하기 때문에 그리 효율적인 방법이 아님

따라서 합친 이후 추가적인 처리를 하려는 경우는 dataframe list를 통해 concat 함수를 사용하는 것이 좋음

# Combining Datasets : Merge and Join
pandas의 강력함 중 하나는 아주 빠른 고성능인 메모리 merge와 join 구현에 있음

편의를 위해 아까 정의한 `display` 기능을 재정의
```
class display(object):
    """Display HTML representation of multiple objects"""
    template = """<div style="float: left; padding: 10px;">
    <p style='font-family:"Courier New", Courier, monospace'>{0}</p>{1}
    </div>"""
    def __init__(self, *args):
        self.args = args
        
    def _repr_html_(self):
        return '\n'.join(self.template.format(a, eval(a)._repr_html_())
                         for a in self.args)
    
    def __repr__(self):
        return '\n\n'.join(a + '\n' + repr(eval(a))
                           for a in self.args)
```

## Relational Algebra
`pd.merge`는 관계형 데이터를 조작하기 위한 공식적인 규칙인 relational algebra의 하위 집합임

또한 대부분의 데이터베이스에서 사용할 수 있는 기능과 같은 개념을 가짐

관계형 대수 접근 방식의 강점은 모든 데이터 세트에서 더 복잡한 연산의 구성 요소가 되는 몇 가지 기본 연산이 정의되어 있음

db와 pandas에서는 이러한 기본 연산을 통해 복잡한 연산을 단순화시킬 수 있음

pandas는 SQL의 병합 기능과 비슷한 기능을 `pd.merge`와 `pd.join` 두 가지로 정의함

## Categories of Joins
`pd.merge` 함수는 1 : 1, 1 : n, n : n join과 같은 여러가지 유형의 join을 구현함

세 가지 연산방식은 모두 `pd.merge` 인터페이스를 사용해서 수행할 수 있고, 입력하는 데이터의 형태에 따라 달라짐

### One - To - One Joins
이는 아까 다룬 column에 대한 concat과 비슷함
```
df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})
df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],
                    'hire_date': [2004, 2008, 2012, 2014]})
display('df1', 'df2')

# pd.merge는 이를 employee 기준으로 병합
df3 = pd.merge(df1, df2)
df3

"""
결과
df1
  	employee	      group
0	     Bob	 Accounting
1	    Jake	Engineering
2	    Lisa	Engineering
3	     Sue	         HR

df2
 	 employee	hire_date
0	     Lisa	     2004
1	      Bob	     2008
2	     Jake	     2012
3	      Sue	     2014
  
 	 employee	      group	hire_date
0	      Bob	 Accounting	     2008
1	     Jake	Engineering	     2012
2	     Lisa	Engineering	     2004
3	      Sue	         HR	     2014
"""
```

`pd.merge` 함수는 각 dataframe에 employee column이 있음을 인식하고 column을 key로 사용하여 자동으로 join함

그 결과로 두 dataframe이 결합된 새 dataframe을 return함

이 때 각 column의 항목 순서가 반드시 유지되는 것은 아님

또한 아주 특수한 경우를 제외하고 merge는 index를 삭제해서 정수 index를 새로 부여함

### Many - To - One Joins
두 key column 중 하나에 중복 항목이 포함 된 경우를 말함

dataframe은 해당 중복 항목을 적절하게 남겨 둠
```
df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],
                    'supervisor': ['Carly', 'Guido', 'Steve']})
display('df3', 'df4', 'pd.merge(df3, df4)')

"""
결과
df3
  	employee	      group	  hire_date
0	     Bob	 Accounting	       2008
1	    Jake	Engineering	       2012
2	    Lisa	Engineering	       2004
3	     Sue	         HR	       2014

df4
          	group	  supervisor
0	   Accounting	       Carly
1	  Engineering	       Guido
2	           HR	       Steve

pd.merge(df3, df4)
 	 employee	      group	  hire_date	  supervisor
0	      Bob	 Accounting	       2008	       Carly
1	     Jake	Engineering	       2012	       Guido
2	     Lisa	Engineering	       2004	       Guido
3	      Sue	         HR	       2014	       Steve
"""
```

return된 dataframe에는 부서에 따른 "supervisor"의 정보가 모두 들어가 있음

Engineering은 두 명이 있으므로, 두 개에 같은 값(Guido)이 채워져있음

### Many - To - Many Joins
왼쪽 및 오른쪽 array의 key column에 모두 중복 항목이 포함된 경우 n : n 병합 수행
```
df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',
                              'Engineering', 'Engineering', 'HR', 'HR'],
                    'skills': ['math', 'spreadsheets', 'coding', 'linux',
                               'spreadsheets', 'organization']})
display('df1', 'df5', "pd.merge(df1, df5)")

"""
결과
df1
  	employee	      group
0	     Bob	 Accounting
1	    Jake	Engineering
2	    Lisa	Engineering
3	     Sue	         HR

df5
       	      group	        skills
0	 Accounting	          math
1	 Accounting	  spreadsheets
2	Engineering	        coding
3	Engineering	         linux
4	         HR	  spreadsheets
5	         HR	  organization

pd.merge(df1, df5)
  	employee	      group	        skills
0	     Bob	 Accounting	          math
1	     Bob	 Accounting	  spreadsheets
2	    Jake	Engineering	        coding
3	    Jake	Engineering	         linux
4	    Lisa	Engineering	        coding
5	    Lisa	Engineering	         linux
6	     Sue	         HR	  spreadsheets
7	     Sue	         HR	  organization
```

### Specification of the Merge Key
`pd.merge`는 두 입력 사이에 하나 이상의 일치하는 column 이름을 찾고 이것을 key로 사용함

그러나 종종 column 이름이 너무 일치하지 않지만 같은 데이터인 경우가 있음

`pd.merge`는 이를 처리하기 위한 다양한 옵션을 제공함

#### The on Keyword
가장 간단하게는 column 이름 또는 column 이름 list를 on 키워드에 입력하여 병합 기준 column을 명시적으로 지정할 수 있음
```
display('df1', 'df2', "pd.merge(df1, df2, on='employee')")

"""
결과
df1
  	employee	      group
0	     Bob	 Accounting
1	    Jake	Engineering
2	    Lisa	Engineering
3	     Sue	         HR

df2
  	employee	hire_date
0	    Lisa	     2004
1	     Bob	     2008
2	    Jake	     2012
3	     Sue	     2014

pd.merge(df1, df2, on='employee')
  	employee	      group	  hire_date
0	     Bob	 Accounting	       2008
1	    Jake	Engineering	       2012
2	    Lisa	Engineering	       2004
3	     Sue	         HR	       2014
"""
```

이 옵션은 왼쪽 및 오른쪽 dataframe 모두에 지정된 column과 같은 이름이 있는 경우에만 작동함

#### The left_on and right_on Keywords
때로는 column 이름이 다른 두 개의 데이터 세트를 병합할 수 있음

예를 들어 직원 이름이 "employee"가 아닌 "name"으로 되어있는 dataset의 경우 left_on과 right_on 키워드를 사용하여 각 dataframe의 열 이름을 지정할 수 있음
```
df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'salary': [70000, 80000, 120000, 90000]})
display('df1', 'df3', 'pd.merge(df1, df3, left_on="employee", right_on="name")')

# 결과에는 원하는 경우 삭제할 수 있는 중복 column 존재
# 이런 경우 drop을 통해 삭제할 수 있음
pd.merge(df1, df3, left_on="employee", right_on="name").drop('name', axis=1)

"""
결과
df1
 	 employee	      group
0	      Bob	 Accounting
1	     Jake	Engineering
2	     Lisa	Engineering
3	      Sue	         HR

df3
  	name	salary
0	 Bob	 70000
1	Jake	 80000
2	Lisa	120000
3	 Sue	 90000

pd.merge(df1, df3, left_on="employee", right_on="name")
  	employee	      group	name	salary
0	     Bob	 Accounting	 Bob	 70000
1	    Jake	Engineering	Jake	 80000
2	    Lisa	Engineering	Lisa	120000
3	     Sue	         HR	 Sue	 90000

	employee	      group	salary
0	     Bob	 Accounting	 70000
1	    Jake	Engineering	 80000
2	    Lisa	Engineering	 120000
3	     Sue	         HR	 90000
"""
```

#### The left_index and right_index Keywords
때로는 column을 병합하는 대신 index를 병합해야 할 때가 있음

사실 index를 병합하는 것이 더 빠름
```
df1a = df1.set_index('employee')
df2a = df2.set_index('employee')
display('df1a', 'df2a')

# pd.merge에서 left_index와 right_index 플래그를 지정하여 index를 병합 key로 사용할 수 있음
display("pd.merge(df1a, df2a, left_index=True, right_index=True)")

# dataframe은 또 이러한 index 기반 병합을 수행하는 join method 존재
display('df1a.join(df2a)')

# index와 column을 혼합해서 병합 key로 쓰러면 left_index를 right_on과 결합하거나 반대로 결합하면 됨
display('df1a', 'df3', "pd.merge(df1a, df3, left_index=True, right_on='name')")

"""
결과
df1a
employee	      group
     Bob	 Accounting
    Jake	Engineering
    Lisa	Engineering
     Sue	         HR

df2a
employee   hire_date
    Lisa	2004
     Bob	2008
    Jake	2012
     Sue	2014

pd.merge(df1a, df2a, left_index=True, right_index=True)
employee 	      group	hire_date
     Bob	 Accounting	     2008
    Jake	Engineering	     2012
    Lisa	Engineering	     2004
     Sue		 HR	     2014

df1a.join(df2a)
employee 	      group	hire_date
     Bob	 Accounting	     2008
    Jake	Engineering	     2012
    Lisa	Engineering	     2004
     Sue		 HR	     2014

df1a
employee	      group
     Bob	 Accounting
    Jake	Engineering
    Lisa	Engineering
     Sue	         HR

df3
  	name	salary
0	 Bob	 70000
1	Jake	 80000
2	Lisa	120000
3	 Sue	 90000

pd.merge(df1a, df3, left_index=True, right_on='name')
	      group	name	salary
0	 Accounting	 Bob	 70000
1	Engineering	Jake	 80000
2	Engineering	Lisa	 120000
3		 HR	 Sue	 90000
"""
```

### Specifying Set Arithmetic for Joins
이런 병합은 집합의 연산 방식도 지정할 수 있음
```
# 예를 들어 하나의 key column에는 값이 있고, 다른 하나에는 없는 경우
df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],
                    'food': ['fish', 'beans', 'bread']},
                   columns=['name', 'food'])
df7 = pd.DataFrame({'name': ['Mary', 'Joseph'],
                    'drink': ['wine', 'beer']},
                   columns=['name', 'drink'])
display('df6', 'df7', 'pd.merge(df6, df7)')

"""
결과
df6
	 name	 food
0	Peter	 fish
1	 Paul	beans
2	 Mary	bread

df7
	  name	drink
0	  Mary	 wine
1	Joseph	 beer

pd.merge(df6, df7)
	name	 food	drink
0	Mary	bread	 wine
"""
```

여기에서 공통된 "name"은 Mary밖에 없음

기본적으로 결과에는 두 입력 집합의 교집합이 사용됨

이것을 inner join이라고 부름

이러한 병합 방법은 how 키워드로 지정 가능
```
pd.merge(df6, df7, how='inner')

# how 키워드에 대해서는 outer, left, right 옵션 존재
# outer join은 합집합을 사용하고, 모든 누락값을 NA로 채움
display("pd.merge(df6, df7, how='outer')")

# left join 및 right join은 각각 왼쪽 항목과 오른쪽 항목을 기준으로 병합
display("pd.merge(df6, df7, how='left')", "pd.merge(df6, df7, how='right')")

"""
결과
	name	 food	drink
0	Mary	bread	 wine

pd.merge(df6, df7, how='outer')
	  name	 food	drink
0	Joseph	  NaN	 beer
1	  Mary	bread	 wine
2	  Paul	beans	  NaN
3	  Peter	 fish	  NaN

pd.merge(df6, df7, how='left')
	 name	 food	drink
0	Peter	 fish	  NaN
1	 Paul	beans	  NaN
2	 Mary	bread	 wine

pd.merge(df6, df7, how='right')
	  name	 food	drink
0	  Mary	bread	 wine
1	Joseph	  NaN	 beer
"""
```

### Overlapping Column Names : The suffixes Keyword
마지막으로 두 입력 dataframe에 충돌하는 column 이름이 있을 수 있음
```
df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'rank': [1, 2, 3, 4]})
df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
                    'rank': [3, 1, 4, 2]})
display('df8', 'df9', 'pd.merge(df8, df9, on="name")')

"""
결과
df8
	name	rank
0	 Bob	   1
1	Jake	   2
2	Lisa	   3
3	 Sue	   4

df9
	name	rank
0	 Bob	   3
1	Jake	   1
2	Lisa	   4
3	 Sue	   2

pd.merge(df8, df9, on="name")
	name	rank_x	rank_y
0	 Bob	     1	     3
1	Jake	     2	     1
2	Lisa	     3	     4
3	 Sue	     4	     2
"""
```

두 개의 충돌하는 column 이름이 있기 때문에 merge 함수는 자동으로 접미사 `_x`와 `_y`를 추가하여 이름을 겹치지 않게 해줌

이러한 기본값이 적절하지 않은 경우 `suffixes` 키워드를 사용하여 사용자 지정 접미사를 사용할 수 있음
```
display('pd.merge(df8, df9, on="name", suffixes=["_L", "_R"])')

"""
결과
pd.merge(df8, df9, on="name", suffixes=["_L", "_R"])

	name	rank_L	rank_R
0	 Bob	     1	     3
1	Jake	     2	     1
2	Lisa	     3	     4
3	 Sue	     4	     2
"""
```

# Aggregation and Grouping
대용량 데이터를 분석하기 위해서는 평균, 총합, 중간값, 최대값, 최소값 등의 요약 통계를 보는 것이 매우 중요함

pandas는 group by라는 기능을 통해 정해진 속성별로 이런 대표값을 구할 수 있음

편의를 위해 `display` 함수를 다시 정의
```
class display(object):
    """Display HTML representation of multiple objects"""
    template = """<div style="float: left; padding: 10px;">
    <p style='font-family:"Courier New", Courier, monospace'>{0}</p>{1}
    </div>"""
    def __init__(self, *args):
        self.args = args
        
    def _repr_html_(self):
        return '\n'.join(self.template.format(a, eval(a)._repr_html_())
                         for a in self.args)
    
    def __repr__(self):
        return '\n\n'.join(a + '\n' + repr(eval(a))
                           for a in self.args)
```

## Planets Data
```
import seaborn as sns
planets = sns.load_dataset('planets')
planets.shape

planets.head()

"""
결과
(1035, 6)

		 method	number	orbital_period	 mass	distance	year
0	Radial Velocity	     1	       269.300	 7.10	   77.40	2006
1	Radial Velocity	     1	       874.774	 2.21	   56.95	2008
2	Radial Velocity	     1	       763.000	 2.60	   19.84	2011
3	Radial Velocity	     1	       326.030	19.40	  110.62	2007
4	Radial Velocity	     1	       516.220	10.50	  119.47	2009
"""
```

## Simple Aggregation in Pandas
numpy 배열과 마찬가지로 pandas series는 단일 값 반환
```
rng = np.random.RandomState(42)
ser = pd.Series(rng.rand(5))
ser

ser.sum()

ser.mean()

# dataframe의 경우 기본적으로 aggregation을 사용하면 각 column마다 결과를 반환
df = pd.DataFrame({'A': rng.rand(5),
                   'B': rng.rand(5)})
df

df.mean()

# axis argument를 지정하면 row마다 평균을 구할 수 있음
df.mean(axis='columns')

# pandas series와 dataframe은 기본적인 aggregation method들을 모두 구현해둠
# 그리고 하나씩 계산할 필요 없이 각 column에 대한 몇 가지 공통 집계를 계산하고 결과를 반환하는 편리한 describe method 존재
planets.dropna().describe()

"""
결과
0    0.374540
1    0.950714
2    0.731994
3    0.598658
4    0.156019
dtype: float64

np.float64(2.811925491708157)

np.float64(0.5623850983416314)

	       A	       B
0	0.155995	0.020584
1	0.058084	0.969910
2	0.866176	0.832443
3	0.601115	0.212339
4	0.708073	0.181825

A    0.477888
B    0.443420
dtype: float64

0    0.088290
1    0.513997
2    0.849309
3    0.406727
4    0.444949
dtype: float64

	   number	orbital_period	      mass	  distance	       year
count	498.00000	    498.000000	498.000000	498.000000	 498.000000
mean	  1.73494	    835.778671	  2.509320	 52.068213	2007.377510
std	  1.17572	   1469.128259	  3.636274	 46.596041	   4.167284
min	  1.00000	      1.328300	  0.003600	  1.350000	1989.000000
25%	  1.00000	     38.272250	  0.212500	 24.497500	2005.000000
50%	  1.00000	    357.000000	  1.245000	 39.940000	2009.000000
75%	  2.00000	    999.600000	  2.867500	 59.332500	2011.000000
max	  6.00000	  17337.500000	  25.000000	354.000000	2014.000000
"""
```

## GroupBy : Split, Apply, Combine
때로는 조건부로 구한 값들의 aggregation이 더 필요한 경우가 있음

이런 경우 groupby라는 함수 사용

### Split, Apply, Combine
- split : 지정된 key의 값에 따라 dataframe을 분리하고 그룹화함
- apply : 개별 그룹 내에서 일반적으로 aggregation, transformation, filtering 등 수행
- combine : 이러한 작업의 결과를 출력 array로 병합
```
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'data': range(6)}, columns=['key', 'data'])
df

# 가장 기본적인 split - apply - combine 연산은 dataframe의 groupby method를 사용하여 원하는 key column 지정
df.groupby('key')

# return되는 결과물이 dataframe이 아니라 dataframegroupby 객체
# 이 객체는 실제로 apply 단계를 사용하기 전에 계산을 수행하기 위한 dataframe 조회용 객체
# 이 객체에 aggregation method를 적용하면 실제 원하는 결과가 나옴
df.groupby('key').sum()

"""
	key	data
0	  A	   0
1	  B	   1
2	  C	   2
3	  A	   3
4	  B	   4
5	  C	   5

<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7ff91204d550>


key	data
  A	   3
  B	   5
  C	   7
"""
```

### The GroupBy Object
groupby 객체는 고도로 추상화 되어있고, 굉장히 유연하게 사용 가능

때때로는 dataframe의 집합처럼 간단히 쓸 수 있고, 내부적으로 복잡한 함수를 적용할 수도 있음

groupby를 통해서 aggregation, filter, transform, map, apply 등을 수행하기 전에 기본적인 작동을 알아보자

#### Column Indexing
groupby object는 dataframe과 동일한 방식으로 column indexing 지원
```
planets.groupby('method')

planets.groupby('method')['orbital_period']

planets.groupby('method')['orbital_period'].median()

"""
결과
<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7ff91204e2a0>

<pandas.core.groupby.generic.SeriesGroupBy object at 0x7ff91204e5a0>

method
Astrometry                         631.180000
Eclipse Timing Variations         4343.500000
Imaging                          27500.000000
Microlensing                      3300.000000
Orbital Brightness Modulation        0.342887
Pulsar Timing                       66.541900
Pulsation Timing Variations       1170.000000
Radial Velocity                    360.200000
Transit                              5.714932
Transit Timing Variations           57.011000
Name: orbital_period, dtype: float64
"""
```

#### Iteration Over Groups
groupby object는 직접적으로 group에 대한 iteration을 지원하지만, 효율적이지 않음
```
for (method, group) in planets.groupby('method'):
    print("{0:30s} shape={1}".format(method, group.shape))

"""
결과
Astrometry                     shape=(2, 6)
Eclipse Timing Variations      shape=(9, 6)
Imaging                        shape=(38, 6)
Microlensing                   shape=(23, 6)
Orbital Brightness Modulation  shape=(3, 6)
Pulsar Timing                  shape=(5, 6)
Pulsation Timing Variations    shape=(1, 6)
Radial Velocity                shape=(553, 6)
Transit                        shape=(397, 6)
Transit Timing Variations      shape=(4, 6)
"""
```

이 기능은 특정 작업을 수동으로 수행하는 데 유용할 수 있지만 apply method를 사용하는 것이 일반적으로 훨씬 더 빠름

#### Dispatch Methods
groupby object에 명시적으로 구현되지 않은 모든 methods는 그룹별로 series나 dataframe에 구현된 연산으로 작동함
```
planets.groupby('method')['year'].describe().unstack()

"""
결과
       method                       
count  Astrometry                          2.0
       Eclipse Timing Variations           9.0
       Imaging                            38.0
       Microlensing                       23.0
       Orbital Brightness Modulation       3.0
                                         ...  
max    Pulsar Timing                    2011.0
       Pulsation Timing Variations      2007.0
       Radial Velocity                  2014.0
       Transit                          2014.0
       Transit Timing Variations        2014.0
Length: 80, dtype: float64
"""
```

### Aggregate, Filter, Transfomr, Apply
```
rng = np.random.RandomState(0)
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
                   'data1': range(6),
                   'data2': rng.randint(0, 10, 6)},
                   columns = ['key', 'data1', 'data2'])
df

"""
결과
	key	data1	data2
0	  A	    0	    5
1	  B	    1	    0
2	  C	    2	    3
3	  A	    3	    3
4	  B	    4	    7
5	  C	    5	    9
"""
```

#### Aggregation
```
df.groupby('key').aggregate(['min', "median", "max"])

df.groupby('key').aggregate({'data1': 'min',
                             'data2': 'max'})

df.groupby('key').aggregate({'data1': ['min', "mean"],
                             'data2': ["median", 'max']})

"""
결과
			data1			data2
	min	median	  max	min	median	  max
key						
  A	  0	   1.5	    3	  3	   4.0	    5
  B	  1	   2.5	    4	  0	   3.5	    7
  C	  2	   3.5	    5	  3	   6.0	    9

	data1	data2
key		
  A	    0	    5
  B	    1	    7
  C	    2	    9

		data1		data2
	min	 mean	median	  max
key				
A	  0	  1.5	   4.0	    5
B	  1	  2.5	   3.5	    7
C	  2	  3.5	   6.0	    9
"""
```

#### Filtering
filtering 작업을 사용하면 그룹 속성에 따라 데이터를 삭제할 수 있음
```
# 표준 편차가 일부 임계값보다 큰 모든 그룹 유지
def filter_func(x):
    return x['data2'].std() > 4

display('df', "df.groupby('key').std()", "df.groupby('key').filter(filter_func)")

"""
결과
df
	key	data1	data2
0	  A	    0	    5
1	  B	    1	    0
2	  C	    2	    3
3	  A	    3	    3
4	  B	    4	    7
5	  C	    5	    9

df.groupby('key').std()
	  data1	   data2
key		
  A	2.12132	1.414214
  B	2.12132	4.949747
  C	2.12132	4.242641

df.groupby('key').filter(filter_func)
	key	data1	data2
1	  B	    1	    0
2	  C	    2	    3
4	  B	    4	    7
5	  C	    5	    9
"""
```

#### Transformation
transformation은 원 데이터를 결합 후 변환한 결과를 원 dataframe과 같은 형태로 return해줌
```
# 데이터값을 그 평균값으로 빼주는 작업
df.groupby('key').transform(lambda x: x - x.mean())

"""
결과
	data1	data2
0	 -1.5	  1.0
1	 -1.5	 -3.5
2	 -1.5	 -3.0
3	  1.5	 -1.0
4	  1.5	  3.5
5	  1.5	  3.0
"""
```

#### The apply Method
`apply` method를 사용하면 groupby 결과에 임의의 함수를 적용할 수 있음

함수는 dataframe을 가져와 pandas 객체(series or dataframe) 또는 스칼라 변수를 반환해야 함

결합 작업은 반환되는 출력 유형에 맞게 자동으로 조정됨
```
# data2값의합을 통해서 data1을 정규화하는 norm_by_data2 함수를 정의해서 apply
def norm_by_data2(x):
    # x is a DataFrame of group values
    x['data1'] /= x['data2'].sum()
    return x

display('df', "df.groupby('key').apply(norm_by_data2, include_groups=False)")

"""
결과
df
	key	data1	data2
0	 A	    0	    5
1	 B	    1	    0
2	 C	    2	    3
3	 A	    3	    3
4	 B	    4	    7
5	 C	    5	    9

df.groupby('key').apply(norm_by_data2, include_groups=False)
		   data1	data2
key			
  A	0	0.000000	    5
	3	0.375000	    3
  B	1	0.142857	    0
	4	0.571429	    7
  C	2	0.166667	    3
	5	0.416667	    9
"""
```

### Specifying the Split Key
지금까지는 한 개의 column을 가지고 groupby를 수행했지만 이외에도 훨씬 복잡하고 다양한 방법으로 groupby 수행 가능

#### A List, Array, Series or Index Providing the Grouping Keys
groupby의 key는 그룹을 지정하는 임의의 list로 주어질 수 있음
```
L = [0, 1, 0, 1, 2, 0]
display('df', 'df.groupby(L).sum()')

# 위의 문법을 생각해보면 df.groupby('key')는 아래처럼 쓸 수 있다는 것을 알 수 있음
display("df.groupby(df['key']).sum()")

"""
결과
df
	key	data1	data2
0	 A	    0	    5
1	 B	    1	    0
2	 C	    2	    3
3	 A	    3	    3
4	 B	    4	    7
5	 C	    5	    9

df.groupby(L).sum()
	key	data1	data2
0	ACC	    7	   17
1	 BA	    4	    3
2	  B	    4	    7

df.groupby(df['key']).sum()

	data1	data2
key		
  A	    3	    8
  B	    5	    7
  C	    7	   12
"""
```

#### A Dictionary or Series Mapping Index to Group
또 다른 방법은 index 값을 gruop key에 mapping하는 dict을 사용하는 것
```
# B와 C를 동일 group으로 묶기
df2 = df.set_index('key')
mapping = {'A': 'vowel', 'B': 'consonant', 'C': 'consonant'}
display('df2', 'df2.groupby(mapping).sum()')

"""
결과
df2
	data1	data2
key		
  A	    0	    5
  B	    1	    0
  C	    2	    3
  A	    3	    3
  B	    4	    7
  C	    5	    9

df2.groupby(mapping).sum()
		data1	data2
      key		
consonant	   12	   19
    vowel	    3	    8
"""
```

#### Any Python Functions
mapping과 마찬가지로 index 값을 입력하고 group 이름을 출력하는 모든 python 함수 사용 가능
```
display('df2.groupby(str.lower).mean()')

"""
결과
df2.groupby(str.lower).mean()

	data1	data2
key		
  a	  1.5	  4.0
  b	  2.5	  3.5
  c	  3.5	  6.0
"""
```

#### A List of Valid Keys
또한 여러 개의 항목을 결합하여 multi index로 group화 할 수 있음
```
df2.groupby([str.lower, mapping]).mean()

"""
결과

			data1	data2
key	      key		
  a	    vowel	  1.5	  4.0
  b	consonant	  2.5	  3.5
  c	consonant	  3.5	  6.0
"""
```

### Grouping Example
```
decade = 10 * (planets['year'] // 10)
decade = decade.astype(str) + 's'
decade.name = 'decade'
planets.groupby(['method', decade])['number'].sum().unstack().fillna(0)

"""
결과
			decade	1980s	1990s	2000s	2010s
			method				
		    Astrometry	  0.0	  0.0	  0.0	  2.0
     Eclipse Timing Variations	  0.0	  0.0	  5.0	 10.0
		       Imaging	  0.0	  0.0	 29.0	 21.0
		  Microlensing	  0.0	  0.0	 12.0	 15.0
 Orbital Brightness Modulation	  0.0	  0.0	  0.0	  5.0
		 Pulsar Timing	  0.0	  9.0	  1.0	  1.0
   Pulsation Timing Variations	  0.0	  0.0	  1.0	  0.0
	       Radial Velocity	  1.0	 52.0	475.0	424.0
		       Transit	  0.0	  0.0	 64.0	712.0
     Transit Timing Variations	  0.0	  0.0	  0.0	  9.0
"""
```
