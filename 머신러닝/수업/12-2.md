# 나이브 베이즈 분류기
## 간단한 베이즈 분류기 구현하기
### 데이터 만들기

```
import pandas as pd
import numpy as np

viagra_spam = {'viagra': [1,0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,1],
               'spam': [1,0,0,0,0,0,1,0,1,0, 0,0,0,0,0,0,0,1,1,1]}
# 교과서는 df로 되어있으나, 겹치는 이름을 안 만들기 위해...
df_spam = pd.DataFrame(viagra_spam, columns = ['viagra', 'spam']) 
np_data = df_spam.values
```

### 확률 구하기
메일에 viagra가 있을 확률 : P(viagra)

```
p_viagra = sum(np_data[:, 0] == 1) / len(np_data)
p_viagra

"""
결과
0.3
"""
```

메일이 spam일 확률 : P(spam)

```
p_spam = sum(np_data[:, 1] == 1) / len(np_data)
p_spam

"""
결과
0.3
"""
```

viagra를 포함하면서 스팸메일일 확률 : P(viagra ∩ spam)

```
p_v_cap_s = sum((np_data[:, 0] == 1) & (np_data[:, 1] == 1)) / len(np_data)
p_v_cap_s

"""
결과
0.15
"""
```

viagra를 포함하지 않으면서 스팸메일일 확률 : P(¬virgra ∩ spam)

```
p_n_v_cap_s = sum((np_data[:, 0] == 0) & (np_data[:, 1] == 1)) / len(np_data)
p_n_v_cap_s

"""
결과
0.15
"""
```

위 확률들을 정리해서 P(spam | viagra)를 구해보자.
- P(spam | viagra) = P(viagra | spam) * P(spam) / P(viagra) = (P(viagra ∩ spam) / P(spam) * P(spam)) / P(viagra)

```
# 사실 수식적으론 두개가 동일하다.
# 하지만 우리는 원칙상으로 교집합의 확률을 "모른다" 라고 일단 가정하자.

# p_spam * (p_v_cap_s / p_spam ) / p_viagra
p_v_cap_s / p_viagra

"""
결과
0.5
"""
```

이제 viagra가 없는데 스팸메일일 확률을 구해보자.
- P(spam | ¬viagra) = P(¬viagra | spam) * P(spam) / P(¬viagra) = (P(¬viagra ∩ spam) / P(spam) * P(spam)) / P(¬viagra)

```
p_spam * (p_n_v_cap_s / p_spam ) / (1-p_viagra)

"""
결과
0.2142857142857143
"""
```

## 나이브 베이지안 분류기 만들기
간단한 베이즈 분류기는 하나의 변수만 고려한다.

하지만 실제로는 다양한 변수가 작용할 수 있다.
- viagra 말고 다른 약의 이름이 있다던지 등

교과서 예제인 fraud 데이터를 사용해 베이즈 분류기를 만들어보자.
- 이 데이터는 3개의 feature를 사용해서 대출사기 여부를 판단하는 데이터이다.

```
import pandas as pd
import numpy as np

data_url = "W12/fraud.csv"
df_fraud = pd.read_csv(data_url, sep=',')
df_fraud.head()
```

<img width="455" alt="image" src="https://github.com/user-attachments/assets/4124c787-5b42-4677-88a6-56e1f4b8a7f8">

일단 T / F 데이터를 ont - hot - encoding으로 처리해보자.

```
del df_fraud["ID"]
Y_data = df_fraud.pop("Fraud")
Y_data = Y_data.values
x_df = pd.get_dummies(df_fraud)
x_df.head(10).T
```

<img width="750" alt="image" src="https://github.com/user-attachments/assets/8df06104-e5c2-48b4-b92e-4b83a38a4199">

모델 입력을 위해 넘파이 배열 형태로 X 데이터를 바꿔준다.

```
x_data = x_df.values
x_data

"""
결과
array([[False,  True, False, False, False, False,  True, False,  True,
        False],
       [False, False, False,  True, False, False,  True, False,  True,
        False],
       [False, False, False,  True, False, False,  True, False,  True,
        False],
       [False, False, False,  True, False,  True, False, False, False,
         True],
       [ True, False, False, False, False, False,  True, False,  True,
        False],
       [ True, False, False, False, False, False,  True, False,  True,
        False],
       [False,  True, False, False, False, False,  True, False,  True,
        False],
       [ True, False, False, False, False, False,  True, False,  True,
        False],
       [False,  True, False, False, False, False,  True, False, False,
         True],
       [False, False,  True, False, False, False,  True, False,  True,
        False],
       [False,  True, False, False,  True, False, False, False,  True,
        False],
       [False,  True, False, False, False, False,  True, False,  True,
        False],
       [False,  True, False, False, False, False,  True, False, False,
...
        False],
       [ True, False, False, False, False, False,  True, False,  True,
        False],
       [False, False, False,  True, False, False,  True, False,  True,
        False]])
"""
```

나이브 베이지안은 독립 사건을 가정한다.

그래서 아래와 같은 수식을 쓸 수 있다.

$P(Y_c|X_1, \cdots, X_n)=P(Y_c)\prod_{i=1}^{n}\frac{P(X_i|Y_c)}{P(X_i)}=\frac{P(Y_c)\prod_{i=1}^{n}P(X_i|Y_c)}{\prod_{i=1}^{n}P(X_i)}$

where $Y_c$ is a label

먼저 P(Y_c = True)와 P(Y_c = False)를 구해보자.

```
P_Y_True = sum(Y_data==True) / len(Y_data)
P_Y_False = 1 - P_Y_True

P_Y_True,P_Y_False

"""
결과
(0.3, 0.7)
"""
```

다음으로 P($X_i$ | $Y_c$)를 표현하기 위해 $Y_c$의 라벨을 기반으로 한 값을 `np.where`로 출력해보자.
- 이 경우에는 True의 index를 반환한다.

```
np.where(Y_data)

"""
결과
(array([ 0,  3,  5,  9, 11, 12]),)
"""
```

True와 False의 위치를 반환받는다.

```
ix_Y_True = np.where(Y_data)
ix_Y_False = np.where(Y_data==False)

ix_Y_True, ix_Y_False

x_data.sum(axis=0)/len(Y_data)

"""
결과
((array([ 0,  3,  5,  9, 11, 12]),),
 (array([ 1,  2,  4,  6,  7,  8, 10, 13, 14, 15, 16, 17, 18, 19]),))

array([0.35, 0.35, 0.05, 0.25, 0.1 , 0.05, 0.85, 0.05, 0.75, 0.2 ])
"""
```

P($X_i$ | $Y_{True}$)를 구하려면 아래처럼 하면 된다.
- x의 각 feature에 대해서, Y_data가 True인 케이스 중 몇 개나 그 feature가 True였는지를 계산한다.

덤으로 P($X_i$ | $Y_{False}$)도 구해보자.

```
p_x_y_true = (x_data[ix_Y_True].sum(axis=0)) / sum(Y_data==True)
p_x_y_false = (x_data[ix_Y_False].sum(axis=0)) / sum(Y_data==False)

p_x_y_true, p_x_y_false

"""
결과
(array([0.16666667, 0.5       , 0.16666667, 0.16666667, 0.        ,
        0.16666667, 0.83333333, 0.        , 0.66666667, 0.33333333]),
 array([0.42857143, 0.28571429, 0.        , 0.28571429, 0.14285714,
        0.        , 0.85714286, 0.07142857, 0.78571429, 0.14285714]))
"""
```

이제 가상의 test data를 보자.

교과서는 아래처럼 원래 수식이 아니라 이상한 형태로 구한다.
- 이 식은 실제 확률을 구해주지 않는다.

```
print(x_df.columns)
x_test = [0,1,0,0,0,1,0,0,1,0]

p_y_true_test = P_Y_True + p_x_y_true.dot(x_test)
p_y_false_test = P_Y_False + p_x_y_false.dot(x_test)

print(p_y_true_test , p_y_false_test)

# 그래서 false일 확률이 더 높다.
p_y_true_test < p_y_false_test

"""
결과
Index(['History_arrears', 'History_current', 'History_none', 'History_paid',
       'CoApplicant_coapplicant', 'CoApplicant_guarantor', 'CoApplicant_none',
       'Accommodation_free', 'Accommodation_own', 'Accommodation_rent'],
      dtype='object')
1.6333333333333333 1.7714285714285714

True
"""
```

이렇게 구한 이유는 실제 확률 식에 위 값들을 대입하면 제대로 구해지지 않기 때문이다.
- x_test에 p_x_y_true의 성분에 0 값이 있기 때문이다.

```
p_y_true_test = P_Y_True * np.prod(p_x_y_true) / np.prod(x_data.sum(axis=0)/len(Y_data))
p_y_false_test = P_Y_False * np.prod(np.multiply(p_x_y_false, x_test)) / np.prod(x_data.sum(axis=0)/len(Y_data))

print(p_y_true_test , p_y_false_test)

"""
결과
0.0 0.0
"""
```

## scikit - learn을 이용한 나이브 베이지안 분류기
우리의 구현체에는 무언가 문제가 있다.

이제 이미 잘 구현된 `scikit-learn`을 사용해보자.

먼저 문장을 sports / not sports로 구분하는 간단한 모델을 만들어보자.

```
y_example_text = ["Sports", "Not sports", "Sports", "Sports", "Not sports"]
y_example = [1 if c=="Sports" else 0 for c in y_example_text ]
text_example = ["A great game game", 
                "The The election was over",
                "Very clean game match",
                "A clean but forgettable game game",
                "It was a close election", ]
```

이러한 텍스트 데이터에 대해서는 먼저 텍스트 데이터를 벡터화시켜야 한다.
- 다양한 기법이 있지만, 여기서는 가장 단순한 bag - of - words를 써보자.
- BOW는 ont - hot - encoding으로 단어를 index화 시키고, 각 문장에 특정 단어가 몇 개나 들어있는지를 가지고 벡터화 시킨다.
- `CounterVerctorizer`로 변환한다.

```
from sklearn.feature_extraction.text import CountVectorizer

countvect_example = CountVectorizer()
X_example = countvect_example.fit_transform(text_example)
countvect_example.get_feature_names_out()

"""
결과
array(['but', 'clean', 'close', 'election', 'forgettable', 'game',
       'great', 'it', 'match', 'over', 'the', 'very', 'was'], dtype=object)
"""
```

쓰기 편하게 array로 바꿔보자.

```
countvect_example.transform(text_example).toarray()

countvect_example.vocabulary_

"""
결과
array([[0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1],
       [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0],
       [1, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]])

{'great': 6,
 'game': 5,
 'the': 10,
 'election': 3,
 'was': 12,
 'over': 9,
 'very': 11,
 'clean': 1,
 'match': 8,
 'but': 0,
 'forgettable': 4,
 'it': 7,
 'close': 2}
"""
```

### 베르누이 나이브 베이지안 분류기
`sklearn`에 구현된 나이브 베이즈 분류기에는 여러 개가 있다.
- 이번에는 베르누이 나이브 베이즈 분류기를 생각해보자.

베르부이 분류기는 모든 데이터가 boolean이라고 가정한다.
- 그래서 정수값이 들어가면 어떤 임계값(threshold) 기준으로 T / F로 변환한다.

```
from sklearn.naive_bayes import BernoulliNB

clf = BernoulliNB(alpha=1, binarize=0)
clf.fit(X_example, y_example)
```

fit을 한 이후에는 다양한 값들을 제공해준다.
- 예를 들어서 아래의 `class_log_prior_`는 각 클래스의 확률의 로그값을 보여준다.(여기서는 sports / not sports)

```
print(clf.class_log_prior_)
print(np.exp(clf.class_log_prior_))

"""
결과
[-0.91629073 -0.51082562]
[0.4 0.6]
"""
```

### 다항 나이브 베이즈 분류기
다항 나이브 분류기는 베르누이 분류기와 다르게 각 feature들이 이산형이지만, 이진 값이 아니라 다양한 값을 가질 수 있다.

이런 경우에는 아래의 식에서

$P(Y_c|X_1, \cdots, X_n)=P(Y_c)\prod_{i=1}^{n}\frac{P(X_i|Y_c)}{P(X_i)}=\frac{P(Y_c)\prod_{i=1}^{n}P(X_i|Y_c)}{\prod_{i=1}^{n}P(X_i)}$

$P(X_i|Y_c) = \frac{\sum{tf(x_i, d \in Y_c)} + \alpha}{\sum {N_d \in Y_c} + \alpha V}$ 형태로 가능도 계산을 바꿔주게 된다.
- $x_i$
  - feature vector에 존재하는 단어를 의미한다.
  - 각 단어는 index i를 할당한다.
 
- $\sum{tf(x_i, d \in Y_c)}$
  - 각 단어 $x_i$가 클래스 $Y_c$에 속하는 모든 문서에 존재하는 개수이다.
    - 예를 들어 game이라는 단어가 sports class 문서들에 몇 번이나 나오는지 등이다.
   
- $\alpha$
  - smoothing parameter로 위에서 본 카운트가 0인 값으로 인해 발생하는 문제를 제거해주는 역할을 한다.
    - 위를 보면 곱이 0인 경우가 나온다.
   
- $\sum N_d \in Y_c$
  - $Y_c$에 해당하는 문서들에 존재하는 모든 단어의 합이다.

- $V$ : 모든 단어의 수, 즉 feature의 수(feature의 차원 수)이다.

```
from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB(alpha=1)
clf.fit(X_example, y_example)
```

### 가우시안 나이브 베이즈 분류기
마지막으로 이산형 데이터가 아닌 경우에 가우스 분포를 가정하는 아래와 같은 가우시안 나이브 베이즈 분류기를 쓸 수도 있다.

이 경우에는 가능도가 아래와 같이 바뀐다.
- 평균에서 멀어질수록 잘 안 일어나는 일이다.

$p(x_i|Y_c)=\frac{1}{\sqrt{2 \pi {\sigma_{Y_i}}^2}}exp(-\frac{(x_i-\mu Y_c)^2}{{2 \sigma_{Y_i}}^2})$

```
from sklearn.naive_bayes import GaussianNB

clf = GaussianNB()
clf.fit(X_example.toarray(), y_example)
```

## 20 newsgroupㅇ으로 분류 연습하기
### 20 newsgroup Dataset
20 newsgroup은 1990년대부터 2000년대가지 newsgroup(news를 공유하는 이메일을 받는 group)의 news들과 주제를 가지고, 텍스트가 어떤 주제와 관련되어있는지를 보여준다.

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
%matplotlib inline
```

### Dataset 불러오기

```
from sklearn.datasets import fetch_20newsgroups
news = fetch_20newsgroups(subset='all')
news.keys()

"""
결과
dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])
"""
```

- data
  - 실제 데이터
 
- filenames
  - 저장된 데이터의 위치
 
- target_names
  - y 값의 이름
 
- target
  - y 값의 index
 
- DESCR
  - 데이터 설명

y 값에 해당하는 target과 target_names를 보자.
- newsgroup의 총 수는 20개이다.

```
news.target

news.target_names

"""
결과
array([10,  3, 17, ...,  3,  1,  7])

['alt.atheism',
 'comp.graphics',
 'comp.os.ms-windows.misc',
 'comp.sys.ibm.pc.hardware',
 'comp.sys.mac.hardware',
 'comp.windows.x',
 'misc.forsale',
 'rec.autos',
 'rec.motorcycles',
 'rec.sport.baseball',
 'rec.sport.hockey',
 'sci.crypt',
 'sci.electronics',
 'sci.med',
 'sci.space',
 'soc.religion.christian',
 'talk.politics.guns',
 'talk.politics.mideast',
 'talk.politics.misc',
 'talk.religion.misc']
"""
```

### 전처리
먼저 데이터프레임을 만들어서 데이터를 정리해보자.

```
news_df = pd.DataFrame({'News' : news.data, 'Target' : news.target})
news_df.head()
```

<img width="513" alt="image" src="https://github.com/user-attachments/assets/fac73e64-f35e-4d8d-941d-0bc4647e313c">

Target의 정보를 실제 이름으로 바꿔보자.

```
target_dict = {idx:name for idx, name in enumerate(news.target_names)}
news_df["Target"] = news_df["Target"].replace(target_dict)
```

그리고 간단한 텍스트 전처리를 수행한다.

```
def data_cleansing(df):
    # 정규표현식으로 이메일 제거 \w = [A-z0-9__]
    delete_email = re.sub(r'\b[\w\+]+@[\w]+.[\w]+.[\w]+.[\w]+\b', ' ', df) 
    # 이메일을 지운 후에 불필요한 숫자 제거
    delete_number = re.sub(r'\b|\d+|\b', ' ', delete_email) 
    # 알파벳이 아닌 문자를 모두 제거. \W = [^A-z0-9__]
    delete_non_word = re.sub(r'\b[\W]+\b', ' ', delete_number)
    # 공백 정규화 (띄어쓰기로 분할하고 다시 join해서 띄어쓰기를 "한 개만" 가지도록 합니다
    cleaning_result = ' '.join(delete_non_word.split())
    return cleaning_result

news_df.loc[:, 'News'] = news_df['News'].apply(data_cleansing)
news_df.head()
```

<img width="615" alt="image" src="https://github.com/user-attachments/assets/3352f2ef-828f-4f10-8928-08f8f7784b93">

### 벡터화
먼저 단어의 수를 줄이고 비슷한 뜻을 가진 경우를 한 가지로 묶기 위해 어간(stem) 추출을 해보자.
- 흔히 쓰이는 nltk를 써보자.

stem을 하면 어간이 같은 단어는 다 한가지의 기본형으로 묶이게 된다.
- 시제나 의미등이 일부 손실되므로, 하는 것이 꼭 좋다고 말할 수는 없다.

```
from nltk import stem
stmmer = stem.SnowballStemmer("english")
sentence = 'looking looks looked'
[stmmer.stem(word) for word in sentence.split()]

stmmer.stem("images"), stmmer.stem("imaging"), stmmer.stem("imagination")

"""
결과
['look', 'look', 'look']

('imag', 'imag', 'imagin')
"""
```

vectorizer는 `CountVectorizer` 이외에도 여러가지가 있다.

한자기 예는 TF - IDF(Term Frequency - Inverse Documnet Frequency) vectorizer이다.
- 문서의 특징을 나타내기 위해서 두가지 조건을 사용한다.
  - 특정 문서에 단어가 "많이 나오고"(Term Frequency)
  - 그 단어가 다른 문서에 "잘 안쓰이면"(Inverse Document Frequency)
  - 이 단어는 그 문서의 특징을 잘 나타낸다.
 
IDF는 여러 수식이 있지만, 그 중 하나는 아래와 같은 로그 식을 쓰는 것이다.
- $\log (\frac{N}{df_i})$

TF - IDF는 위 식을 쓰면 아래와 같이 쓰게 된다.
- TF - IDF = $tf_{i, j}\log (\frac{N}{df_i})$

```
from sklearn.feature_extraction.text import CountVectorizer
import nltk

enlish_stemmer = nltk.stem.SnowballStemmer("english")
class StemmedCountVectorizer(CountVectorizer): # CounterVectorizer를 상속받아 어간만 사용하는 Vectorizer를 만듭니다.
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer,self).build_analyzer()
        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))
    
from sklearn.feature_extraction.text import TfidfVectorizer

enlish_stemmer = nltk.stem.SnowballStemmer("english")
class StemmedTfidfVectorizer(TfidfVectorizer): # TfidfVectorizer를 상속받아 어간만 사용하는 Vectorizer를 만듭니다.
    def build_analyzer(self):
        analyzer = super(StemmedTfidfVectorizer,self).build_analyzer()
        return lambda doc: (enlish_stemmer.stem(w) for w in analyzer(doc))
```

### 모델링하기
지금 우리는 여러가지의 vectorizer와 여러가지의 ML 알고리즘이 있다.

`fit`과 `transform`을 이용한 pipelin을 만들어보자.

```
from sklearn.naive_bayes import MultinomialNB, BernoulliNB,GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline

vectorizer = [CountVectorizer(), TfidfVectorizer(), StemmedCountVectorizer(), StemmedTfidfVectorizer()]
# algorithms = [BernoulliNB(), MultinomialNB(), GaussianNB(), LogisticRegression()]
algorithms = [MultinomialNB(), LogisticRegression(max_iter=500)]

pipelines = []
```

`itertools.product`를 이용해서 두 array의 조합의 경우의 수를 pipeline으로 만들어보자.

```
import itertools
for case in list(itertools.product(vectorizer, algorithms)):
    pipelines.append(make_pipeline(*case))
pipelines

"""
결과
[Pipeline(steps=[('countvectorizer', CountVectorizer()),
                 ('multinomialnb', MultinomialNB())]),
 Pipeline(steps=[('countvectorizer', CountVectorizer()),
                 ('logisticregression', LogisticRegression(max_iter=500))]),
 Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),
                 ('multinomialnb', MultinomialNB())]),
 Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),
                 ('logisticregression', LogisticRegression(max_iter=500))]),
 Pipeline(steps=[('stemmedcountvectorizer', StemmedCountVectorizer()),
                 ('multinomialnb', MultinomialNB())]),
 Pipeline(steps=[('stemmedcountvectorizer', StemmedCountVectorizer()),
                 ('logisticregression', LogisticRegression(max_iter=500))]),
 Pipeline(steps=[('stemmedtfidfvectorizer', StemmedTfidfVectorizer()),
                 ('multinomialnb', MultinomialNB())]),
 Pipeline(steps=[('stemmedtfidfvectorizer', StemmedTfidfVectorizer()),
                 ('logisticregression', LogisticRegression(max_iter=500))])]
"""
```

이제 여러 알고리즘의 하이퍼파라미터를 만들어보자.
- 기본적으로는 클래스 이름_하이퍼파라미터의 이름 형태로 되어있다.

먼저 벡터화 파라미터를 고르자.

```
ngrams_params = [(1,1),(1,3)]
stopword_params = ["english"]
lowercase_params = [True, False]
max_df_params = np.linspace(0.4, 0.6, num=6)
min_df_params = np.linspace(0.0, 0.0, num=1)

attributes = {"ngram_range":ngrams_params, "max_df":max_df_params,"min_df":min_df_params,
              "lowercase":lowercase_params,"stop_words":stopword_params}
vectorizer_names = ["countvectorizer","tfidfvectorizer","stemmedcountvectorizer","stemmedtfidfvectorizer"]
vectorizer_params_dict = {}

for vect_name in vectorizer_names:
    vectorizer_params_dict[vect_name] = {}
    for key, value in attributes.items():
        param_name = vect_name + "__" + key
        vectorizer_params_dict[vect_name][param_name] = value

vectorizer_params_dict

"""
결과
{'countvectorizer': {'countvectorizer__ngram_range': [(1, 1), (1, 3)],
  'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
  'countvectorizer__min_df': array([0.]),
  'countvectorizer__lowercase': [True, False],
  'countvectorizer__stop_words': ['english']},
 'tfidfvectorizer': {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],
  'tfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
  'tfidfvectorizer__min_df': array([0.]),
  'tfidfvectorizer__lowercase': [True, False],
  'tfidfvectorizer__stop_words': ['english']},
 'stemmedcountvectorizer': {'stemmedcountvectorizer__ngram_range': [(1, 1),
   (1, 3)],
  'stemmedcountvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
  'stemmedcountvectorizer__min_df': array([0.]),
  'stemmedcountvectorizer__lowercase': [True, False],
  'stemmedcountvectorizer__stop_words': ['english']},
 'stemmedtfidfvectorizer': {'stemmedtfidfvectorizer__ngram_range': [(1, 1),
   (1, 3)],
  'stemmedtfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
  'stemmedtfidfvectorizer__min_df': array([0.]),
  'stemmedtfidfvectorizer__lowercase': [True, False],
  'stemmedtfidfvectorizer__stop_words': ['english']}}
"""
```

그 다음은 각 알고리즘의 파라미터를 고르자.

```
algorithm_names = ["multinomialnb", "logisticregression"]

algorithm_params_dict = {}
alpha_params = np.linspace(1.0, 1.0, num=1)
for i in range(1):
    algorithm_params_dict[algorithm_names[i]] = {
        algorithm_names[i]+ "__alpha" : alpha_params
    }
c_params = [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]

algorithm_params_dict[algorithm_names[1]] = [{
    #"logisticregression__multi_class" : ["multinomial"],
    "logisticregression__solver" : ["saga"], #sag나 saga는 multinomial이 기본입니다.
    "logisticregression__penalty" : ["l1"],
    "logisticregression__C" : c_params
    },{ 
    #"logisticregression__multi_class" : ["ovr"],
    "logisticregression__solver" : ['liblinear'], #Sublinear solver를 쓰면 기본이 ovr입니다. (교과서 코드 warning 해결)
    "logisticregression__penalty" : ["l2"],
    "logisticregression__C" : c_params
    }
]

algorithm_params_dict

"""
결과
{'multinomialnb': {'multinomialnb__alpha': array([1.])},
 'logisticregression': [{'logisticregression__solver': ['saga'],
   'logisticregression__penalty': ['l1'],
   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},
  {'logisticregression__solver': ['liblinear'],
   'logisticregression__penalty': ['l2'],
   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}]}
"""
```

마지막으로 pipeline에 이 파라미터를 차례대로 입력해보자.

```
pipeline_params= []
for case in list(itertools.product(vectorizer_names, algorithm_names)):
    vect_params = vectorizer_params_dict[case[0]].copy()
    algo_params = algorithm_params_dict[case[1]]  
    
    if isinstance(algo_params, dict):
        vect_params.update(algo_params)
        pipeline_params.append(vect_params)
    else:
        temp = []
        for param in algo_params:
            vect_params.update(param)
            temp.append(vect_params)
        pipeline_params.append(temp)

pipeline_params

"""
결과
[{'countvectorizer__ngram_range': [(1, 1), (1, 3)],
  'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
  'countvectorizer__min_df': array([0.]),
  'countvectorizer__lowercase': [True, False],
  'countvectorizer__stop_words': ['english'],
  'multinomialnb__alpha': array([1.])},
 [{'countvectorizer__ngram_range': [(1, 1), (1, 3)],
   'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
   'countvectorizer__min_df': array([0.]),
   'countvectorizer__lowercase': [True, False],
   'countvectorizer__stop_words': ['english'],
   'logisticregression__solver': ['liblinear'],
   'logisticregression__penalty': ['l2'],
   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},
  {'countvectorizer__ngram_range': [(1, 1), (1, 3)],
   'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
   'countvectorizer__min_df': array([0.]),
   'countvectorizer__lowercase': [True, False],
   'countvectorizer__stop_words': ['english'],
   'logisticregression__solver': ['liblinear'],
   'logisticregression__penalty': ['l2'],
   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],
 {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],
  'tfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
  'tfidfvectorizer__min_df': array([0.]),
...
   'stemmedtfidfvectorizer__lowercase': [True, False],
   'stemmedtfidfvectorizer__stop_words': ['english'],
   'logisticregression__solver': ['liblinear'],
   'logisticregression__penalty': ['l2'],
   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}]]
"""
```

### 학습하기
이제 학습을 해보자.
- 이미 pipeline을 다 만들었으니, 큰 문제는 없다.

먼저 feature(X)와 target(Y)를 아래와 같이 정리해보자.
- `LabelEncoder`를 사용하자.

```
from sklearn.preprocessing import LabelEncoder

X_data = news_df.loc[:, 'News'].tolist()
y_data = news_df['Target'].tolist()
# y_data 값을 기준으로 label으로 바꿔준다.
y = LabelEncoder().fit_transform(y_data)

y

"""
결과
array([10,  3, 17, ...,  3,  1,  7])
"""
```

여러 개의 모델과 파라미터를 바꿔가며 최적 모델을 찾는 방법으로 grid search가 있다.
- 파라미터의 모든 조합을 테스트하며 가장 좋은 성능의 모델을 찾는 것이다.
- `sklearn.model_selection.GridSerachCV`로 구현되어 있다.

```
pipeline_params

"""
결과
[{'countvectorizer__ngram_range': [(1, 1), (1, 3)],
  'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
  'countvectorizer__min_df': array([0.]),
  'countvectorizer__lowercase': [True, False],
  'countvectorizer__stop_words': ['english'],
  'multinomialnb__alpha': array([1.])},
 [{'countvectorizer__ngram_range': [(1, 1), (1, 3)],
   'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
   'countvectorizer__min_df': array([0.]),
   'countvectorizer__lowercase': [True, False],
   'countvectorizer__stop_words': ['english'],
   'logisticregression__solver': ['liblinear'],
   'logisticregression__penalty': ['l2'],
   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]},
  {'countvectorizer__ngram_range': [(1, 1), (1, 3)],
   'countvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
   'countvectorizer__min_df': array([0.]),
   'countvectorizer__lowercase': [True, False],
   'countvectorizer__stop_words': ['english'],
   'logisticregression__solver': ['liblinear'],
   'logisticregression__penalty': ['l2'],
   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}],
 {'tfidfvectorizer__ngram_range': [(1, 1), (1, 3)],
  'tfidfvectorizer__max_df': array([0.4 , 0.44, 0.48, 0.52, 0.56, 0.6 ]),
  'tfidfvectorizer__min_df': array([0.]),
...
   'stemmedtfidfvectorizer__lowercase': [True, False],
   'stemmedtfidfvectorizer__stop_words': ['english'],
   'logisticregression__solver': ['liblinear'],
   'logisticregression__penalty': ['l2'],
   'logisticregression__C': [0.1, 5.0, 7.0, 10.0, 15.0, 20.0, 100.0]}]]
"""
```

```
%%time
# 컴퓨터 성능에 따라 실행하면 컴퓨터가 멍출 가능성이 있다.
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score

import warnings
warnings.filterwarnings(action='ignore') # 경고 메시지 출력을 안 하도록...

scoring = ['accuracy']
estimator_results = []
for i, (estimator, params) in enumerate(zip(pipelines,pipeline_params)):
    n_jobs = -1 #
    gs_estimator = GridSearchCV(
            refit="accuracy", estimator=estimator,param_grid=params,
            scoring=scoring, cv=5, verbose=1, n_jobs=n_jobs)
    print(gs_estimator)
    
    gs_estimator.fit(X_data, y)
    estimator_results.append(gs_estimator)
```

- n_jobs
  - 동시에 몇 개의 worker가 돌아갈지 결정한다.(병렬화)
  - -1은 모든 processors를 사용하는 것을 의미한다.
 
- cv
  - cross - validation splitting strategy를 정한다.
  - 기본값은 5 - fold cross validation이다.
 
<img width="997" alt="image" src="https://github.com/user-attachments/assets/4ab38bdd-66d3-4794-b127-6ca25c075e97">

