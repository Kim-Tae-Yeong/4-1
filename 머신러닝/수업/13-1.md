# 의사 결정 트리
## 의사결정트리(Decision Tree)
어떤 규칙을 하나의 트리(tree) 형태로 표현한 후 이를 바탕으로 분류나 회귀 문제를 해결하는 것이다.

규칙은 'if - else'문으로 표현이 가능하다.

트리는 일종의 경로를 표현하는 것이다.

트리 구조의 마지막 노드에는 분류 문제에서 클래스, 회귀 문제에서는 예측치가 들어간다.

<img width="674" alt="image" src="https://github.com/user-attachments/assets/a8447994-594d-46f3-b1dc-049085e84d78">

의사결정트리는 딥러닝 기반을 제외한 전통적인 통계 기반의 머신러닝 모델 중 효과와 실용성이 가장 좋다.
- 테이블형 데이터에 있어 설명려고가 성능의 측면에서 딥러닝 모델들과 대등하게 경쟁한다.
- 앙상블(ensemble) 모델이나 부스팅(boosting) 같은 새로운 기법들이 모델들의 성능을 대폭 향상시키고 있다.

## 의사결정트리 분류기
의사결정트리의 노드(node) 구성이 가장 중요하다.

마지막 노드에 클래스나 예측치를 기입하고 상위의 부모 노드들에는 if - else문의 조건에 해당하는 정보가 있다.

분할 속성(splitting attributes)란 부모 노드에 들어가는 if - else문의 조건들이다.
- 어떤 분할 속성이 가장 모호성을 줄일 것인지 파악한다.
- ex.
  - 1 ~ 100까지의 숫자 중 하나를 맞추는 '숫자 예측 게임'
    - '재귀적 지역 최적화 방법
      - 현재 주어진 문제에서 분할 속성을 설정하고, 그 다음 남은 데이터 속에서 최적의 분할 속성을 찾아내는 것을 반복한다.
     
<img width="497" alt="image" src="https://github.com/user-attachments/assets/a81d576c-30cc-46be-8b9b-d1134da14513">

실질적으로는

1. 여러가지 독립 변수 중 하나의 독립 변수를 선택하고 그 변수에 대한 기준값(threshold)을 정한다.
    - 이를 분류 규칙이라고 한다.

2. 전체 학습 데이터 집합(부모 노드)을 해당 독립 변수의 값이 기준값보다 작은 데이터 그룹(자식 노드 1)과 해당 독립 변수의 값이 기준값보다 큰 데이터 그룹(자식 노드 2)으로 나눈다.
3. 각각의 자식 노드에 대해 1 ~ 2의 단계를 반복하여 하위의 자식 노드를 만든다.
    - 단, 자식 노드에 한가지 클래스의 데이터만 존재한다면 더 이상 자식 노드를 나누지 않고 중지한다.
  
## 트리로 분류하기
이렇게 자식 노드 나누기를 연속적으로 적용하면 노드가 계속 증가하는 나무(tree)와 같은 형태로 표현할 수 있다.
- 의사결정나무에 전체 트레이닝 데이터를 모두 적용해 보면 각 데이터는 특정한 노드를 타고 내려가게 된다.
- 각 노드는 그 노드를 선택한 데이터 집합을 갖는다.
- 이 때 노드에 속한 데이터의 클래스의 비율을 구하여 이를 그 노드의 조건부 확률 분포 P(Y = k | X)<sub>node</sub>라고 정의한다.

<img width="244" alt="image" src="https://github.com/user-attachments/assets/9011a777-8399-4b7e-8f07-9e8c8803fa05">

- 테스트 데이터 X_test의 클래스를 예측할 때는 가장 상위의 노드부터 분류 규칙을 차례대로 적용하여 마지막에 도달하는 노드의 조건부 확률 분포를 이용하여 클래스를 예측한다.

## 엔트로피(Entropy)
물질의 열적 상태를 나타내는 물리량의 하나이다.

"무질서"의 척도라고 보통 사용하지만 완전한 설명은 아니다.
- 무질서보다는 "균등" 혹은 "평형"에 대한 척도이다.

물리량의로의 엔트로피에서 정보량의 기대치로의 엔트로피가 등장했다.

Y = 0 또는 Y = 1인 두 가지 값을 가지는 확률변수의 확률분포가 다음과 같이 세 종류가 있다고 해보자.
- 확률분포 Y<sub>1</sub> : P(Y = 0) = 0.5, P(Y = 1) = 0.5
- 확률분포 Y<sub>2</sub> : P(Y = 0) = 0.8, P(Y = 1) = 0.2
- 확률분포 Y<sub>3</sub> : P(Y = 0) = 1.0, P(Y = 1) = 0.0

<img width="530" alt="image" src="https://github.com/user-attachments/assets/b632532d-69d8-4611-a8cf-23317dee05dc">

베이지안 관점에서 위 확률분포는 다음과 같은 정보를 나타낸다.
- 확률분포 Y<sub>1</sub>은 y값에 대해 아무것도 모르는 상태이다.
- 확률분포 Y<sub>2</sub>는 y값이 0이라고 믿지만 아닐 가능성도 있다는 것을 아는 상태이다.
- 확률분포 Y<sub>3</sub>은 y값이 0이라고 100% 확신하는 상태이다.

확률분포가 가지는 이러한 차이를 하나의 숫자로 나타낸 것이 바로 엔트로피이다.

## 엔트로피를 측정하는 방법
수학적으로 정보 엔트로피는 확률분포함수를 입력으로 받아 숫자를 출력하는 범함수(functional)로 정의한다.(H[X] 기호로 표기한다.)

확률변수 Y가 카테고리분포와 같은 이산확률변수이면 아래처럼 정의한다.

<img width="271" alt="image" src="https://github.com/user-attachments/assets/c4244ade-057f-4d0f-80b9-708adabe8537">

- 이 식에서 K는 X가 가질 수 있는 클래스의 수이고 p(y)는 확률질량함수이다.
- 확률은 무조건 1보다 작다.
- 그래서 로그값이 항상 음수이므로 음수 기호를 붙여서 양수로 만들었다.

확률변수 Y가 정규분포와 같은 연속확률변수이면 아래처럼 정의한다.

<img width="266" alt="image" src="https://github.com/user-attachments/assets/be929147-f59d-4ce8-9bfa-1898ee187592">

- 이 식에서 p(y)는 확률밀도함수이다.

엔트로피 계산에서 p(y) = 0인 경우에는 로그값이 정의되지 않으므로 아래와 같은 극한값을 보통 사용한다.

<img width="141" alt="image" src="https://github.com/user-attachments/assets/2674f01c-2c3e-481e-b3e7-4fbd6d75c74e">

앞에서 예를 든 Y<sub>1</sub>, Y<sub>2</sub>, Y<sub>3</sub> 3개의 이산확률분포에 대해 엔트로피를 구하면 아래와 같다.

<img width="380" alt="image" src="https://github.com/user-attachments/assets/ac62e520-edd5-403d-a43e-01ab3ee4c802">

## 엔트로피 계산 예시
사람의 나이, 소득, 학생 여부, 신용 등급 등을 고려하여 컴퓨터를 구매할지 구매하지 않을지 의사결정을 해보자.

컴퓨터를 구매할 확률은 9 / 14이고, 구매하지 않을 확률은 5 / 14이다.

<img width="297" alt="image" src="https://github.com/user-attachments/assets/58cda6c6-a09e-4c07-a1a2-d5d1142211b3">

<br />

<img width="357" alt="image" src="https://github.com/user-attachments/assets/9a7fde95-2d9e-4391-b9a1-816c950ffae7">

## 정보 이득(정보 획득량, Information Gain)
정보획득량은 X라는 조건에 의해 확률 변수 Y의 엔트로피가 얼마나 감소하였는가를 나타내는 값이다.
- 아래처럼 Y의 엔트로피에서 X에 대한 Y의 조건부 엔트로피를 뺀 값으로 정의한다.
- IG[Y, X] = H[Y] - H[Y | X]

각각을 계산해보자.

전체 엔트로피는 아래처럼 구한다.

<img width="274" alt="image" src="https://github.com/user-attachments/assets/d45097ba-19e6-4a9e-ba2b-ab766dbdb5db">

조건부 엔트로피(속성별 엔트로피)는 아래와 같이 구한다.

<img width="323" alt="image" src="https://github.com/user-attachments/assets/dcbd1f59-e0c3-47bc-80b6-d442c2d7b814">

- 속성 A로 데이터를 분류했을 때 속성 A가 가진 모든 클래스의 각 엔트로피를 계산한 후, 데이터의 개수만큼 가중치를 합한다.

## 실제 계산 예시
에를 들어 A, B 두 가지의 다른 분류 규칙을 적용했다고 생각해보자.

<img width="403" alt="image" src="https://github.com/user-attachments/assets/7ea3952b-2ee7-4c58-852e-14e362cd3bd7">

A 방법과 B 방법 모두 노드 분리 전에는 Y = 0인 데이터의 수와 Y = 1인 데이터의 수가 모두 40개였다.

A 방법으로 노드를 분리하면 아래과 같은 두 개의 자식 노드가 생긴다.
- 자식 노드 A1은 Y = 0인 데이터가 30개, Y = 1인 데이터가 10개이다.
- 자식 노드 A2는 Y = 0인 데이터가 10개, Y = 1인 데이터가 30개이다.

B 방법으로 노드를 분리하면 아래와 같이 두 개의 자식 노드가 생긴다.
- 자식 노드 B1은 Y = 0인 데이터가 20개, Y = 1인 데이터가 40개이다.
- 자식 노드 B2는 Y = 0인 데이터가 20개, Y = 1인 데이터가 0개이다.

우선 부모 노드의 엔트로피를 계산하면 아래와 같다.

<img width="330" alt="image" src="https://github.com/user-attachments/assets/5ffc7076-ebf0-4c38-b964-e169f42e122e">

A 방법의 IG는 아래와 같다.

<img width="384" alt="image" src="https://github.com/user-attachments/assets/152c0aba-e20b-429a-8c18-f8bc60d1e1df">

B 방법의 IG는 아래와 같다.

<img width="374" alt="image" src="https://github.com/user-attachments/assets/639dd8a8-107a-4972-9c96-17ffb7eb6a56">

따라서 B가 더 나은 선택이다.

## ID3(Iterative Dichotomiser 3) 알고리즘
일반적으로 의사결정트리를 생성하는 방법을 성장이라고 부른다.
- 나무를 성장시키는 개념이다.(가지가 점점 늘어난다.)

ID3
- 반복적으로(iteratively) 데이터를 나누는(divides) 알고리즘이다.
- 탑다운(top - down) 방식으로 데이터를 나누면서 탐욕적(greedy)으로 현재 상태에서 최적화를 추진하는 방법을 추구한다.

## 기본적인 ID3 알고리즘
<img width="605" alt="image" src="https://github.com/user-attachments/assets/e0b62a3a-54f9-41fb-bc9d-ca549d8b695e">

<br />

<img width="384" alt="image" src="https://github.com/user-attachments/assets/e3559e8c-a1e7-4843-87b4-24da76eee99d">

모든 데이터가 동일한 클래스가 아니다.

최적 분류 기준이 되는 속성을 선정하기 위해, 정보 이득을 기반으로 속성별 데이터 분류의 기준을 정한다.

<img width="438" alt="image" src="https://github.com/user-attachments/assets/d38e1c29-a298-405b-8301-ff20e2c7847c">

age 기준의 정보 이득은 아래와 같다.

<img width="192" alt="image" src="https://github.com/user-attachments/assets/43a61949-2ba5-40f1-be16-961e5852aa59">

- j는 age 속성의 클래스인 youth, middle_age, senior이다.

<img width="312" alt="image" src="https://github.com/user-attachments/assets/207e20ff-f707-419d-8b83-89cb2e703013">

<br />

<img width="355" alt="image" src="https://github.com/user-attachments/assets/e9562d9e-0f13-4189-8eef-1a851626ac30">

같은 방식으로 다른 속성도 구해보자.

<img width="479" alt="image" src="https://github.com/user-attachments/assets/df1ca7f2-8b78-4fde-8ded-5588118615f2">

- ID3 알고리즘의 순서에 따라 가장 많은 정보를 주는 age 속성을 첫 번째 가지(branch) 속성이라고 한다.

age 속성 기준으로 데이터를 나누어 새로운 트리를 만든다.
- youth와 senior는 3 : 2 비율로 컴퓨터 구매 여부가 나누어진다.
- middle_age의 경우 모두 컴퓨터를 구입한다는 데이터이므로 더 이상 데이터를 분류할 필요가 없다.

<img width="393" alt="image" src="https://github.com/user-attachments/assets/9fb42a61-b3c3-48b8-a89e-b82d8a59ea44">

다음 단계를 해보자.
- age가 youth로 분류된 5개의 데이터에 대한 정보 이득을 구해보자.

<img width="436" alt="image" src="https://github.com/user-attachments/assets/bea98f92-e9f2-4396-8731-1a3fef52eeb6">

- student를 기준으로 데이터를 분리했을 때 가장 많은 정보를 얻는다.

<img width="458" alt="image" src="https://github.com/user-attachments/assets/de56bb1e-4cb2-4390-a379-8d3c67c9bad3">

이 과정을 반복하면 의사결정트리가 완성된다.

<img width="474" alt="image" src="https://github.com/user-attachments/assets/141b9db1-76c5-4776-9e02-1d4227ecbbff">

## 의사결정트리 알고리즘의 특징
### 재귀적 작동
가지가 되는 속성을 선택한 후 해당 가지로 데이터를 나누면 이전에 적용되었던 알고리즘이 남은 데이터에 다시 적용된다.
- 남은 데이터에서만 최적의 모델을 찾는 방법으로 작동한다.

### 속성 기준으로 가지치기 수행
가장 불확실성이 적은 속성을 기준으로 가지치기를 수행한다.

### 중요한 속성 정보 제공
처음 분리 대상이 되는 속성이 가장 중요한 속성이다.
- 이 특징을 '해석 가능한 머신러닝'이라고 부른다.

## 의사결정트리의 장점
### 불필요한 속성 값에 대한 스케일링
전처리 단계 없이 바로 사용할 수 있다.

### 강건(Robust)한 이상치(Outlier)
관측치의 절대값이 아닌 순서가 중요하기 때문에 필요 이상으로 엄청 큰 값이나 작은 값에 대해서도 분류 성능이 크게 떨어지지 않는다.

### 자동적인 변수 선택
알고리즘에 의해 중요한 변수들이 우선적으로 선택되어 조금 더 손쉽게 중요한 속성을 확인할 수 있다.
- 의사결정트리 계열의 알고리즘이 가지고 있는 가장 큰 장점 중 하나이다.

## 의사결정트리의 확장
### 정보 이득의 문제점
수식의 특성상 속성의 값이 다양할수록 선택의 확률이 높아지는 문제가 발생한다.

<img width="382" alt="image" src="https://github.com/user-attachments/assets/822da0db-6fec-4057-b622-cadc8d5ec003">

- 데이터가 매우 많고 속성이 다양할 때 위 수식의 |D<sub>j</sub>| / |D| 값이 작아진다.
- 해당 속성의 엔트로피가 낮아져 단순히 속성 안에 있는 값의 종류를 늘리는 것만으로 정보 이득이 높아진다.

## Information Gain의 한계점
Windy라는 지표로 분기했을 때 아래 경우의 information gaindms 0.5568이다.

<img width="442" alt="image" src="https://github.com/user-attachments/assets/592661e3-6c4a-4c8d-9a51-d710285cbf3a">

<br />

<img width="559" alt="image" src="https://github.com/user-attachments/assets/4a7911f1-618b-4325-945c-5a482d4431fe">

다른 지표인 With whom이라는 지표로 분기했을 때는 IG가 0.6813이다.

하지만 이 경우는 실제로 정보에 비해 분할이 너무 많아 그리 효율적이지 않다.

<img width="627" alt="image" src="https://github.com/user-attachments/assets/089636f5-2ad4-4422-b116-7b93f909f816">

<br />

<img width="672" alt="image" src="https://github.com/user-attachments/assets/c4a6c257-1824-4652-8d36-bda5815ec1fa">

## 해결책 : C4.5 알고리즘
### C4.5
정보 이득을 측정하는 방식을 좀 더 평준화시켜 단순한 정보 값을 대신 사용한다.
- 기존 정보 이득의 분모에 평준화 함수 SplitInfo(혹은 intrinsic value)를 사용한다.

<img width="533" alt="image" src="https://github.com/user-attachments/assets/29c09cd3-ae83-4c67-b2ed-7c6ddd29fa4c">

- 특정 지표로 분기했을 때 생성되는 가지의 수를 v라고 하고, i번째 가지에 해당하는 확률을 p<sub>i</sub> = |D<sub>i</sub>| / |D|라고 할 때, intrinsic value는 아래와 같다.

<img width="404" alt="image" src="https://github.com/user-attachments/assets/28392e11-8ba2-414d-be07-f9b292d7ac71">

- 클래스가 많을 수록 |D<sub>i</sub>| / |D| 값이 작아지고, -log<sub>2</sub>|D<sub>i</sub>| / |D| 값은 커져 정규화된다.
- SplitInfo 함수 값이 분모에 들어가면서 클래스 불균형에 의해 생기는 불합리한 속성 분류를 보정할 수 있다.

<img width="461" alt="image" src="https://github.com/user-attachments/assets/52be3bd2-fd8b-40fb-baa9-e836b519a461">

## CART : Classification And Regression Tree
### 지니 지수
경제학에서 소득의 불평등도를 측정할 때 사용하는 지표이다.
- 의사결정 트리에서 각 속성의 불순도를 측정하는 방법으로 사용한다.

<img width="1063" alt="image" src="https://github.com/user-attachments/assets/d2cbbc3d-7a22-4b4d-8c55-d1d7a662a5ba">

### CART 알고리즘
불확실성을 측정하는 기준 값이 엔트로피에서 지니 지수로 바뀐 것이다.

구현 측면에서 가장 큰 차이점은 이진 분할을 실시한다는 것이다.
- k가 속성 내에 있는 데이터의 개수일 때, 2<sup>k - 1</sup>개의 분할이 생성된다.
- 각 속성별 지니 지수 정보는 아래와 같다.

<img width="789" alt="image" src="https://github.com/user-attachments/assets/1d9c85ab-8d3a-4641-b660-220260cae89c">

<br />

<img width="914" alt="image" src="https://github.com/user-attachments/assets/afd7d810-5427-4bac-ba27-3119b56b2de5">

## 지니 지수로 의사결정트리 만들기
<img width="620" alt="image" src="https://github.com/user-attachments/assets/df2d11ed-2a9b-4d2f-b3fd-231a8be82517">

age, credit, income, student의 4가지 속성들을 정의한다.

<img width="675" alt="image" src="https://github.com/user-attachments/assets/8fe4b3de-e588-484a-80c9-cb49e147b0a9">

age 속성에 3가지 클래스가 존재하기 때문에 3가지 종류의 이진 분할 경우의 수로 나눌 수 있다.

<img width="490" alt="image" src="https://github.com/user-attachments/assets/1a1528ed-b5c0-43dc-aa11-111e83b01c47">

<br />

<img width="619" alt="image" src="https://github.com/user-attachments/assets/b38da3ae-e212-45ef-aa4f-198700aa2de9">

## 의사결정트리의 확장
<img width="496" alt="image" src="https://github.com/user-attachments/assets/eda8eef8-2b89-4617-afa9-e9ba696f59f2">

age_1, age_2, age_3 각각 연산하면 0.393, 0.357, 0.457이다.

age 속성의 지니 지수는 이중 가장 작은 값인 0.357이다.

<img width="250" alt="image" src="https://github.com/user-attachments/assets/5b698633-f5ab-4940-a670-ac3aee17535f">

## 의사결정나무의 크기 선택
### 트리 가지치기
의사결정트리의 마지막 노드의 개수를 지정하여 트리의 깊이를 조정하는 방법이다.

클래스의 마지막 노드인 잎 노드(leaf node)의 개수를 개발자가 직접 결정한다.

1개로 이루어진 잎 노드가 많을 경우 과대적합되어 있는 상태이다.

잎 노드의 개수와 관계 없이 해당 가지에 불확실성이 너무 높을 경우 의사결정트리의 성능에 문제를 줄 수 있다.

일댄 최대한 분할한 후, 불필요한 가지를 쳐내는 방법이다.

### 사전 가지치기(Pre - Pruning)
처음 트리를 만들 때 트리의 깊이나 마지막 노드의 최소 개수 등을 사전에 결정하여 입력하는 방법이다.

데이터 분석가가 하이퍼 매개변수로 모든 값을 입력해야 하는 점이 어렵다.

계산 효율이 좋고 작은 데이터셋에도 쉽게 작동한다.

사용자가 중요한 속성 값을 놓치거나 과소적합 문제가 발생할 수도 있다.

### 사후 가지치기(Post - Pruning)
트리를 먼저 생성한 후 실험적으로 하이퍼 매개변수를 조정하는 방법이다.

하나의 지표를 정해두고 실험적으로 다양한 하이퍼 매개변수를 조정하며 최적의 값을 찾는다.

'최종 노드의 개수', '트리의 깊이', 또는 '선택되는 속성의 개수' 등을 하이퍼 매개변수로 보고 조정하며 성능을 비교한다.

먼저 전체 데이터를 훈련셋, 검증셋, 테스트셋으로 분류하고, 훈련셋과 테스트셋의 성능을 비교한다.

<img width="624" alt="image" src="https://github.com/user-attachments/assets/f89bfe64-37cf-4255-a4fb-fab981a27520">

### 분할정지 방법
트리 모형에서 분할 단계마다 꼭 필요한지 통계적 유의성을 검정해서 평가하는 방법이다.

통계적으로 유의하면 분할하고, 아니라면 분할하지 않는다.

CHAID라는 방법에서 주로 사용한다.

### CHAID(CHi - square Automatic Interaction Detection) 방법
카이제곱검정을 사용해 분할점과 분할집합을 고르는 방법이다.

범주형 데이터만 가능하다.

## 연속형 변수 나누기
### 연속형 데이터를 나누는 기준
#### 모든 데이터를 기준점으로 하여 데이터 나누기
너무 많은 기준점이 생겨 과대적합 문제가 발생하거나 분류의 정확도가 떨어진다.

#### 통계적 수치로 중위값이나 4분위수를 기준점으로 나누기
25%씩 데이터를 나눠서 분류 기준을 변경한다.

과소적합 문제가 발생하여 분류의 성능을 떨어뜨릴 수 있다.

#### Y 클래스의 값을 기준으로 해당 값이 변할 때를 기준점으로 삼아 분기
가장 많이 사용하는 방법이다.

<img width="948" alt="image" src="https://github.com/user-attachments/assets/3b27610d-3bbb-4a09-9cd8-87bc70810a68">

분류 대상이 되는 ELEVATION 속성의 데이터를 정렬한다.

<img width="599" alt="image" src="https://github.com/user-attachments/assets/de1cb79e-9648-47aa-bb6f-64d86420975a">

데이터 중 분류 대상이 되는 기준점을 찾는다.
- 300, 1200, 1500, 3900에서 각각 Y 데이터의 라벨이 변한다.

<img width="703" alt="image" src="https://github.com/user-attachments/assets/19abee02-4ccb-41f3-8257-768c32f951d1">

데이터를 자르는 기준값을 정한다.
- 구간별 경계의 평균값을 사용한다.

<img width="716" alt="image" src="https://github.com/user-attachments/assets/d028981b-f22b-4e89-8b4b-3cee160ec723">

구간별 경계값을 기준으로 엔트로피를 산출한다.
- 4개의 기준점 각각의 정보 이득을 구했을 때 가장 큰 값이 ELEVATION의 대표 정보 이득이 되어 다른 속성값 정보 이득과 비교하여 최종적으로 분기가 일어나는 속성으로 선택된다.

<img width="545" alt="image" src="https://github.com/user-attachments/assets/b3a9f389-ec0c-404e-9c8d-ad53345e8f6c">

- STREAM, SLOPE 속성의 정보 이득은 각각 0.3059, 0.5774이다.
- ELEVATION이 4175인 값을 기준으로 트리를 분기한다.

## 회귀 트리(Regression Tree)
Y 데이터의 값이 연속형일 때의 의사결정트리 생성 방법이다.

### 연속형 속성 분기의 특징
명목 속성과는 달리 여러 번 재사용이 가능하다.

경계값을 기준으로 여러 번의 분기를 할 수 있다.

처음에는 ELEVATION 4175의 값을 기준으로 분기를 했다면 동일하게 맨 마지막에는 2250의 값으로 분기할 수 있다.

<img width="291" alt="image" src="https://github.com/user-attachments/assets/02e179f8-6a1d-495d-88a8-dd2141f0c546">

## 회귀 트리
Y 값의 각 속성별 분산이 얼마나 작은지를 측정한다.

최종 결과 노드에서는 결과 노드들의 Y 평균값으로 최종 예상치를 반환한다.

<img width="574" alt="image" src="https://github.com/user-attachments/assets/fedac8f2-c990-4c50-84ba-5b66b6006ab4">

<br />

<img width="503" alt="image" src="https://github.com/user-attachments/assets/15d91029-107a-4775-a54f-607e6beeec2f">

### 속성별 분산 구하기
각 클래스값들의 분산을 구하고 해당 클래스가 가진 데이터 개수의 비율만큼 곱한다.

<img width="456" alt="image" src="https://github.com/user-attachments/assets/0eeed0da-52a4-40b1-ad94-164d849f4179">

## 회귀 트리
<img width="777" alt="image" src="https://github.com/user-attachments/assets/83098bf2-5066-4cef-b8c5-5677df587bd5">

SEASON 속성이 WORK_DAY에 비해서 분산이 낮으므로 해당 값으로 분기가 일어난다.

<img width="792" alt="image" src="https://github.com/user-attachments/assets/ffb8f3c4-336a-4b42-ba4e-5ba968144beb">

<br />

<img width="798" alt="image" src="https://github.com/user-attachments/assets/994ea74e-0e38-497d-a9b4-41ea850169cd">

각 분기별로는 WORK_DAY로 데이터를 나누고, 나누어진 데이터는 평균값으로 최종 예측값이 반환된다.
- summer를 기준으로 분기가 일어난 후 WORK_DAY가 True인 분기에서 RENTALS 값이 5,800과 6,200이어서 평균값 6,000이 예측치로 반환된다.
